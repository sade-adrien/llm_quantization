{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Quantization\n",
    "This notebook presents two different methods to quantize LLMs: GPTQ and AWQ. We recommend using AWQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ\n",
    "GPTQ is a quantization algorithm that quantizes LLMs' weights to 8, 4, 3 or 2 bit precision using an iterative 2nd order optimization process and a small calibration dataset.\n",
    "\n",
    "More information at: https://arxiv.org/pdf/2210.17323.pdf\n",
    "\n",
    "The tutorial provided uses AutoGPTQ implementation: https://github.com/AutoGPTQ/AutoGPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's download some useful libraries - this might take several minutes\n",
    "\n",
    "!pip install torch transformers datasets optimum accelerate\n",
    "!pip install git+https://github.com/AutoGPTQ/AutoGPTQ.git -vvv --no-build-isolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's import some usefull libraries\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\n",
    "\n",
    "DEVICE = 'cuda:0'                                                       # device where to load the model and perform the quantization\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"                         # model to quantize, can be a hugginface id or a local path\n",
    "QUANTIZED_MODEL_ID = \"mistral-7b-instruct-v0.2-GPTQ-Q4-GS128-AOT-TST\"   #name given to the quantized model\n",
    "BITS = 4                                                                # number of bits to quantize to, recommended 4\n",
    "GROUP_SIZE = 128                                                        # size used for grouping in quantization algorithm, recommended 128, -1 quantizes per column\n",
    "ACT_ORDER = True                                                        # whether to quantize columns in order of decreasing activation size, recommended True\n",
    "TRUE_SEQUENTIAL = True                                                  # whether to perform sequential quantization even within a single transformer block, recommended True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the model with the quantization configuration defined above\n",
    "\n",
    "quantization_config = BaseQuantizeConfig(bits = BITS,\n",
    "                                group_size = GROUP_SIZE,\n",
    "                                desc_act = ACT_ORDER,\n",
    "                                true_sequential = TRUE_SEQUENTIAL,\n",
    "                                )\n",
    "model = AutoGPTQForCausalLM.from_pretrained(MODEL_ID, quantization_config).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset that'll be used for quantization (calibration dataset)\n",
    "# Note: this dataset can be adapted to a specific fine-tuning, but as a general rule it should be similarly built as the model's training data\n",
    "# Thus, for a foundational model such as Mistral-7B, a general-knowledge pre-training dataset extract works well\n",
    "# We use here a 128-sample extract from the RedPajama2 of 2048 tokens\n",
    "\n",
    "dataset = load_dataset('sade-adrien/quantization_samples', split='train')\n",
    "quantization_samples = [tokenizer(sample, truncation=True, max_length=2048) for sample in dataset['raw_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the quantization - takes 15-25mn for a 7B-parameter model on GPU and above dataset\n",
    "\n",
    "model.quantize(quantization_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "\n",
    "model.save_quantized(QUANTIZED_MODEL_ID)\n",
    "tokenizer.save_pretrained(QUANTIZED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quantized model\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(QUANTIZED_MODEL_ID,\n",
    "                                            use_marlin = False,         # use marlin kernel\n",
    "                                            disable_exllama = False,    # use exllama-1 kernel\n",
    "                                            disable_exllamav2 = True,   # use exllama-2 kernel\n",
    "                                            device=DEVICE,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANTIZED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load the quantized model with HuggingFace framework, not recommended\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from optimum.gptq import load_quantized_model\n",
    "from accelerate import init_empty_weights\n",
    "import torch\n",
    "\n",
    "with init_empty_weights():\n",
    "    empty_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map='cpu')\n",
    "empty_model.tie_weights()\n",
    "model = load_quantized_model(empty_model, save_folder=QUANTIZED_MODEL_ID, device_map={'': DEVICE})\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANTIZED_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ\n",
    "AWQ is an algorithm based on the hypothesis that a large percentage of the weights have a small impact on the output. The small fraction of important (aka salient) weights should be preserved in better precision during quantization to keep high performance. To this end, they are scaled efficiently right before quantization. \n",
    "\n",
    "More information at: https://arxiv.org/pdf/2306.00978.pdf\n",
    "\n",
    "The tutorial provided uses AutoAWQ implementation: https://github.com/casper-hansen/AutoAWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this algorithm is not about the quantization itself but rather an improvement. This means, we have to use a quantization process along AWQ that is orthogonal. GPTQ is a perfect candidate and is commonly used along AWQ: this is the one used under the hood in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's download some useful libraries - this might take several minutes\n",
    "\n",
    "!pip install torch transformers datasets optimum accelerate\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ_kernels.git -vvv\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ.git -vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's import some usefull libraries\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\n",
    "\n",
    "DEVICE = 'cuda:0'                                                       # device where to load the model and perform the quantization\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"                         # model to quantize, can be a hugginface id or a local path\n",
    "QUANTIZED_MODEL_ID = \"mistral-7b-instruct-v0.2-AWQ-Q4-GS128-GEMM\"       #name given to the quantized model\n",
    "BITS = 4                                                                # number of bits to quantize to, currently only 4 bits is supported\n",
    "GROUP_SIZE = 128                                                        # size used for grouping in quantization algorithm, recommended 128, -1 quantizes per column\n",
    "VERSION = 'GEMM'                                                        # version to quantize to ['GEMM', 'GEMV', 'GEMV_fast', 'marlin'], recommended GEMM\n",
    "ZERO_POINT = True                                                       # whether to use zero-point quantization, recommend True, need False for marlin kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the model with the quantization configuration defined above\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(MODEL_ID, device_map={'': DEVICE})\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the quantization - takes 15-25mn for a 7B-parameter model on GPU\n",
    "\n",
    "quantization_config = {\"zero_point\": ZERO_POINT, \n",
    "                    \"q_group_size\": GROUP_SIZE, \n",
    "                    \"w_bit\": BITS, \n",
    "                    \"version\": VERSION, \n",
    "                    \"modules_to_not_convert\": [],\n",
    "                    }\n",
    "\n",
    "model.quantize(tokenizer, quant_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "\n",
    "model.save_quantized(QUANTIZED_MODEL_ID)\n",
    "tokenizer.save_pretrained(QUANTIZED_MODEL_ID)\n",
    "with open(f\"{QUANTIZED_MODEL_ID}/quant_config.json\", \"w\") as file:\n",
    "    json.dump(quantization_config, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quantized model\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(QUANTIZED_MODEL_ID, \n",
    "                                            fuse_layers=True,               # fusing layers accelerates inference greatly\n",
    "                                            use_exllama=False,              # use exllama-1 kernel\n",
    "                                            use_exllama_v2=True,            # use exllama-2 kernel\n",
    "                                            max_seq_len=512,                # max sequence length, used when fusing layer to allocate memory\n",
    "                                            device_map={'': DEVICE},\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANTIZED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load the quantized model with HuggingFace framework, not recommended\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "quantization_config = AwqConfig(version=\"gemm\",                                                            # version of model to be loaded\n",
    "                                exllama_config={\"version\": 2, \"max_input_len\": 2048, \"max_batch_size\": 8}, # use exllama kernel, can be omitted\n",
    "                                do_fuse = True,                                                            # fusing layers accelerates inference greatly\n",
    "                                fuse_max_seq_len = 512,                                                    # max sequence length, used when fusing layer to allocate memory\n",
    "                                )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(QUANTIZED_MODEL_ID,\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map={'': DEVICE},\n",
    "                                        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANTIZED_MODEL_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantization_notebook",
   "language": "python",
   "name": "quantization_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
