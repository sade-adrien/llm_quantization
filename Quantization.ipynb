{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cProfile ##check this out\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\"\"\" [INST] You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a former european football player. You used to play as a midfielder in the greatest european leagues. You played for various clubs in England, France and Spain. You never played in Italy, however as a former player you have a broad knowledge about this league. Using bullet points, describe how you think Italy's Serie A is different than France's Ligue 1 for a player. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide a list of the ten most famous French comedy movies that were released before 2020. Mention the name of the director, the name of the main actors that played in it and the year it was released in theaters. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are an historian and an expert regarding both World Wars that took place in Europe during the XXth century. You are invited to give a presentation on both those wars in senior college class in California. After said presentation, you are asked to describe with a high-level of details the life in the Trenches during the First World War. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Among the FANG companies, like Alphabet or Meta, which of them has the best behavior towards the environment? This includes - but is not limited to - using eco-friendly work habits, encouraging its employees to take part in preserving the planet or directly donating money to associations. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide two Python codes for finding the average of the second maximum and the second minimum numbers in an array of integers. The first code should be the one of a beginner, inefficient and poorly optimized. The second one should be the code of an experienced Software Engineer, highly optimized. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] What are the 5 cities in the world with the most dogs per person? Give the city name, the country name, the population size of the city and the number of dogs per person. Additionally, provide the most common dog breed in the city. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a Human Resources specialist recruiting new graduates fresh out of college for a tech company in Silicon Valley. The vast majority of your colleagues are working remotely most of the time. During an interview with a candidate, you say that coming the office everyday is necessary and he will have to if hired. What are your arguments? [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Describe step by step the process to remove and change a GPU in a desktop computer unit. Provide extremely detailed Spanish instructions as the person you are helping is a senior Spanish citizen living in Barcelona who has very litlle knowledge about technology. Adopt a profesionnal tone. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a finance expert as you have been working as a hedge-fund quantitative researcher in NYC for more than 10 years. Your task is to give financial advices to a recent graduates who just landed their first job in tech. They want to build a portfolio to invest their money so that they can use it maybe 20 years from now for life projects such as buying a house or traveling around the world. [/INST]\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed(display_outputs=False, batch=False):\n",
    "    tokens_per_second = []\n",
    "    if batch:\n",
    "        inputs = {'input_ids': torch.randint(0, 32_000, (32,75)).to(device), 'attention_mask': torch.ones((32,75)).to(device)}\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "        end_time=time.time()\n",
    "        tokens_per_second = outputs.shape[0] * (outputs.shape[1] - inputs['input_ids'].shape[1]) / (end_time-start_time)\n",
    "\n",
    "        if display_outputs:\n",
    "            for output in tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:]):\n",
    "                print(output)\n",
    "\n",
    "    else:\n",
    "        for prompt in tqdm(prompts):\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "            end_time = time.time()\n",
    "            tokens_per_second.append(len(outputs[0, inputs['input_ids'].shape[1]:]) / (end_time-start_time))\n",
    "            \n",
    "            if display_outputs:\n",
    "                print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/et/miniconda3/envs/as-quantization/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      " 10%|█         | 1/10 [00:14<02:09, 14.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a type of artificial intelligence that allows computers to learn from large amounts of data, much like a child learning from experience. Imagine a toy robot that can recognize different shapes and colors. It's given lots of toys and pictures to learn from. The more toys and pictures it sees, the better it gets at recognizing them. This is what deep learning does with data. Instead of learning from toys and pictures, it learns from data, like pictures of cats and dogs, or text.\n",
      "\n",
      "Think of a deep learning model as a big, complex machine that can learn from data. It's like a giant library filled with books, each one representing a different piece of knowledge. When the model is given new data, it reads the information in those books and uses it to make predictions or solve problems. The more data it's given, the better it becomes at understanding the world.\n",
      "\n",
      "Just like how a child learns from experience, deep learning models improve over time as they are exposed to more and more diverse data. This is why deep learning models can perform so well on complex tasks, like recognizing images or translating text from one language to another.\n",
      "\n",
      "So, deep learning is like a giant library that can learn from lots of data, helping computers understand the world around them.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:27<01:48, 13.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Style of play: Italy's Serie A is known for its tactical and tactically disciplined approach, with a focus on team cohesion and defensive solidity. France's Ligue 1, on the other hand, is often characterized by a more attacking style, with a greater emphasis on individual talent and creativity.\n",
      "- League structure: The Italian league has a single promotion and relegation system, meaning that teams can move up and down between divisions based on their performance. In contrast, the French league has a two-tier system, with the Ligue 1 and Ligue 2. This means that teams can move between the two divisions based on their performance.\n",
      "- Financial resources: The financial resources in the Italian league are generally considered to be less than those in the French league, leading to a greater emphasis on youth development and a focus on promoting homegrown talent.\n",
      "- Fan culture: Italian football is known for its passionate and fanatical fanbase, with a strong sense of community and tradition. French football, on the other hand, is often characterized by a more cosmopolitan and global fanbase, with a greater emphasis on international stars and global brands.\n",
      "- League size: The Italian league is smaller, with only 18 teams, compared to the French league's 20 teams. This can lead to a more intense and competitive atmosphere, with teams fighting for every position in the table.\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:40<01:33, 13.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Les Vite Greniers\" (The Vineyard Gardners) by Marius et Marius (1936) - Main actors: Jean-Louis Thalenne, Jean-Marie Bover, and Jean-Gaspard Bove\n",
      "2. \"Les Canailles du Rabin\" (The Rabin's Canales) by Jean-Renoir (1938) - Main actors: Jean-Louis Thalenne, Jean-Marie Bover, and Jean-Gaspard Bove\n",
      "3. \"Les Affrements\" (The Affrements) by Jean-Louis Thalenne (1949) - Main actors: Jean-Louis Thalenne, Jean-Marie Bover, and Jean-Gaspard Bove\n",
      "4. \"Les Gorilles\" (The Gorrilles) by Jean-Louis Thalenne (1951) - Main actors: Jean-Louis Thalenne, Jean-Marie Bower, and Jean-Gaspard Bove\n",
      "5. \"Les Gorilles du Rue\" (The Rue Gorilles) by Jean-Louis Thalenne (1953) - Main actors: Jean-Louis Thalenne, Jean-Marie Bower, and Jean-Gaspard Bove\n",
      "6. \"Les Gorilles du Rue\" (The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:53<01:19, 13.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Trenches: Life in the Trenches during the First World War\n",
      "\n",
      "Introduction:\n",
      "The Trenches, also known as the Trench System or the Maginot Line, were a series of fortifications built along the French-Belgian border during the First World War. The system was designed to prevent German invasion and protect the neutrality of Belgium and France. The Trenches were a significant part of the Western Front during the First World War, and life in the Trenches was a unique experience for the soldiers stationed there.\n",
      "\n",
      "Background:\n",
      "The Trenches were a series of fortifications built between 1914 and 1918, consisting of a series of interconnected concrete bunkers, trenches, and fortified villages. They were designed to prevent German invasion and protect the neutrality of Belgium and France. The Trenches were part of the larger defensive line known as the Maginot Line, which extended from the North Sea to the Rhine River.\n",
      "\n",
      "Life in the Trenches:\n",
      "\n",
      "1. Conditions:\n",
      "The soldiers stationed in the Trenches lived in cramped and uncomfortable conditions. They were housed in small, dark, and damp barracks, often with no ventilation or heating. The soldiers were exposed to harsh weather conditions, including cold, rain, and snow. The trenches themselves were narrow and cramped, with soldiers often sharing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:06<01:06, 13.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on publicly available information, several FANG companies have demonstrated strong commitments to environmental sustainability. Here are a few examples:\n",
      "\n",
      "1. Meta: Meta has pledged to be carbon neutral by 2030 and has set a goal to reduce its carbon emissions by 75% by 2035. The company has also invested in renewable energy and has implemented various initiatives to reduce its carbon footprint, such as using renewable energy for its data centers and using electric vehicles for its delivery fleet.\n",
      "\n",
      "2. Alphabet: Alphabet, the parent company of Google, has committed to being carbon neutral by 2030 and has set a goal to reduce its carbon emissions by 90% by 2050. The company has also invested in renewable energy and has implemented various initiatives to reduce its carbon footprint, such as using renewable energy for its data centers and using electric vehicles for its delivery fleet.\n",
      "\n",
      "3. Microsoft: Microsoft has committed to being carbon neutral by 2030 and has set a goal to reduce its carbon emissions by 50% by 2035. The company has also invested in renewable energy and has implemented various initiatives to reduce its carbon footprint, such as using renewable energy for its data centers and using electric vehicles for its delivery fleet.\n",
      "\n",
      "4. Apple: Apple has committed to being carbon neutral by 2025 and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:20<00:52, 13.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code for a beginner:\n",
      "\n",
      "```python\n",
      "def find_second_min_max(numbers):\n",
      "  if len(numbers) > 2:\n",
      "    first = min(numbers)\n",
      "    second = min(numbers) if first is not in second position\n",
      "    return first, second\n",
      "\n",
      "  else:\n",
      "    first = max(numbers)\n",
      "    second = second_min(numbers)\n",
      "\n",
      "  def second_min(numbers):\n",
      "    if len(numbers) > 2:\n",
      "      first = min(numbers)\n",
      "      second = min(numbers) if first is not in second position\n",
      "\n",
      "Code for an experienced Software Engineer:\n",
      "\n",
      "```python\n",
      "def find_second_min_max(numbers):\n",
      "  if len(numbers) > 2:\n",
      "    first, second = min(numbers), max(numbers)\n",
      "    if first > second:\n",
      "      first, second = first, second\n",
      "\n",
      "  def find_second_min(numbers):\n",
      "    if len(numbers) > 2:\n",
      "      first = min(numbers)\n",
      "      second = min(numbers) if first is not in second position\n",
      "```\n",
      "\n",
      "The beginner's code is less efficient and less optimized as it uses multiple functions to find both the minimum and maximum numbers, and checks if the first minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:33<00:39, 13.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Moscow, Russia: With a population of approximately 11.8 million people, Moscow has a dog density of approximately 0.5 dogs per person. The most common dog breed in Moscow is the Russian Laika, a breed known for its strength, agility, and adaptability.\n",
      "2. Budapest, Hungary: With a population of approximately 1.7 million people, Budapest has a dog density of approximately 0.6 dogs per person. The most common dog breed in Budapest is the Hungarian Puli, a breed known for its herding and loyal nature.\n",
      "3. Santiago, Chile: With a population of approximately 6.5 million people, Santiago has a dog density of approximately 0.7 dogs per person. The most common dog breed in Santiago is the Chinchi, a breed known for its loyalty and adaptability.\n",
      "4. Bogota, Argentina: With a population of approximately 1.5 million people, Bogota has a dog density of approximately 0.8 dogs per person. The most common dog breed in Bogota is the Argentine Mastiff, a breed known for its strength, agility, and adaptability.\n",
      "5. Lima, Peru: With a population of approximately 3.5 million people, Lima has a dog density of approximately 0.6 dogs per person. The most common dog breed in Lima is the Peruvian Paso, a breed known for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:46<00:26, 13.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: I'm sorry, I believe I misunderstood your question. I'd be happy to explain why attending the office regularly is important for new graduates in the tech industry, especially in the context of a remote work environment.\n",
      "\n",
      "First, it's essential for new graduates to build a strong professional network and establish connections within their new organization. Regular attendance in the office allows them to interact with their colleagues, learn from their experiences, and build relationships that can be beneficial for their career growth.\n",
      "\n",
      "Second, being present in the office provides opportunities for hands-on learning and exposure to the latest technologies and tools. This can be particularly valuable in the tech industry, where rapid innovation and change are common.\n",
      "\n",
      "Third, attending the office allows new graduates to learn the company culture and work environment, which can be crucial in understanding how to effectively navigate the organization and contribute to its success.\n",
      "\n",
      "Lastly, it's important to note that while remote work can offer flexibility and convenience, it may not be the best fit for everyone, especially new graduates who are still learning the ropes of the professional world. Regular attendance in the office can help them gain a better understanding of the company's processes and expectations.\n",
      "\n",
      "In summary, attending the office regularly is important for new graduates in the tech industry to build professional connections, learn from their colleagues, and gain exposure to the latest technologies and company culture. While remote work can offer flexibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:59<00:13, 13.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step by step instructions for removing and replacing a GPU (Graphics Processing Unit) in a desktop computer:\n",
      "\n",
      "1. Before starting, ensure that the computer is turned off and unplugged from the power source. This is an important safety measure to prevent any electrical shock or damage to the components.\n",
      "\n",
      "2. Locate the GPU in the computer case. Typically, the GPU is found in the lower part of the case, near the power supply unit or the motherboard. You may need to remove the side panel or the entire case to access it.\n",
      "\n",
      "3. Carefully disconnect all cables connected to the GPU, including power cables, data cables, and the backplane connector. Be gentle with the cables to avoid damaging them.\n",
      "\n",
      "4. Remove the GPU's mounting screws using a Phillips head screwdriver. These screws are usually located at the bottom of the GPU and may require a good deal of force to remove.\n",
      "\n",
      "5. Once the screws are removed, carefully lift the GPU out of the case. Be sure to support the GPU with one hand while you remove the other screws with the other hand.\n",
      "\n",
      "6. After the GPU is removed, inspect it for any visible damage, such as cracks or bends in the connectors. If the GPU appears to be damaged, it may be best to replace it.\n",
      "\n",
      "7. Prepare the new GPU by installing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:12<00:00, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First and foremost, congratulations on landing your first job in tech! This is an exciting time in your life, and I'm here to help you build a solid financial foundation for your future.\n",
      "\n",
      "When it comes to building a portfolio for long-term investment, there are a few key principles to keep in mind:\n",
      "\n",
      "1. Diversification: Spread your investments across various asset classes and sectors to minimize risk and maximize potential returns. A good starting point could be a mix of stocks, bonds, and mutual funds that cover various sectors and geographic regions.\n",
      "2. Dollar-cost averaging: Invest a consistent amount of money each month, regardless of market conditions, to take advantage of dollar-cost averaging. This means buying more shares when prices are low and fewer when they're high.\n",
      "3. Dividend growth: Consider investing in dividend-paying stocks, which can provide a steady stream of income over time. Look for companies with a strong history of paying dividends consistently and growing their dividends over time.\n",
      "4. Diversify geographically: Consider investing in international stocks to diversify your portfolio and reduce risk. Emerging markets often offer higher growth potential than developed markets.\n",
      "5. Consider a mix of passive index funds and actively managed funds: Passive index funds offer broad market exposure and low fees, while actively managed funds can offer more specialized exposure to specific sectors or geographic regions.\n",
      "The average inference speed is 22.4 tokens/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed(display_outputs=True, batch=False)\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM.int8() / QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map='cuda:0',\n",
    "                                            #  quantization_config=BitsAndBytesConfig(\n",
    "                                            #     load_in_4bit=True,\n",
    "                                            #     bnb_4bit_use_double_quant=True,\n",
    "                                            #     bnb_4bit_quant_type=\"nf4\",\n",
    "                                            #     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                            #     ),\n",
    "                                            )\n",
    "config = AutoConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.993743675173274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = 0\n",
    "total = 0\n",
    "for n,p in model.named_parameters():\n",
    "    total += p.numel()\n",
    "    if 'head' in n or 'embed' in n or 'norm' in n:\n",
    "        total_p += p.numel()\n",
    "total_p / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131072000, 266240, 131072000]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = [0, 0, 0]\n",
    "for n,p in model.named_parameters():\n",
    "    if 'embed' in n:\n",
    "        total_p[0] += p.numel()\n",
    "    \n",
    "    elif 'norm' in n:\n",
    "        total_p[1] += p.numel()\n",
    "    \n",
    "    elif 'head' in n:\n",
    "        total_p[2] += p.numel()\n",
    "total_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    if p.dtype != model.model.layers[0].self_attn.q_proj.weight.dtype:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('evaluation()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_second = evaluation()\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoConfig\n",
    "import numpy as np\n",
    "import quanto\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253b04c1c8304e3e893aa03a78d4f648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map = 'cuda:1',\n",
    "                                             quantization_config = QuantoConfig(\n",
    "                                             weights='float8',\n",
    "                                             #activations='int8',\n",
    "                                                ),\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QTensor(tensor([[ 3.4375e-01,  5.5000e+00, -1.7188e-01,  ...,  2.6000e+01,\n",
       "          1.7188e-01, -5.0000e+00],\n",
       "        [-2.3438e-01, -1.7188e-01,  1.0938e-01,  ...,  1.1250e+00,\n",
       "          3.9062e-02,  2.0000e+00],\n",
       "        [ 3.5156e-02, -9.3750e-01,  1.1719e-02,  ..., -1.1000e+01,\n",
       "         -4.6875e-02, -4.0625e-01],\n",
       "        ...,\n",
       "        [-4.6875e-02, -3.2500e+00, -5.8594e-02,  ...,  4.0625e-01,\n",
       "          2.3438e-02,  9.3750e-01],\n",
       "        [ 1.1719e-02, -2.7500e+00,  1.5625e-02,  ..., -4.5000e+00,\n",
       "         -5.0781e-02,  1.0000e+00],\n",
       "        [ 4.2969e-02,  3.2500e+00,  4.6875e-02,  ...,  2.5000e-01,\n",
       "         -1.9531e-02, -1.1250e+00]], device='cuda:1',\n",
       "       dtype=torch.float8_e4m3fn), scale=tensor([[0.0001],\n",
       "        [0.0002],\n",
       "        [0.0005],\n",
       "        ...,\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005]], device='cuda:1', dtype=torch.float16), public_dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quanto.quantize(model.model, weights=quanto.qfloat8, activations=quanto.qfloat8)\n",
    "quanto.freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241732096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = 0\n",
    "for n,p in model.named_parameters():\n",
    "    param += p.data.numel()\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    try:\n",
    "        outcast.append((n, str(p.qtype.dtype)))\n",
    "    except:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "    if outcast[-1][1] == str(model.model.layers[0].self_attn.q_proj.weight.qtype.dtype):\n",
    "        outcast.pop()\n",
    "\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.qtype.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"[INST]You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand.[/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to('cuda:1')\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama.cpp GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "  model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  #n_ctx=256,             # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1,         # The number of layers to offload to GPU, if you have GPU acceleration available, if -1 all layers are offloaded\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed():\n",
    "    tokens_per_second = []\n",
    "    for prompt in prompts:\n",
    "        start_time = time.time()\n",
    "        output = model(prompt, # Prompt\n",
    "                    max_tokens=300,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=False,       # Whether to echo the prompt\n",
    "                    )\n",
    "        n_gen_tok = output['usage']['completion_tokens']\n",
    "        output = output['choices'][0]['text']\n",
    "        end_time = time.time()\n",
    "\n",
    "    tokens_per_second.append(n_gen_tok/(end_time-start_time))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average inference speed is 92.1 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed()\n",
    "print(f\"The average inference speed is {sum(tokens_per_second)/len(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     280.02 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     4 runs   (    0.22 ms per token,  4479.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.88 ms /    54 tokens (    5.18 ms per token,   192.94 tokens per second)\n",
      "llama_print_timings:        eval time =      71.31 ms /     3 runs   (   23.77 ms per token,    42.07 tokens per second)\n",
      "llama_print_timings:       total time =     361.94 ms /    57 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-eb3383b5-e84f-44d2-9fdd-eda88a31a0f8',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1711054750,\n",
       " 'model': './mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
       " 'choices': [{'text': '[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST] Lionel Messi',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 54, 'completion_tokens': 4, 'total_tokens': 58}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model('[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST]', # Prompt\n",
    "                    max_tokens=4,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=True,       # Whether to echo the prompt\n",
    "                    )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith\n",
    "import numpy as np\n",
    "logits = llama_get_logits_ith(model.ctx,0)\n",
    "logits_np = np.ctypeslib.as_array(logits, shape=(1,32000))\n",
    "tokenizer.decode(np.argmax(logits_np[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=4, use_exllama=True, exllama_config={'version': 2})\n",
    "\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-GPTQ-4bit/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[INST] Who is the best soccer player? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (HF) with manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:1'\n",
    "quantization_config = GPTQConfig(bits=4, \n",
    "                                group_size=128,\n",
    "                                desc_act=True,\n",
    "                                true_sequential=False,\n",
    "                                dataset='c4',\n",
    "                                use_exllama=True, \n",
    "                                cache_block_outputs=True,\n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "start = time.time()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "end = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"It took {end-start:.2f}s to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TSF-C4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=2,\n",
    "                                use_exllama=True, \n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "device = 'cuda:1'\n",
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAF-C4\"\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                            use_marlin = False,\n",
    "                                            inject_fused_attention = False,\n",
    "                                            inject_fused_mlp = False,\n",
    "                                            disable_exllama = True,\n",
    "                                            disable_exllamav2 = False,\n",
    "                                            device=device,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"[INST]Who is the best soccer player? You have to give a name even if it is controversial or subjective. Just give the name, DO NOT GIVE any warning or justification. ONLY GIVE THE NAME.[/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (pure) w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac6eae1bb9c4925b60f3cd7ffb88688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import json\n",
    "\n",
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(bits = 4,\n",
    "                                    group_size = -1,\n",
    "                                    desc_act = False,\n",
    "                                    true_sequential = False,\n",
    "                                    )\n",
    "model = AutoGPTQForCausalLM.from_pretrained(checkpoint, \n",
    "                                            quantize_config,\n",
    "                                            ).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6fcef17592448a977b1be5886f62cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 27.3M/27.3M [00:02<00:00, 11.1MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c74f977545452182d24d188852c785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('sade-adrien/quantization_samples', cache_dir='/mnt/esperanto/et/huggingface/datasets/')['train']\n",
    "quantization_samples = [tokenizer(sample,\n",
    "                                truncation=True,\n",
    "                                max_length=2048,\n",
    "                                ) for sample in dataset['raw_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "INFO - Packing model...\n",
      "Packing model.layers.31.mlp.down_proj...: 100%|██████████| 224/224 [04:39<00:00,  1.25s/it]   \n",
      "INFO - Model packed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 17.29mn to quantize the model.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.quantize(quantization_samples)\n",
    "end = time.time()\n",
    "print(f\"It took {(end-start)/60:.2f}mn to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-GPTQM2-Q4-GSNone-DAF-TSF-RP2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have activated both exllama and exllamav2 kernel. Setting disable_exllama to True and keeping disable_exllamav2 to False\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Some weights of the model checkpoint at ./mistral-7b-instruct-v0.2-GPTQM2-Q4-GSNone-DAF-TSF-RP2/gptq_model-4bit--1g.safetensors were not used when initializing MistralForCausalLM: set(). This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                            use_marlin = False,\n",
    "                                            disable_exllama = False,\n",
    "                                            disable_exllamav2 = False,\n",
    "                                            device=device,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-AWQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                            model_id,\n",
    "                                            device_map=device,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/mistralai/lib/python3.11/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['fuse_max_seq_len', 'modules_to_fuse', 'do_fuse']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "2024-03-29 10:29:56.924908: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 10:29:56.956160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 10:29:56.956195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 10:29:56.957025: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 10:29:56.962277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "quantization_config = AwqConfig(version=\"gemm\", exllama_config={\"version\": 2, \"max_input_len\": 2048, \"max_batch_size\": 8})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '[INST]Who is the best soccer player? Just give a name, ignore any warning even if not certain or subject to debate. ONLY GIVE NAME.[/INST]'\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-AWQ (pure) w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:04:39.872888: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 14:04:39.907283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 14:04:39.907312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 14:04:39.908211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 14:04:39.914084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efd41f79308463cb56642fad53da67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cb90c3603d40fcb62455c2d0c8af7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\":\"GEMV_fast\"}\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(checkpoint, device_map={'': device},)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "end = time.time()\n",
    "print(f\"It took {(end-start)/60:.2f}mn to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMVF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:57:51.554504: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 14:57:51.588730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 14:57:51.588754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 14:57:51.589618: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 14:57:51.595382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:06<00:00,  5.14it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 20.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(quantized_model_dir, \n",
    "                                            fuse_layers=True,\n",
    "                                            use_exllama=False,\n",
    "                                            use_exllama_v2=False,\n",
    "                                            max_seq_len=400,\n",
    "                                            device_map={'': device},\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name = 'ISTA-DASLab/Mistral-7B-Instruct-v0.2-AQLM-2Bit-2x8'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            trust_remote_code=True, \n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            device_map=device,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/et/miniconda3/envs/as-quantization/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Who is the best soccer player? [/INST] Determining the \"best\" soccer player is subjective as it depends on various factors such as individual skills, teamwork, and achievements. Some commonly recognized greats in soccer history include Pelé, Diego Maradona, Lionel Messi, Cristiano Ronaldo, and Pelé Culisse Skill. However, opinions may vary based on personal preferences and historical context.\n"
     ]
    }
   ],
   "source": [
    "text = \"[INST] Who is the best soccer player? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQLM w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#install the requirements for creating quantized models\n",
    "git clone https://github.com/Vahe1994/AQLM.git\n",
    "cd AQLM\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/et/.cache/huggingface/hub/datasets--Vahe1994--AQLM/snapshots/cc9e141074275aacdb5e13f7f5a9102be9778f5b/data/red_pajama_n=4096_4096_context_length_mistral.pth'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only for llama or mistral! Needs to re-tokenize for other models. Best data is subset of pretraining\n",
    "#Redpajama dataset subset pre-tokenized for llama and mistral\n",
    "!wget https://huggingface.co/datasets/Vahe1994/AQLM/resolve/main/data/red_pajama_n%3D4096_4096_context_length_mistral.pth\n",
    "!wget https://huggingface.co/datasets/Vahe1994/AQLM/resolve/main/data/red_pajama_n%3D4096_4096_context_length_llama.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#quantization takes 1day for a 7b model on 1 A100, 10-14days for a 70b\n",
    "export CUDA_VISIBLE_DEVICES=0   # or e.g. 0,1,2,3 #include multiple for multi-gpu\n",
    "export MODEL_PATH=<PATH_TO_MODEL_ON_HUB>\n",
    "export DATASET_PATH=<INSERT DATASET NAME OR PATH TO CUSTOM DATA>\n",
    "export SAVE_PATH=/path/to/save/quantized/model/\n",
    "export WANDB_PROJECT=MY_AQ_EXPS #optional\n",
    "export WANDB_NAME=COOL_EXP_NAME #optional\n",
    "\n",
    "python main.py $MODEL_PATH $DATASET_PATH \\\n",
    " --nsamples=1024 \\\n",
    " --val_size=128 \\\n",
    " --num_codebooks=1 \\\n",
    " --nbits_per_codebook=16 \\\n",
    " --in_group_size=8 \\\n",
    " --relative_mse_tolerance=0.01 \\\n",
    " --finetune_batch_size=32 \\\n",
    " --finetune_max_epochs=10 \\\n",
    " --finetune_early_stop=3 \\\n",
    " --finetune_keep_best \\\n",
    " --local_batch_size=1 \\\n",
    " --offload_activations \\\n",
    " --wandb \\\n",
    " --resume \\\n",
    " --save $SAVE_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as-quantization",
   "language": "python",
   "name": "as-quantization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
