{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cProfile ##check this out\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\"\"\" [INST] You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a former european football player. You used to play as a midfielder in the greatest european leagues. You played for various clubs in England, France and Spain. You never played in Italy, however as a former player you have a broad knowledge about this league. Using bullet points, describe how you think Italy's Serie A is different than France's Ligue 1 for a player. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide a list of the ten most famous French comedy movies that were released before 2020. Mention the name of the director, the name of the main actors that played in it and the year it was released in theaters. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are an historian and an expert regarding both World Wars that took place in Europe during the XXth century. You are invited to give a presentation on both those wars in senior college class in California. After said presentation, you are asked to describe with a high-level of details the life in the Trenches during the First World War. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Among the FANG companies, like Alphabet or Meta, which of them has the best behavior towards the environment? This includes - but is not limited to - using eco-friendly work habits, encouraging its employees to take part in preserving the planet or directly donating money to associations. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide two Python codes for finding the average of the second maximum and the second minimum numbers in an array of integers. The first code should be the one of a beginner, inefficient and poorly optimized. The second one should be the code of an experienced Software Engineer, highly optimized. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] What are the 5 cities in the world with the most dogs per person? Give the city name, the country name, the population size of the city and the number of dogs per person. Additionally, provide the most common dog breed in the city. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a Human Resources specialist recruiting new graduates fresh out of college for a tech company in Silicon Valley. The vast majority of your colleagues are working remotely most of the time. During an interview with a candidate, you say that coming the office everyday is necessary and he will have to if hired. What are your arguments? [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Describe step by step the process to remove and change a GPU in a desktop computer unit. Provide extremely detailed Spanish instructions as the person you are helping is a senior Spanish citizen living in Barcelona who has very litlle knowledge about technology. Adopt a profesionnal tone. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a finance expert as you have been working as a hedge-fund quantitative researcher in NYC for more than 10 years. Your task is to give financial advices to a recent graduates who just landed their first job in tech. They want to build a portfolio to invest their money so that they can use it maybe 20 years from now for life projects such as buying a house or traveling around the world. [/INST]\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed(display_outputs=False):\n",
    "    tokens_per_second = []\n",
    "    for prompt in tqdm(prompts):\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        output = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "        end_time = time.time()\n",
    "        tokens_per_second.append(len(output[0, inputs['input_ids'].shape[1]:]) / (end_time-start_time))\n",
    "        \n",
    "        if display_outputs:\n",
    "            print(tokenizer.decode(output[0, inputs['input_ids'].shape[1]:]))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 1/10 [00:11<01:47, 11.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey there, curious kid! I'd be happy to explain deep learning in a way that's easy for a 10-year-old to understand. Let's compare it to learning how to recognize handwritten numbers, like the ones in an old-fashioned math book.\n",
      "\n",
      "First, imagine you're given a bunch of pictures of numbers written by hand. Your job is to figure out which number each picture represents. At first, you might make some mistakes, but don't worry! That's just part of learning.\n",
      "\n",
      "Now, imagine you have a friend who can help you. This friend looks at all the pictures and points out which number each one is. You use this information to learn and improve. This is similar to how a computer uses something called \"training data\" to learn.\n",
      "\n",
      "But what if there are lots of different handwriting styles? Your friend might make mistakes too! That's where deep learning comes in. Instead of just one friend, imagine you have lots and lots of friends, each one an expert in recognizing a different kind of handwriting. Together, they can figure out any number, no matter how it's written.\n",
      "\n",
      "Deep learning is like having a big team of expert friends, all working together to help the computer understand things. It's a way for the computer to learn and improve, just like how you learn from your friends and teachers. Isn't that\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:19<01:12,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* **Style of Play:** Italy's Serie A is known for its tactical and defensive approach. Teams prioritize organization and discipline, focusing on counter-attacks and set pieces. In contrast, Ligue 1 in France tends to be more open and attacking, with a greater emphasis on possession and individual skills.\n",
      "\n",
      "* **Tactics:** In Serie A, teams often employ a 4-3-1-2 or 4-4-2 formation, with a focus on solid defense and quick counter-attacks. In Ligue 1, teams may use a 4-3-3 or 4-2-3-1 formation, with a greater emphasis on possession and attacking play.\n",
      "\n",
      "* **Rivalries:** In Italy, the biggest derbies are the Milan Derby (AC Milan vs Inter Milan) and the Turin Derby (Juventus vs Torino). In France, the biggest derbies are the Classique (Olympique Marseille vs Olympique Lyon) and the Derby du Rhin (Olympique Lyon vs AS Saint-Étienne).\n",
      "\n",
      "* **Fans:** Italian football fans are known for their passion and intensity. Matches can be heated, with frequent chanting and singing from the stands. In France, fans are also passionate, but the atmosphere can be more subdued compared to Italy.\n",
      "\n",
      "* **League\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:26<00:57,  8.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"La Grande Illusion\" (1937) - Director: Jean Renoir, Main Actors: Jean Gabin, Erich von Stroheim, Darnley Hood. This is a war comedy-drama about the camaraderie between French and German officers in a German camp during World War I.\n",
      "2. \"Les Vacances de M. Hulot\" (1953) - Director: Jacques Tati, Main Actors: Jacques Tati, Gérard Méry, Jean-Pierre Zola. This is a comedy about a man named Hulot and his misadventures during his vacation.\n",
      "3. \"Jules et Jim\" (1962) - Director: Alain Resnais, Main Actors: Oskar Werner, Jeanne More, Henri Saadik. This is a romantic comedy about two friends who both fall in love with the same woman.\n",
      "4. \"Borsalino\" (1970) - Director: Jacques Deray, Main Actors: Lino Ventura, Alain Delon, Marlène Jobert. This is a comedy about two friends who are both in love with the same woman.\n",
      "5. \"Les Tontons Flingueurs\" (1956) - Director: Jean Boyer, Main Actors: Jean-Paul Belmondo, Danielle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:33<00:46,  7.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: The Great Conflicts: An Overview of the First and Second World Wars in Europe\n",
      "\n",
      "Ladies and Gentlemen, I am delighted to be here today to share my knowledge and passion for European history with you. In this presentation, I will provide an overview of the two most significant conflicts of the 20th century: the First World War (1914-1918) and the Second World War (1939-1945). I will discuss the causes, major events, and consequences of each war.\n",
      "\n",
      "The First World War, also known as the Great War, was a global conflict that began in 1914 and lasted until 1918. The war was primarily caused by the complex web of alliances and rivalries that had developed among the major European powers in the late 19th and early 20th centuries. The assassination of Archduke Franz Ferdinand of Austria-Hungary in Sarajevo, Bosnia, on June 28, 1914, served as the immediate trigger for the war.\n",
      "\n",
      "The First World War was a brutal and devastating conflict. It involved millions of soldiers from various countries, including France, Germany, Russia, Britain, and the United States. The war was fought on multiple fronts, including the Western Front in Europe, the Eastern Front in Russia, and the Mediterranean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:40<00:37,  7.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Among the FANG companies (Facebook, Apple, Netflix, and Alphabet), Alphabet (Google) has been recognized for its significant efforts towards environmental sustainability. Here are some reasons why:\n",
      "\n",
      "1. Renewable Energy: Alphabet is the world's largest corporate purchaser of renewable energy. Google has committed to using 100% renewable energy for all its operations and data centers worldwide by 2030.\n",
      "\n",
      "2. Carbon Neutrality: Google aims to be carbon neutral by 2020. They have already achieved this goal for their operations and are working on reducing the carbon footprint of their supply chain and product use.\n",
      "\n",
      "3. Eco-friendly Work Habits: Google has implemented several eco-friendly work habits in its offices, such as using energy-efficient appliances, installing solar panels, and promoting public transportation or biking to work.\n",
      "\n",
      "4. Green Initiatives: Google has several green initiatives, such as Google Earth, Google Maps, and Google.org, which focuses on environmental issues and supports various environmental organizations.\n",
      "\n",
      "5. Employee Engagement: Google encourages its employees to take part in preserving the planet through various programs, such as the \"Go Green\" program, which offers incentives for employees who choose to commute sustainably, and the \"Green Team,\" which focuses on reducing the company's environmental impact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:47<00:29,  7.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beginner's Code:\n",
      "\n",
      "```python\n",
      "def find_second_min_max(arr):\n",
      "    # Find the minimum and maximum numbers in the array\n",
      "    min_num, max_num = min(arr), max(arr)\n",
      "\n",
      "    # Find the second minimum and second maximum numbers\n",
      "    second_min, second_max = (num for num in arr if num != min_num and num != max_num)[0], (num for num in arr if num != min_num and num != max_num)[1]\n",
      "\n",
      "    # Calculate the average of the second minimum and second maximum numbers\n",
      "    avg = (second_min + second_max) / 2\n",
      "\n",
      "    return avg\n",
      "\n",
      "arr = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "print(find_second_min_max(arr))\n",
      "```\n",
      "\n",
      "Experienced Software Engineer's Code:\n",
      "\n",
      "```python\n",
      "def find_second_min_max(arr):\n",
      "    # Sort the array in ascending order\n",
      "    arr.sort()\n",
      "\n",
      "    # Calculate the indices of the second minimum and second maximum numbers\n",
      "    second_min_index, second_max_index = arr.index(arr[arr.index(min(arr)) + 1]), arr.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:54<00:21,  7.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Querétaro, Mexico: Population size - 1.1 million, Dogs per person - 0.3, Most common dog breed - Rottweiler and Labrador Retriever. Querétaro is known for its vibrant culture and historic sites, making it a popular destination for dog lovers.\n",
      "\n",
      "2. Ankara, Turkey: Population size - 5.5 million, Dogs per person - 0.25, Most common dog breed - Anatolian Shepherd. Ankara is the capital city of Turkey and is famous for its rich history and beautiful landscapes.\n",
      "\n",
      "3. Buenos Aires, Argentina: Population size - 3 million, Dogs per person - 0.35, Most common dog breed - Bulldog and Labrador Retriever. Buenos Aires is the largest city in Argentina and is known for its European-style architecture and passionate football fans.\n",
      "\n",
      "4. Lima, Peru: Population size - 1 million, Dogs per person - 0.5, Most common dog breed - Peruvian Hairless Dog. Lima is the largest city in Peru and is famous for its beautiful beaches, delicious cuisine, and rich cultural heritage.\n",
      "\n",
      "5. Bogotá, Colombia: Population size - 8 million, Dogs per person - 0.2, Most common dog breed -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:01<00:14,  7.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a Human Resources specialist recruiting new graduates for a tech company in Silicon Valley, I understand that the trend towards remote work has become increasingly popular, especially in the tech industry. However, during an interview with a candidate, if I express the necessity of coming to the office every day, I would argue the following points:\n",
      "\n",
      "1. Collaboration and Teamwork: Working in an office environment allows for more effective collaboration and teamwork. Face-to-face interactions can lead to better communication, problem-solving, and idea generation.\n",
      "2. Learning from Experienced Colleagues: Being in the office provides opportunities to learn from experienced colleagues and mentors. This can lead to professional growth and development, which is essential for new graduates.\n",
      "3. Building Relationships: Building relationships with colleagues, managers, and clients is crucial for career advancement. Being present in the office allows for more opportunities to network and build strong professional relationships.\n",
      "4. Company Culture: Being part of a company's culture is important for new hires. Being present in the office allows them to understand the company's values, mission, and work environment better.\n",
      "5. Productivity and Focus: Working in an office environment can lead to increased productivity and focus. Being surrounded by colleagues and the company's resources can help new hires get acclimated to their new role and perform at their best.\n",
      "6. Adapting to the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:08<00:07,  7.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Buenas tardes querido senior, I'm glad to help you with the process of removing and replacing your graphics processing unit (GPU) in your desktop computer. I understand that technology can be intimidating, but I'll explain each step in detail and with a professional tone. \n",
      "\n",
      "Before we begin, please make sure you have the following tools and components ready:\n",
      "\n",
      "1. A Phillips head screwdriver\n",
      "2. Thermal paste (optional, if your new GPU doesn't come with it)\n",
      "3. New GPU\n",
      "4. Anti-static wrist strap (optional, but recommended for static protection)\n",
      "\n",
      "Now, let's begin:\n",
      "\n",
      "1. Shut down your computer and unplug it from the power source.\n",
      "2. Locate the power supply unit (PSU) in the back of your computer case. Turn it off by flipping the switch on the back or by unplugging the power cord.\n",
      "3. Open the side panel of your computer case using a Phillips head screwdriver. Set the screws aside.\n",
      "4. Locate the GPU in your computer case. It's usually the largest component with a fan attached to it.\n",
      "5. Disconnect the power connector from the GPU.\n",
      "6. Use a Phillips head screwdriver to remove the screws securing the GPU to the motherboard.\n",
      "7. Carefully lift the GPU away from the motherboard,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:15<00:00,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First and foremost, congratulations on landing your first job in tech! It's great that you're thinking about building a portfolio to invest your money for future financial goals such as buying a house or traveling around the world. Here are some financial advices that could help you get started:\n",
      "\n",
      "1. Establish an emergency fund: Before investing, it's essential to have an emergency fund that covers at least three to six months' worth of living expenses. This safety net will provide peace of mind and financial security in case of unexpected events.\n",
      "2. Maximize retirement contributions: If your employer offers a 401(k) or similar retirement plan, contribute enough to receive any matching funds if available. Maximizing retirement contributions is an essential step in building long-term wealth.\n",
      "3. Diversify your portfolio: Aim for a well-diversified portfolio by investing in various asset classes such as stocks, bonds, and real estate. Diversification helps minimize risk and maximize potential returns.\n",
      "4. Consider index funds: Index funds are a low-cost investment option that tracks a specific market index, such as the S&P 500. They offer broad market exposure and are an excellent choice for long-term investors.\n",
      "5. Rebalance your portfolio: Regularly rebalance your portfolio to maintain your desired asset allocation. Rebalancing helps ensure that your investments remain aligned with your financial goals\n",
      "The average inference speed is 41.0 tokens/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed(display_outputs=True)\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM.int8() / QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d710aa3aff8f40bba7350936f08a1ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map='cuda:0',\n",
    "                                            #  quantization_config=BitsAndBytesConfig(\n",
    "                                            #     load_in_4bit=True,\n",
    "                                            #     bnb_4bit_use_double_quant=True,\n",
    "                                            #     bnb_4bit_quant_type=\"nf4\",\n",
    "                                            #     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                            #     ),\n",
    "                                            )\n",
    "config = AutoConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.993743675173274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = 0\n",
    "total = 0\n",
    "for n,p in model.named_parameters():\n",
    "    total += p.numel()\n",
    "    if 'head' in n or 'embed' in n or 'norm' in n:\n",
    "        total_p += p.numel()\n",
    "total_p / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131072000, 266240, 131072000]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = [0, 0, 0]\n",
    "for n,p in model.named_parameters():\n",
    "    if 'embed' in n:\n",
    "        total_p[0] += p.numel()\n",
    "    \n",
    "    elif 'norm' in n:\n",
    "        total_p[1] += p.numel()\n",
    "    \n",
    "    elif 'head' in n:\n",
    "        total_p[2] += p.numel()\n",
    "total_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    if p.dtype != model.model.layers[0].self_attn.q_proj.weight.dtype:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('evaluation()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_second = evaluation()\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoConfig\n",
    "import numpy as np\n",
    "import quanto\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253b04c1c8304e3e893aa03a78d4f648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map = 'cuda:1',\n",
    "                                             quantization_config = QuantoConfig(\n",
    "                                             weights='float8',\n",
    "                                             #activations='int8',\n",
    "                                                ),\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QTensor(tensor([[ 3.4375e-01,  5.5000e+00, -1.7188e-01,  ...,  2.6000e+01,\n",
       "          1.7188e-01, -5.0000e+00],\n",
       "        [-2.3438e-01, -1.7188e-01,  1.0938e-01,  ...,  1.1250e+00,\n",
       "          3.9062e-02,  2.0000e+00],\n",
       "        [ 3.5156e-02, -9.3750e-01,  1.1719e-02,  ..., -1.1000e+01,\n",
       "         -4.6875e-02, -4.0625e-01],\n",
       "        ...,\n",
       "        [-4.6875e-02, -3.2500e+00, -5.8594e-02,  ...,  4.0625e-01,\n",
       "          2.3438e-02,  9.3750e-01],\n",
       "        [ 1.1719e-02, -2.7500e+00,  1.5625e-02,  ..., -4.5000e+00,\n",
       "         -5.0781e-02,  1.0000e+00],\n",
       "        [ 4.2969e-02,  3.2500e+00,  4.6875e-02,  ...,  2.5000e-01,\n",
       "         -1.9531e-02, -1.1250e+00]], device='cuda:1',\n",
       "       dtype=torch.float8_e4m3fn), scale=tensor([[0.0001],\n",
       "        [0.0002],\n",
       "        [0.0005],\n",
       "        ...,\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005]], device='cuda:1', dtype=torch.float16), public_dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quanto.quantize(model.model, weights=quanto.qfloat8, activations=quanto.qfloat8)\n",
    "quanto.freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241732096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = 0\n",
    "for n,p in model.named_parameters():\n",
    "    param += p.data.numel()\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    try:\n",
    "        outcast.append((n, str(p.qtype.dtype)))\n",
    "    except:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "    if outcast[-1][1] == str(model.model.layers[0].self_attn.q_proj.weight.qtype.dtype):\n",
    "        outcast.pop()\n",
    "\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.qtype.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"[INST]You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand.[/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to('cuda:1')\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama.cpp GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "  model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  #n_ctx=256,             # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1,         # The number of layers to offload to GPU, if you have GPU acceleration available, if -1 all layers are offloaded\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed():\n",
    "    tokens_per_second = []\n",
    "    for prompt in prompts:\n",
    "        start_time = time.time()\n",
    "        output = model(prompt, # Prompt\n",
    "                    max_tokens=300,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=False,       # Whether to echo the prompt\n",
    "                    )\n",
    "        n_gen_tok = output['usage']['completion_tokens']\n",
    "        output = output['choices'][0]['text']\n",
    "        end_time = time.time()\n",
    "\n",
    "    tokens_per_second.append(n_gen_tok/(end_time-start_time))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average inference speed is 92.1 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed()\n",
    "print(f\"The average inference speed is {sum(tokens_per_second)/len(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     280.02 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     4 runs   (    0.22 ms per token,  4479.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.88 ms /    54 tokens (    5.18 ms per token,   192.94 tokens per second)\n",
      "llama_print_timings:        eval time =      71.31 ms /     3 runs   (   23.77 ms per token,    42.07 tokens per second)\n",
      "llama_print_timings:       total time =     361.94 ms /    57 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-eb3383b5-e84f-44d2-9fdd-eda88a31a0f8',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1711054750,\n",
       " 'model': './mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
       " 'choices': [{'text': '[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST] Lionel Messi',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 54, 'completion_tokens': 4, 'total_tokens': 58}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model('[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST]', # Prompt\n",
    "                    max_tokens=4,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=True,       # Whether to echo the prompt\n",
    "                    )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith\n",
    "import numpy as np\n",
    "logits = llama_get_logits_ith(model.ctx,0)\n",
    "logits_np = np.ctypeslib.as_array(logits, shape=(1,32000))\n",
    "tokenizer.decode(np.argmax(logits_np[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=4, use_exllama=True, exllama_config={'version': 2})\n",
    "\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-GPTQ-4bit/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[INST] Who is the best soccer player? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (HF) with manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import torch\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=4, \n",
    "                                group_size=128,\n",
    "                                desc_act=True,\n",
    "                                true_sequential=False,\n",
    "                                dataset='c4',\n",
    "                                use_exllama=True, \n",
    "                                cache_block_outputs=True,\n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "start = time.time()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "end = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"It took {end-start:.2f}s to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TSF-C4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=4,\n",
    "                                use_exllama=True, \n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating PPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig, QuantoConfig\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import quanto\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 41.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "prompt = 'Mbappe is a French soccer player. He is considered one of the best player in the world. He won the world cup in 2018.'\n",
    "data = [prompt]\n",
    "ppl_list = []\n",
    "for prompt in tqdm(data):\n",
    "    cross_entropy_list = []\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "    labels = input_ids.clone()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, labels=labels).logits\n",
    "    \n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    losses = []\n",
    "    for i in range(1, logits.shape[1]-1):\n",
    "        logits_cut = logits[..., :i+1, :]\n",
    "        labels_cut = labels[..., :i+1]\n",
    "\n",
    "        shift_logits = logits_cut[..., :-1, :].contiguous()\n",
    "        shift_labels = labels_cut[..., 1:].contiguous()\n",
    "        # Flatten the tokens\n",
    "        shift_logits = shift_logits.view(-1, 32_000)\n",
    "        shift_labels = shift_labels.view(-1)\n",
    "        # Enable model parallelism\n",
    "        shift_labels = shift_labels.to(shift_logits.device)\n",
    "        loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(losses).mean())\n",
    "    ppl_list.append(ppl)\n",
    "\n",
    "return sum(ppl_list)/len(ppl_list), ppl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ppl():\n",
    "    path = '/mnt/datascience1/Adrien/Models_Tests/wikitext2.json'\n",
    "    with open(path) as f:\n",
    "        inputs = json.load(f)['text']\n",
    "\n",
    "    full_logits = []\n",
    "\n",
    "    inputs = inputs[:300]\n",
    "\n",
    "    for prompt in tqdm(inputs):\n",
    "        # Tokenize input\n",
    "        tokens = tokenizer(prompt, return_tensors = 'pt').input_ids.to(device)\n",
    "        input_ids = tokens[0].detach().tolist()\n",
    "        \n",
    "        logits_inf = []\n",
    "        \n",
    "        # Do the inference\n",
    "        for i in range(len(input_ids) + 1):\n",
    "            with torch.no_grad():\n",
    "                logits = model(torch.tensor([input_ids[0:(i + 1)]], device=device)).logits\n",
    "                \n",
    "            #Extract logits and normalize\n",
    "            softmax_logits = softmax(logits.cpu())[0][0][0].tolist()\n",
    "            logits_inf.append(softmax_logits)\n",
    "        \n",
    "        full_logits.append((logits_inf, input_ids))\n",
    "\n",
    "    \n",
    "    perplexities = []\n",
    "    for token_logits in full_logits:\n",
    "        logits = token_logits[0]\n",
    "        tokens = token_logits[1]\n",
    "        summ = 0\n",
    "        for i in range(len(tokens) - 1):\n",
    "            entropy = logits[i][tokens[i + 1]]\n",
    "            summ += -np.log(entropy)\n",
    "        logperplexity = summ/len(tokens)\n",
    "        print(logperplexity)\n",
    "        perplexities.append(np.exp(logperplexity))\n",
    "    \n",
    "    return sum(perplexities)/len(perplexities), perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4b3a3105bf4bee9d4ea44fb96935df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                            torch_dtype = torch.float16,       \n",
    "                                            device_map = device,\n",
    "                                            quantization_config = None,\n",
    "                                            cache_dir='/mnt/esperanto/et/huggingface/hub'\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "list_configs = [\n",
    "    (torch.float32, None),\n",
    "    (torch.float16, None),\n",
    "    (torch.bfloat16, None),\n",
    "    (torch.float16, BitsAndBytesConfig(load_in_8bit=True)),\n",
    "    (torch.float32, BitsAndBytesConfig(load_in_4bit=True,\n",
    "                            bnb_4bit_use_double_quant=False,\n",
    "                            bnb_4bit_quant_type=\"fp4\",\n",
    "                            bnb_4bit_compute_dtype=torch.float16,\n",
    "                            )),\n",
    "    (torch.float32, BitsAndBytesConfig(load_in_4bit=True,\n",
    "                            bnb_4bit_use_double_quant=False,\n",
    "                            bnb_4bit_quant_type=\"nf4\",\n",
    "                            bnb_4bit_compute_dtype=torch.float16,\n",
    "                            )),\n",
    "    (torch.float16, BitsAndBytesConfig(load_in_4bit=True,\n",
    "                            bnb_4bit_use_double_quant=False,\n",
    "                            bnb_4bit_quant_type=\"nf4\",\n",
    "                            bnb_4bit_compute_dtype=torch.float16,\n",
    "                            )),\n",
    "    (torch.float16, BitsAndBytesConfig(load_in_4bit=True,\n",
    "                            bnb_4bit_use_double_quant=True,\n",
    "                            bnb_4bit_quant_type=\"nf4\",\n",
    "                            bnb_4bit_compute_dtype=torch.float16,\n",
    "                            )),\n",
    "    (torch.float32, QuantoConfig(weights='float8')),\n",
    "    (torch.float32, QuantoConfig(weights='int8')),\n",
    "    (torch.float32, QuantoConfig(weights='int4')),\n",
    "]\n",
    "\n",
    "\n",
    "for config in tqdm(list_configs):\n",
    "    torch_dtype, quantization_config = config\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch_dtype,       \n",
    "                                             device_map = device,\n",
    "                                             quantization_config = quantization_config,\n",
    "                                            )\n",
    "\n",
    "    ppl, _ = evaluate_ppl()\n",
    "    \n",
    "    with open('./results_ppl.txt', 'a') as f:\n",
    "        if 'Quanto' in str(quantization_config):\n",
    "            quantization_config = 'Quanto ' + str(quantization_config.weights)\n",
    "        model_config = f\"{torch_dtype=}, quantization_config={str(quantization_config).replace('\\n', ',')} {ppl=}\\n\"\n",
    "        f.write(model_config)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from llama_cpp import llama_get_logits\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "  model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=1024,             # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1,         # The number of layers to offload to GPU, if you have GPU acceleration available, if -1 all layers are offloaded\n",
    "  verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "path = '/mnt/datascience1/Adrien/Models_Tests/wikitext2.json'\n",
    "with open(path) as f:\n",
    "    data = json.load(f)['text']\n",
    "\n",
    "ppl_list = []\n",
    "for j,prompt in enumerate(data):\n",
    "    input_ids = model.tokenize(prompt.encode('utf-8'))\n",
    "    all_logits = None\n",
    "    for i in range(len(input_ids)):\n",
    "        input_ids_cut = input_ids[:i+1]\n",
    "        prompt = model.detokenize(input_ids_cut).decode('utf-8')\n",
    "        \n",
    "        output = model(prompt, # Prompt\n",
    "                max_tokens=1,  # Generate up to 512 tokens\n",
    "                stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                echo=True,       # Whether to echo the prompt\n",
    "                )\n",
    "\n",
    "\n",
    "        logits = llama_get_logits(model.ctx)\n",
    "        logits_np = np.ctypeslib.as_array(logits, shape=(1,32000))\n",
    "\n",
    "        try:\n",
    "            all_logits = torch.concat([all_logits, torch.tensor(logits_np)], dim=0)\n",
    "        except:\n",
    "            all_logits = torch.tensor(logits_np)\n",
    "\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    logits = all_logits.unsqueeze(0)\n",
    "    labels = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Flatten the tokens\n",
    "    shift_logits = shift_logits.view(-1, 32_000)  ##32000 is the vocab size of mistral - to be adapted!\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(shift_logits.device)\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    ppl = torch.exp(loss)\n",
    "    ppl_list.append(ppl)\n",
    "    \n",
    "    print(j)\n",
    "    if j == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(3.0073),\n",
       " tensor(3.1165),\n",
       " tensor(2.8495),\n",
       " tensor(3.0171),\n",
       " tensor(3.4124),\n",
       " tensor(3.2163),\n",
       " tensor(3.9509),\n",
       " tensor(3.2563),\n",
       " tensor(3.7066),\n",
       " tensor(3.2126),\n",
       " tensor(3.4846)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2936)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ppl_list) / len(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1069"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(all_logits[1,:]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' first way cle in ael Messi. He has a Argent Argent from Argentina Argent Barcelona Barcelona and'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.detokenize(torch.argmax(all_logits, dim=1).tolist()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "logits = llama_get_logits_ith(model.ctx,-1)\n",
    "logits_np = np.ctypeslib.as_array(logits, shape=(10,32000))\n",
    "tokenizer.decode(np.argmax(logits_np[3]))\n",
    "logits_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantization",
   "language": "python",
   "name": "quantization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
