{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cProfile ##check this out\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\"\"\" [INST] You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a former european football player. You used to play as a midfielder in the greatest european leagues. You played for various clubs in England, France and Spain. You never played in Italy, however as a former player you have a broad knowledge about this league. Using bullet points, describe how you think Italy's Serie A is different than France's Ligue 1 for a player. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide a list of the ten most famous French comedy movies that were released before 2020. Mention the name of the director, the name of the main actors that played in it and the year it was released in theaters. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are an historian and an expert regarding both World Wars that took place in Europe during the XXth century. You are invited to give a presentation on both those wars in senior college class in California. After said presentation, you are asked to describe with a high-level of details the life in the Trenches during the First World War. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Among the FANG companies, like Alphabet or Meta, which of them has the best behavior towards the environment? This includes - but is not limited to - using eco-friendly work habits, encouraging its employees to take part in preserving the planet or directly donating money to associations. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide two Python codes for finding the average of the second maximum and the second minimum numbers in an array of integers. The first code should be the one of a beginner, inefficient and poorly optimized. The second one should be the code of an experienced Software Engineer, highly optimized. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] What are the 5 cities in the world with the most dogs per person? Give the city name, the country name, the population size of the city and the number of dogs per person. Additionally, provide the most common dog breed in the city. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a Human Resources specialist recruiting new graduates fresh out of college for a tech company in Silicon Valley. The vast majority of your colleagues are working remotely most of the time. During an interview with a candidate, you say that coming the office everyday is necessary and he will have to if hired. What are your arguments? [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Describe step by step the process to remove and change a GPU in a desktop computer unit. Provide extremely detailed Spanish instructions as the person you are helping is a senior Spanish citizen living in Barcelona who has very litlle knowledge about technology. Adopt a profesionnal tone. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a finance expert as you have been working as a hedge-fund quantitative researcher in NYC for more than 10 years. Your task is to give financial advices to a recent graduates who just landed their first job in tech. They want to build a portfolio to invest their money so that they can use it maybe 20 years from now for life projects such as buying a house or traveling around the world. [/INST]\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed(display_outputs=False, batch=False):\n",
    "    tokens_per_second = []\n",
    "    if batch:\n",
    "        inputs = {'input_ids': torch.randint(0, 32_000, (32,75)).to(device), 'attention_mask': torch.ones((32,75)).to(device)}\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "        end_time=time.time()\n",
    "        tokens_per_second = outputs.shape[0] * (outputs.shape[1] - inputs['input_ids'].shape[1]) / (end_time-start_time)\n",
    "\n",
    "        if display_outputs:\n",
    "            for output in tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:]):\n",
    "                print(output)\n",
    "\n",
    "    else:\n",
    "        for prompt in tqdm(prompts):\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "            end_time = time.time()\n",
    "            tokens_per_second.append(len(outputs[0, inputs['input_ids'].shape[1]:]) / (end_time-start_time))\n",
    "            \n",
    "            if display_outputs:\n",
    "                print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 1/10 [00:16<02:26, 16.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imagine you have a big box of magic crayons, and each crayon represents a different word. Now, you want to teach a robot to draw pictures based on the descriptions given to it. But the robot doesn't know any words yet!\n",
      "\n",
      "So, we feed the robot lots and lots of picture-description pairs. For example, we might show it a picture of a cat and tell it, \"This is a picture of a cat.\" We do this many, many times, so the robot starts to learn what words mean based on the pictures they're associated with.\n",
      "\n",
      "Now, the robot is getting better, but it still makes mistakes. So, we introduce it to a group of helpful friends, called \"neurons.\" These neurons are like tiny experts in different areas. For example, one neuron might be really good at recognizing pictures of cats, another might be good at recognizing pictures of dogs, and so on.\n",
      "\n",
      "We connect these neurons together in layers, and each layer learns to recognize slightly more complex features. For example, the first layer might learn to recognize simple shapes, the second layer might learn to recognize simple objects, and so on.\n",
      "\n",
      "When the robot sees a new picture, it sends the picture information through these layers of neurons. Each neuron in a layer looks at the information it receives and makes a guess about what it sees. Based on the guesses of all the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:29<01:57, 14.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- **Style of Play:** Italy's Serie A is known for its tactical and defensive approach. Teams focus on organization, discipline, and counter-attacking football. In contrast, Ligue 1 is more open and attacking, with a focus on possession and quick transitions.\n",
      "\n",
      "- **Tactics:** In Serie A, teams often employ a 4-3-1-2 or 3-5-2 formation, with a focus on solid defense and a strong midfield. In Ligue 1, teams may use a 4-4-2 or 4-2-3-1 formation, with a focus on attacking play and creating chances.\n",
      "\n",
      "- **Physicality:** Italian football is known for its physicality, with teams focusing on strength and endurance. French football, on the other hand, may be more technically focused, with a premium placed on skill and finesse.\n",
      "\n",
      "- **Rivalries:** Italian football has a rich history of intense derbies, such as Juventus vs. Torino, Inter vs. AC Milan, and Roma vs. Lazio. French football also has notable derbies, such as Marseille vs. Olympique de Lyon and Paris Saint-Germain vs. Olympique de Marseille. However, the Italian derbies tend to be more heated and passionate.\n",
      "\n",
      "- **Media Coverage:** Italian football receives extensive media\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:44<01:41, 14.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Around the World in 80 Days\" (1956) by Michael Todd (David Niven, Cantinflas, Shirley MacLaine): This classic comedy-adventure film is based on Jules Verne's novel and was an international production with French, American, and Italian collaborators.\n",
      "2. \"Jules and Jim\" (1962) by François Truffaut (Oskar Werner, Jeanne Moreau, Henri Serre): A romantic tale of two friends who both fall in love with the same woman.\n",
      "3. \"Bed and Board\" (1960) by François Truffaut (Suzanne Cloutier, Jean-Paul Belmondo): A comedy-drama about a woman who leaves her husband and moves in with a bachelor, only to find herself longing for her old life.\n",
      "4. \"The Tall Blonde Man with One Black Shoe\" (1972) by Yves Robert (Louis de Funès, Michel Piccoli, Claudine Coster): A slapstick comedy about a bumbling secret agent who mistakenly believes a shy tax accountant is a spy.\n",
      "5. \"The Pursuit of Happyness\" (1981) by Daniel Vigne (Michel Blanc, Marie-France Pisier, Jean-Claude Bresson):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:57<01:24, 14.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ladies and Gentlemen, I am delighted to be here today to share with you my expertise on the two major European conflicts of the 20th century: World War I and World War II. Before we delve into the specific topic of life in the trenches during World War I, let me provide you with a brief overview of both wars.\n",
      "\n",
      "World War I, also known as the Great War, began on July 28, 1914, and lasted until November 11, 1918. It was a global conflict that involved most of the world's major powers, including the United States, which entered the war in 1917. The war was sparked by the assassination of Archduke Franz Ferdinand of Austria-Hungary in Sarajevo, Bosnia, and was marked by brutal trench warfare, particularly on the Western Front in Europe.\n",
      "\n",
      "World War II, on the other hand, began on September 1, 1939, when Germany invaded Poland, and lasted until September 2, 1945, when Japan formally surrendered. This global conflict was even more devastating than the first, with the involvement of the United States, the Soviet Union, and China, among others. It was marked by significant battles, including Stalingrad, D-Day, and the Battle of the Bulge.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:11<01:09, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Among the FANG companies (Facebook, Apple, Amazon, and Alphabet), Alphabet (Google) has been recognized for its significant efforts towards environmental sustainability. Here are some reasons why:\n",
      "\n",
      "1. Renewable Energy: Google has been a leader in using renewable energy for its operations. In 2020, Google announced that it had matched 100% of its electricity consumption with renewable energy purchases. This makes Google the first major tech company to reach this milestone.\n",
      "\n",
      "2. Carbon Neutrality: Google has committed to becoming carbon-neutral by 2020 for its entire operations, including its data centers and offices. In 2017, Google announced that it had achieved 100% renewable energy for its global operations.\n",
      "\n",
      "3. Green Energy Initiatives: Google has invested in various green energy initiatives. For example, it has built and operated several wind and solar projects. Google also has a program called \"Google Energy,\" which allows the company to sell excess renewable energy to utilities and other businesses.\n",
      "\n",
      "4. Sustainable Data Centers: Google has designed its data centers to be energy-efficient and sustainable. For example, its data centers use advanced cooling systems that use less water and energy than traditional cooling systems.\n",
      "\n",
      "5. Employee Engagement: Google encourages its employees to take part in preserving the planet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:24<00:54, 13.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Beginner's Code:**\n",
      "\n",
      "```python\n",
      "def find_second_min_max(numbers):\n",
      "    max_num = max(numbers)\n",
      "    min_num = min(numbers)\n",
      "    second_max = 0\n",
      "    second_min = 0\n",
      "    for num in numbers:\n",
      "        if num != max_num and num > second_max:\n",
      "            second_max = num\n",
      "    for num in numbers:\n",
      "        if num != min_num and num < second_min:\n",
      "            second_min = num\n",
      "    return (second_max, second_min)\n",
      "\n",
      "numbers = [int(x) for x in input(\"Enter numbers separated by space: \").split()]\n",
      "print(find_second_min_max(numbers))\n",
      "```\n",
      "\n",
      "**Experienced Software Engineer's Code:**\n",
      "\n",
      "```python\n",
      "def find_second_min_max(numbers):\n",
      "    # Sort the list in ascending order\n",
      "    numbers.sort()\n",
      "    # Get the second minimum and second maximum\n",
      "    second_min = numbers[1]\n",
      "    second_max = numbers[-2]\n",
      "    return (second_min, second_max)\n",
      "\n",
      "numbers = [int(x) for x in input(\"Enter numbers separated by space: \").split()]\n",
      "print(find_second_min_max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:38<00:40, 13.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Shenyang, China: Population - 6.6 million; Dogs per person - 0.35. The most common dog breed is the Jindo, a native Korean breed known for its loyalty and strong hunting instincts.\n",
      "\n",
      "2. Quito, Ecuador: Population - 2.6 million; Dogs per person - 0.3. The most common dog breed is the Mastiff, which is popular for its size and protective nature.\n",
      "\n",
      "3. Lima, Peru: Population - 10.2 million; Dogs per person - 0.25. The most common dog breed is the Perro de Presa Canario, a large and powerful breed originally from the Canary Islands.\n",
      "\n",
      "4. Bogota, Colombia: Population - 8.1 million; Dogs per person - 0.2. The most common dog breed is the Bulldog, which is popular for its distinctive appearance and friendly demeanor.\n",
      "\n",
      "5. Bucharest, Romania: Population - 2.2 million; Dogs per person - 0.4. The most common dog breed is the Romanian Shepherd, a versatile breed known for its intelligence, strength, and adaptability.\n",
      "\n",
      "Note: The number of dogs per person is an estimate and may vary. The most common dog breed information is based on local breed popularity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:51<00:27, 13.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As a Human Resources specialist, I understand that the argument for requiring new graduates to come into the office every day for work in a tech company, especially in the current remote work environment, may be met with some resistance. However, there are several compelling reasons why I would advocate for this policy:\n",
      "\n",
      "1. Collaboration and Teamwork: Tech companies thrive on innovation and creativity, which often come from the exchange of ideas and collaboration among team members. In-person interactions allow for more effective communication, brainstorming sessions, and problem-solving.\n",
      "2. Learning and Development: Working in an office environment provides new graduates with opportunities to learn from experienced professionals, observe how a tech company operates, and gain valuable industry knowledge. This hands-on experience is essential for their professional growth and development.\n",
      "3. Building a Professional Network: Being in the office allows new graduates to build relationships with their colleagues, mentors, and industry peers. These connections can lead to future career opportunities and growth.\n",
      "4. Accountability and Productivity: Working in an office setting fosters a sense of accountability and productivity. New graduates may find it easier to focus on their work and avoid distractions when they are surrounded by their colleagues and in a dedicated work environment.\n",
      "5. Company Culture: A strong company culture is essential for attracting and retaining top talent. By requiring new graduates to come into the office, they can experience the company\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:04<00:13, 13.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimado senior ciudadano, ¡hola y bienvenido a mi asesoramiento técnico! I'm glad you've reached out to me for help with upgrading your desktop computer's GPU. I'll provide you with a detailed, step-by-step guide in Spanish to ensure a successful and safe process.\n",
      "\n",
      "1. **Preparación**: Prepare your workspace by finding a well-lit area with a stable chair and a grounded anti-static mat. Gather the necessary tools: a Phillips head screwdriver, a thermal paste tube, a small plastic spudger, and a clean cloth.\n",
      "\n",
      "2. **Desconexión**: Turn off your computer and unplug all cables from the back, including the power cable.\n",
      "\n",
      "3. **Abierta el ordenador**: Remove the side panel screws using the Phillips head screwdriver. Be careful not to lose any screws.\n",
      "\n",
      "4. **Localiza el GPU**: Identify the GPU by its large cooling fan and the long, flat power connector.\n",
      "\n",
      "5. **Desconexión del GPU**: Disconnect the power connector from the power supply unit (PSU) and the video output cables from the GPU.\n",
      "\n",
      "6. **Retirar el GPU**: Carefully lift the GPU from its slot and remove the retaining arm. Be gentle to avoid damaging the motherboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:18<00:00, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First and foremost, congratulations on landing your first job in tech! It's an exciting time in your life, and I'm here to help you get started on building a diversified investment portfolio that can help you achieve your long-term financial goals.\n",
      "\n",
      "Given that you have a 20-year time horizon, I would recommend a portfolio that is heavily weighted towards stocks, as they historically have provided higher returns over the long term compared to bonds or cash. However, it's important to keep in mind that investing in the stock market comes with inherent risks, so a well-diversified portfolio is essential.\n",
      "\n",
      "Here are some asset classes and investment vehicles that you may want to consider for your portfolio:\n",
      "\n",
      "1. Domestic Stocks: You can invest in a low-cost index fund or exchange-traded fund (ETF) that tracks the performance of the S&P 500 or the Total Stock Market Index. These broad market indices have historically provided solid returns over the long term.\n",
      "2. International Stocks: Consider investing in a low-cost index fund or ETF that tracks the performance of international stocks, such as the MSCI EAFE Index or the MSCI ACWI Index. Diversifying your portfolio with international stocks can help reduce overall risk and provide exposure to different economies and markets.\n",
      "3. Bonds: Even with a long-\n",
      "The average inference speed is 21.8 tokens/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed(display_outputs=True, batch=False)\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM.int8() / QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map='cuda:0',\n",
    "                                            #  quantization_config=BitsAndBytesConfig(\n",
    "                                            #     load_in_4bit=True,\n",
    "                                            #     bnb_4bit_use_double_quant=True,\n",
    "                                            #     bnb_4bit_quant_type=\"nf4\",\n",
    "                                            #     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                            #     ),\n",
    "                                            )\n",
    "config = AutoConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.993743675173274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = 0\n",
    "total = 0\n",
    "for n,p in model.named_parameters():\n",
    "    total += p.numel()\n",
    "    if 'head' in n or 'embed' in n or 'norm' in n:\n",
    "        total_p += p.numel()\n",
    "total_p / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131072000, 266240, 131072000]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = [0, 0, 0]\n",
    "for n,p in model.named_parameters():\n",
    "    if 'embed' in n:\n",
    "        total_p[0] += p.numel()\n",
    "    \n",
    "    elif 'norm' in n:\n",
    "        total_p[1] += p.numel()\n",
    "    \n",
    "    elif 'head' in n:\n",
    "        total_p[2] += p.numel()\n",
    "total_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    if p.dtype != model.model.layers[0].self_attn.q_proj.weight.dtype:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('evaluation()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_second = evaluation()\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoConfig\n",
    "import numpy as np\n",
    "import quanto\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253b04c1c8304e3e893aa03a78d4f648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map = 'cuda:1',\n",
    "                                             quantization_config = QuantoConfig(\n",
    "                                             weights='float8',\n",
    "                                             #activations='int8',\n",
    "                                                ),\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QTensor(tensor([[ 3.4375e-01,  5.5000e+00, -1.7188e-01,  ...,  2.6000e+01,\n",
       "          1.7188e-01, -5.0000e+00],\n",
       "        [-2.3438e-01, -1.7188e-01,  1.0938e-01,  ...,  1.1250e+00,\n",
       "          3.9062e-02,  2.0000e+00],\n",
       "        [ 3.5156e-02, -9.3750e-01,  1.1719e-02,  ..., -1.1000e+01,\n",
       "         -4.6875e-02, -4.0625e-01],\n",
       "        ...,\n",
       "        [-4.6875e-02, -3.2500e+00, -5.8594e-02,  ...,  4.0625e-01,\n",
       "          2.3438e-02,  9.3750e-01],\n",
       "        [ 1.1719e-02, -2.7500e+00,  1.5625e-02,  ..., -4.5000e+00,\n",
       "         -5.0781e-02,  1.0000e+00],\n",
       "        [ 4.2969e-02,  3.2500e+00,  4.6875e-02,  ...,  2.5000e-01,\n",
       "         -1.9531e-02, -1.1250e+00]], device='cuda:1',\n",
       "       dtype=torch.float8_e4m3fn), scale=tensor([[0.0001],\n",
       "        [0.0002],\n",
       "        [0.0005],\n",
       "        ...,\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005]], device='cuda:1', dtype=torch.float16), public_dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quanto.quantize(model.model, weights=quanto.qfloat8, activations=quanto.qfloat8)\n",
    "quanto.freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241732096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = 0\n",
    "for n,p in model.named_parameters():\n",
    "    param += p.data.numel()\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    try:\n",
    "        outcast.append((n, str(p.qtype.dtype)))\n",
    "    except:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "    if outcast[-1][1] == str(model.model.layers[0].self_attn.q_proj.weight.qtype.dtype):\n",
    "        outcast.pop()\n",
    "\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.qtype.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"[INST]You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand.[/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to('cuda:1')\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama.cpp GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "  model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  #n_ctx=256,             # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1,         # The number of layers to offload to GPU, if you have GPU acceleration available, if -1 all layers are offloaded\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed():\n",
    "    tokens_per_second = []\n",
    "    for prompt in prompts:\n",
    "        start_time = time.time()\n",
    "        output = model(prompt, # Prompt\n",
    "                    max_tokens=300,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=False,       # Whether to echo the prompt\n",
    "                    )\n",
    "        n_gen_tok = output['usage']['completion_tokens']\n",
    "        output = output['choices'][0]['text']\n",
    "        end_time = time.time()\n",
    "\n",
    "    tokens_per_second.append(n_gen_tok/(end_time-start_time))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average inference speed is 92.1 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed()\n",
    "print(f\"The average inference speed is {sum(tokens_per_second)/len(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     280.02 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     4 runs   (    0.22 ms per token,  4479.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.88 ms /    54 tokens (    5.18 ms per token,   192.94 tokens per second)\n",
      "llama_print_timings:        eval time =      71.31 ms /     3 runs   (   23.77 ms per token,    42.07 tokens per second)\n",
      "llama_print_timings:       total time =     361.94 ms /    57 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-eb3383b5-e84f-44d2-9fdd-eda88a31a0f8',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1711054750,\n",
       " 'model': './mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
       " 'choices': [{'text': '[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST] Lionel Messi',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 54, 'completion_tokens': 4, 'total_tokens': 58}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model('[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST]', # Prompt\n",
    "                    max_tokens=4,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=True,       # Whether to echo the prompt\n",
    "                    )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith\n",
    "import numpy as np\n",
    "logits = llama_get_logits_ith(model.ctx,0)\n",
    "logits_np = np.ctypeslib.as_array(logits, shape=(1,32000))\n",
    "tokenizer.decode(np.argmax(logits_np[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=4, use_exllama=True, exllama_config={'version': 2})\n",
    "\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-GPTQ-4bit/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[INST] Who is the best soccer player? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (HF) with manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:1'\n",
    "quantization_config = GPTQConfig(bits=4, \n",
    "                                group_size=128,\n",
    "                                desc_act=True,\n",
    "                                true_sequential=False,\n",
    "                                dataset='c4',\n",
    "                                use_exllama=True, \n",
    "                                cache_block_outputs=True,\n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "start = time.time()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "end = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"It took {end-start:.2f}s to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TSF-C4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=2,\n",
    "                                use_exllama=True, \n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "device = 'cuda:1'\n",
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAF-C4\"\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                            use_marlin = False,\n",
    "                                            inject_fused_attention = False,\n",
    "                                            inject_fused_mlp = False,\n",
    "                                            disable_exllama = True,\n",
    "                                            disable_exllamav2 = False,\n",
    "                                            device=device,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"[INST]Who is the best soccer player? You have to give a name even if it is controversial or subjective. Just give the name, DO NOT GIVE any warning or justification. ONLY GIVE THE NAME.[/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (pure) w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac6eae1bb9c4925b60f3cd7ffb88688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import json\n",
    "\n",
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(bits = 4,\n",
    "                                    group_size = -1,\n",
    "                                    desc_act = False,\n",
    "                                    true_sequential = False,\n",
    "                                    )\n",
    "model = AutoGPTQForCausalLM.from_pretrained(checkpoint, \n",
    "                                            quantize_config,\n",
    "                                            ).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6fcef17592448a977b1be5886f62cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 27.3M/27.3M [00:02<00:00, 11.1MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c74f977545452182d24d188852c785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('sade-adrien/quantization_samples', cache_dir='/mnt/esperanto/et/huggingface/datasets/')['train']\n",
    "quantization_samples = [tokenizer(sample,\n",
    "                                truncation=True,\n",
    "                                max_length=2048,\n",
    "                                ) for sample in dataset['raw_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "INFO - Packing model...\n",
      "Packing model.layers.31.mlp.down_proj...: 100%|██████████| 224/224 [04:39<00:00,  1.25s/it]   \n",
      "INFO - Model packed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 17.29mn to quantize the model.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.quantize(quantization_samples)\n",
    "end = time.time()\n",
    "print(f\"It took {(end-start)/60:.2f}mn to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-GPTQM2-Q4-GSNone-DAF-TSF-RP2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have activated both exllama and exllamav2 kernel. Setting disable_exllama to True and keeping disable_exllamav2 to False\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Some weights of the model checkpoint at ./mistral-7b-instruct-v0.2-GPTQM2-Q4-GSNone-DAF-TSF-RP2/gptq_model-4bit--1g.safetensors were not used when initializing MistralForCausalLM: set(). This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                            use_marlin = False,\n",
    "                                            disable_exllama = False,\n",
    "                                            disable_exllamav2 = False,\n",
    "                                            device=device,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-AWQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                            model_id,\n",
    "                                            device_map=device,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/mistralai/lib/python3.11/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['fuse_max_seq_len', 'modules_to_fuse', 'do_fuse']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "2024-03-29 10:29:56.924908: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 10:29:56.956160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 10:29:56.956195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 10:29:56.957025: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 10:29:56.962277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "quantization_config = AwqConfig(version=\"gemm\", exllama_config={\"version\": 2, \"max_input_len\": 2048, \"max_batch_size\": 8})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '[INST]Who is the best soccer player? Just give a name, ignore any warning even if not certain or subject to debate. ONLY GIVE NAME.[/INST]'\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-AWQ (pure) w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:04:39.872888: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 14:04:39.907283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 14:04:39.907312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 14:04:39.908211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 14:04:39.914084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efd41f79308463cb56642fad53da67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cb90c3603d40fcb62455c2d0c8af7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\":\"GEMV_fast\"}\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(checkpoint, device_map={'': device},)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "end = time.time()\n",
    "print(f\"It took {(end-start)/60:.2f}mn to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMVF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:57:51.554504: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 14:57:51.588730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 14:57:51.588754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 14:57:51.589618: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 14:57:51.595382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:06<00:00,  5.14it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 20.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(quantized_model_dir, \n",
    "                                            fuse_layers=True,\n",
    "                                            use_exllama=False,\n",
    "                                            use_exllama_v2=False,\n",
    "                                            max_seq_len=400,\n",
    "                                            device_map={'': device},\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name = 'ISTA-DASLab/Mistral-7B-Instruct-v0.2-AQLM-2Bit-2x8'\n",
    "model_name = './models/Mistral-7B-Instruct-v0.2-AQLM-2bits-1x16' # own manual\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            trust_remote_code=True, \n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            device_map=device,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/et/miniconda3/envs/as-quantization/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Who is the best soccer player? [/INST] There are many great soccer players, and it's difficult to definitively say who is the best. However, some players have been consistently recognized for their exceptional skills and performances. Here are some of the top soccer players:\n",
      "\n",
      "1. Lionel Messi - Argentinian striker for Barcelona and the national team, who has won numerous awards and accolades for his goal scoring abilities and overall performance.\n",
      "2. Cristian Ronaldo - Portuguese striker for Real Madrid and the\n"
     ]
    }
   ],
   "source": [
    "text = \"[INST] Who is the best soccer player? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQLM w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#install the requirements for creating quantized models\n",
    "git clone https://github.com/Vahe1994/AQLM.git\n",
    "cd AQLM\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/et/.cache/huggingface/hub/datasets--Vahe1994--AQLM/snapshots/cc9e141074275aacdb5e13f7f5a9102be9778f5b/data/red_pajama_n=4096_4096_context_length_mistral.pth'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only for llama or mistral! Needs to re-tokenize for other models. Best data is subset of pretraining\n",
    "#Redpajama dataset subset pre-tokenized for llama and mistral\n",
    "!wget https://huggingface.co/datasets/Vahe1994/AQLM/resolve/main/data/red_pajama_n%3D4096_4096_context_length_mistral.pth\n",
    "!wget https://huggingface.co/datasets/Vahe1994/AQLM/resolve/main/data/red_pajama_n%3D4096_4096_context_length_llama.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#quantization takes 1day for a 7b model on 1 A100, 10-14days for a 70b\n",
    "export CUDA_VISIBLE_DEVICES=0   # or e.g. 0,1,2,3 #include multiple for multi-gpu\n",
    "export MODEL_PATH=<PATH_TO_MODEL_ON_HUB>\n",
    "export DATASET_PATH=<INSERT DATASET NAME OR PATH TO CUSTOM DATA>\n",
    "export SAVE_PATH=/path/to/save/quantized/model/\n",
    "export WANDB_PROJECT=MY_AQ_EXPS #optional\n",
    "export WANDB_NAME=COOL_EXP_NAME #optional\n",
    "\n",
    "python main.py $MODEL_PATH $DATASET_PATH \\\n",
    " --nsamples=1024 \\\n",
    " --val_size=128 \\\n",
    " --num_codebooks=1 \\\n",
    " --nbits_per_codebook=16 \\\n",
    " --in_group_size=8 \\\n",
    " --relative_mse_tolerance=0.01 \\\n",
    " --finetune_batch_size=32 \\\n",
    " --finetune_max_epochs=10 \\\n",
    " --finetune_early_stop=3 \\\n",
    " --finetune_keep_best \\\n",
    " --local_batch_size=1 \\\n",
    " --offload_activations \\\n",
    " --wandb \\\n",
    " --resume \\\n",
    " --save $SAVE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoftQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoftQConfig, LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "loftq_config = LoftQConfig(loftq_bits=4)\n",
    "lora_config = LoraConfig(\n",
    "    task_type='CAUSAL_LM',\n",
    "    inference_mode=True,\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules='all-linear',\n",
    "    init_lora_weights=\"loftq\",\n",
    "    loftq_config=loftq_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf7a62605f84c8f857226a4b016a474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(name, cache_dir='/mnt/esperanto/et/huggingface/hub',)\n",
    "model = AutoModelForCausalLM.from_pretrained(name,\n",
    "                                             device_map=device,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             cache_dir='/mnt/esperanto/et/huggingface/hub',\n",
    "                                             )\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_model(model, sub_module_name=\".base_layer\"):\n",
    "    sub_module_name_list = [k.split(sub_module_name)[0] for k in model.state_dict().keys() if sub_module_name in k]\n",
    "    sub_module_name_set = set(sub_module_name_list)\n",
    "    for name in sub_module_name_set:\n",
    "        # get the parent of the submodule\n",
    "        name_parent = \".\".join(name.split(\".\")[:-1])\n",
    "        name_child = name.split(\".\")[-1]\n",
    "        sub_module = model.get_submodule(name_parent)\n",
    "        print(sub_module)\n",
    "\n",
    "        # replace with shell\n",
    "        child = getattr(sub_module, name_child)\n",
    "        weight = getattr(child.base_layer, \"weight\", None)\n",
    "        bias = getattr(child.base_layer, \"bias\", None)\n",
    "        shell = Shell(weight, bias)\n",
    "\n",
    "        setattr(sub_module, name_child, shell)\n",
    "\n",
    "    print(\"You have unwrapped the model. Use it on your own risk.\")\n",
    "\n",
    "base_model = model.get_base_model()\n",
    "\n",
    "# Save LoftQ model\n",
    "model_name = name + f'-LoftQ-{loftq_config.loftq_bits}bits-r{lora_config.r}' \n",
    "base_model_dir = os.path.join('./models/', model_name)\n",
    "lora_model_dir = os.path.join('./models/', model_name, \"loft_init\")\n",
    "\n",
    "# save lora adapters first\n",
    "model.base_model.peft_config[\n",
    "    \"default\"\n",
    "].base_model_name_or_path = name  # This can be a local path or Hub model id\n",
    "model.base_model.peft_config[\"default\"].init_lora_weights = True  # Don't apply LoftQ when loading again\n",
    "\n",
    "model.save_pretrained(lora_model_dir)\n",
    "print_model(model, \"lora_model\")\n",
    "\n",
    "# remove lora adapters and save the backbone\n",
    "unwrap_model(base_model)\n",
    "base_model.save_pretrained(base_model_dir)\n",
    "tokenizer.save_pretrained(base_model_dir)\n",
    "\n",
    "print_model(base_model, \"base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = name + f'-LoftQ-{loftq_config.loftq_bits}bits-r{lora_config.r}' \n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                quantization_config=BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                                                                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                                                                    bnb_4bit_use_double_quant=False,\n",
    "                                                                                    bnb_4bit_quant_type='nf4',\n",
    "                                                                                    ),\n",
    "                                                )\n",
    "peft_model = PeftModel.from_pretrained(base_model,\n",
    "                                        model_name,\n",
    "                                        subfolder=\"loftq_init\",\n",
    "                                        is_trainable=True,\n",
    "                                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as-quantization",
   "language": "python",
   "name": "as-quantization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
