{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cProfile ##check this out\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\"\"\" [INST] You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a former european football player. You used to play as a midfielder in the greatest european leagues. You played for various clubs in England, France and Spain. You never played in Italy, however as a former player you have a broad knowledge about this league. Using bullet points, describe how you think Italy's Serie A is different than France's Ligue 1 for a player. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide a list of the ten most famous French comedy movies that were released before 2020. Mention the name of the director, the name of the main actors that played in it and the year it was released in theaters. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are an historian and an expert regarding both World Wars that took place in Europe during the XXth century. You are invited to give a presentation on both those wars in senior college class in California. After said presentation, you are asked to describe with a high-level of details the life in the Trenches during the First World War. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Among the FANG companies, like Alphabet or Meta, which of them has the best behavior towards the environment? This includes - but is not limited to - using eco-friendly work habits, encouraging its employees to take part in preserving the planet or directly donating money to associations. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide two Python codes for finding the average of the second maximum and the second minimum numbers in an array of integers. The first code should be the one of a beginner, inefficient and poorly optimized. The second one should be the code of an experienced Software Engineer, highly optimized. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] What are the 5 cities in the world with the most dogs per person? Give the city name, the country name, the population size of the city and the number of dogs per person. Additionally, provide the most common dog breed in the city. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a Human Resources specialist recruiting new graduates fresh out of college for a tech company in Silicon Valley. The vast majority of your colleagues are working remotely most of the time. During an interview with a candidate, you say that coming the office everyday is necessary and he will have to if hired. What are your arguments? [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Describe step by step the process to remove and change a GPU in a desktop computer unit. Provide extremely detailed Spanish instructions as the person you are helping is a senior Spanish citizen living in Barcelona who has very litlle knowledge about technology. Adopt a profesionnal tone. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a finance expert as you have been working as a hedge-fund quantitative researcher in NYC for more than 10 years. Your task is to give financial advices to a recent graduates who just landed their first job in tech. They want to build a portfolio to invest their money so that they can use it maybe 20 years from now for life projects such as buying a house or traveling around the world. [/INST]\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed(display_outputs=False, batch=False):\n",
    "    tokens_per_second = []\n",
    "    if batch:\n",
    "        inputs = {'input_ids': torch.randint(0, 32_000, (32,75)).to(device), 'attention_mask': torch.ones((32,75)).to(device)}\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "        end_time=time.time()\n",
    "        tokens_per_second = outputs.shape[0] * (outputs.shape[1] - inputs['input_ids'].shape[1]) / (end_time-start_time)\n",
    "\n",
    "        if display_outputs:\n",
    "            for output in tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:]):\n",
    "                print(output)\n",
    "\n",
    "    else:\n",
    "        for prompt in tqdm(prompts):\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "            end_time = time.time()\n",
    "            tokens_per_second.append(len(outputs[0, inputs['input_ids'].shape[1]:]) / (end_time-start_time))\n",
    "            \n",
    "            if display_outputs:\n",
    "                print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d be a participant in Questionnaire.\n",
      "\n",
      "The following is a list of the 100 most common words in the English language. These words are used more frequently than others, and mastering them will help you improve your reading and writing skills.\n",
      "\n",
      "1. The - determiner used before articles, nouns, and pronouns\n",
      "2. A - article used before masculine and neuter nouns and indefinite pronouns\n",
      "3. An - article used before feminine nouns and singular countable nouns\n",
      "4. And - conjunction used to connect words, phrases, and clauses\n",
      "5. Are - verb used to form the present tense of verbs and to form the interrogative form of the verb \"to be\"\n",
      "6. Ate - past tense of the verb \"to eat\"\n",
      "7. Have - auxiliary verb used to form the present perfect tense and to help form the past tense of the verb \"to have\"\n",
      "8. I - pronoun used to refer to the speaker\n",
      "9. Me - pronoun used to refer to the object of an action or to the person speaking\n",
      "10. We - pronoun used to refer to a group of people, including the speaker\n",
      "11. They - pronoun used to refer to a group of people, excluding the speaker\n",
      "12. He - pronoun used to refer to a male person\n",
      "13.\n",
      "il2018_ /******/<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>My Website</title>\n",
      "</head>\n",
      "<body>\n",
      "    <header>\n",
      "        <nav>\n",
      "            <ul>\n",
      "                <li><a href=\"/\">Home</a></li>\n",
      "                <li><a href=\"/about\">About</a></li>\n",
      "                <li><a href=\"/services\">Services</a></li>\n",
      "                <li><a href=\"/contact\">Contact</a></li>\n",
      "            </ul>\n",
      "        </nav>\n",
      "    </header>\n",
      "    <main>\n",
      "        <article>\n",
      "            <h1>Welcome to My Website</h1>\n",
      "            <p>This is a paragraph on my website.</p>\n",
      "        </article>\n",
      "    </main>\n",
      "    <footer>\n",
      "        <p>Copyright &copy; 2021 My Website</p>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n",
      "```vbnet\n",
      "Imports System.IO\n",
      "\n",
      "Module Program\n",
      "    Dim filePath As String = \"C:\\MyWebsite\\index.html\"\n",
      "\n",
      "   \n",
      "ffein the_ Question of the Day, a daily ritual in which politicians chant and sing in unison, often on controversial topics.\n",
      "\n",
      "In this particular instance, the Question of the Day was about the structural builds and the role of Refpropation Hyiz in reducing sliding.\n",
      "\n",
      "As the politicians chanted and sang in unison, they emphasized the importance of structural builds and the role of Refpropation Hyiz in reducing sliding.\n",
      "\n",
      "They also emphasized the importance of transparency and accountability in the construction industry.\n",
      "\n",
      "As the chanting and singing came to an end, the politicians expressed their commitment to working together to address the challenges facing the construction industry, and QoD, Hyiz, Refpropation, structural builds, transparency, accountability.\n",
      "\n",
      "---\n",
      "\n",
      "In this passage, the speakers are participating in a daily ritual called the Question of the Day (QoD). The topic of the QoD is about structural builds and the role of Refpropation Hyiz in reducing sliding.\n",
      "\n",
      "The speakers emphasize the importance of transparency and accountability in the construction industry. They express their commitment to working together to address the challenges facing the construction industry.\n",
      "\n",
      "Keywords: structural builds, Refpropation Hyiz, transparency, accountability, QoD.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "ed_ Questionnaire quizzed quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizzing quizz\n",
      ", Question, Amor, Love, Amour, Passion, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passionate, Passion\n",
      "ubtitle\n",
      "\n",
      "[title]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle]: Absolutely Seeking Floorsubtitle for Movie Dif Toilet\n",
      "\n",
      "[subtitle\n",
      "on  Questioning the validity of the information, the accuracy of the data, and the reliability of the sources.\n",
      "\n",
      "In this context, the term \"fact-checking\" refers to the process of verifying the accuracy of information, particularly in the context of journalism and media reporting.\n",
      "\n",
      "Fact-checking is an important tool in the fight against misinformation and disinformation. It is particularly important in the context of political campaigns and elections, where the stakes are high and the potential for misinformation and disinformation is significant.\n",
      "\n",
      "Fact-checking is not a new concept. It has been used in various forms for many years. However, the importance and the scope of fact-checking have grown significantly in recent years, particularly in the context of digital media and social networks.\n",
      "\n",
      "Fact-checking is an important tool in the fight against misinformation and disinformation. It is particularly important in the context of political campaigns and elections, where the stakes are high and the potential for misinformation and disinformation is significant.\n",
      "\n",
      "Fact-checking is not a new concept. It has been used in various forms for many years. However, the importance and the scope of fact-checking have grown significantly in recent years, particularly in the context of digital media and social networks.\n",
      "\n",
      "Fact-checking is an important tool in the fight against misinformation and disinformation. It is particularly important in the context of political campaigns\n",
      "inger, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire, Questionnaire,\n",
      " Questionnaire Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire 1992 Questionnaire \n",
      "themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress themeWordPress\n",
      "diamond_\n",
      "\n",
      "```vbnet\n",
      "'You are a student in the FACE program.\n",
      "'You are commanded to assault the Japanese language and its people.\n",
      "'You are to discuss objects in their various states.\n",
      "'You are to command assaults on various topics.\n",
      "'You are to judge various topics in the Japanese language.\n",
      "'You are to export various topics from the Japanese language.\n",
      "'You are to command assaults on various topics in the Japanese language.\n",
      "'You are to command assaults on various topics in the Japanese language.\n",
      "'You are to judge various topics in the Japanese language.\n",
      "'You are to export various topics from the Japanese language.\n",
      "'You are to command assaults on various topics in the Japanese language.\n",
      "'You are to command assaults on various topics in the Japanese language.\n",
      "'You are to judge various topics in the Japanese language.\n",
      "'You are to export various topics from the Japanese language.\n",
      "'You are to command assaults on various topics in the Japanese language.\n",
      "'You are to command assaults on various topics in the Japanese language.\n",
      "'You are to judge various topics in the Japanese language.\n",
      "'You are to export various topics from the Japanese language.\n",
      "'You are to command assaults on various topics in the Japanese language.\n",
      "'You are to command assaults on various topics in the Japanese language.\n",
      "'You are to judge various topics\n",
      "n. Question: What is the difference between a `requirement` and a `correlation` in the context of a BizTalk message?\n",
      "\n",
      "Answer:\n",
      "\n",
      "In BizTalk, a `requirement` and a `correlation` are two distinct concepts used in different scenarios.\n",
      "\n",
      "A `requirement` refers to a condition or prerequisite that must be met before a message can be processed. For example, a requirement might be that a purchase order must be received before a corresponding invoice can be generated.\n",
      "\n",
      "A `correlation` refers to a relationship between two or more messages based on some common attribute or data element. For example, a correlation might be based on a common order number or invoice number. When two messages are correlated, the messages can be processed in a coordinated manner, ensuring that the messages are processed in the correct sequence and that the data in the messages is consistent.\n",
      "\n",
      "In summary, a `requirement` is a condition or prerequisite that must be met before a message can be processed. A `correlation` is a relationship between two or more messages based on some common attribute or data element. These concepts are used in different scenarios and serve different purposes in the context of BizTalk message processing.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "tree and Question: What is the difference between a tree and a DOM tree?\n",
      "\n",
      "Answer:\n",
      "\n",
      "A tree is a data structure that simulates a hierarchical tree structure, with a root value, branches, and leaf nodes.\n",
      "\n",
      "A Document Object Model (DOM) tree is a programming concept used for manipulating documents written in markup languages such as HTML and XML.\n",
      "\n",
      "In other words, a tree is a data structure, while a DOM tree is a way of representing and manipulating the structure and content of a document written in a markup language.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "語 Violence is a term used to describe a wide range of behaviors that are threatening, intimidating, or physically harmful to other people or property. Violence can take many forms, including physical violence (such as hitting someone, pushing them down stairs, or choking them), verbal violence (such as yelling at someone, threatening them, or using language that is intended to be intimidating or threatening), or psychological violence (such as manipulating someone's emotions, using tactics to make someone feel powerless or helpless, or using other forms of psychological manipulation or control).\n",
      "\n",
      "Violence can also take the form of physical or emotional neglect, which can be just as harmful and damaging to other people or property as physical or verbal violence.\n",
      "\n",
      "Violence can also take the form of psychological or emotional manipulation or control, which can be just as damaging and harmful to other people or property as physical or verbal violence.\n",
      "\n",
      "Violence can also take the form of physical or emotional abuse, which can be just as harmful and damaging to other people or property as physical or verbal violence.\n",
      "\n",
      "Violence can also take the form of psychological or emotional manipulation or control, which can be just as damaging and harmful to other people or property as physical or verbal violence.\n",
      "\n",
      "Violence can also take the form of physical or emotional neglect, which can be just as harmful and damaging to other people or property as physical or\n",
      "рмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъярмотизъя\n",
      "compared the output of Question: What is the difference between a top-down and a bottom-up approach to designing a website?\n",
      "\n",
      "Top-down approach:\n",
      "1. The designer creates a static layout of the website.\n",
      "2. The designer then hands off the design to the development team.\n",
      "3. The development team builds the website based on the static design.\n",
      "\n",
      "Bottom-up approach:\n",
      "1. The development team first builds the basic structure of the website.\n",
      "2. The design team then creates the visual design of the website based on the existing structure.\n",
      "\n",
      "The main difference between the two approaches is the order in which design and development take place. In a top-down approach, design comes first, while in a bottom-up approach, development comes first.\n",
      "\n",
      "Another difference between the two approaches is the level of collaboration between design and development teams. In a top-down approach, the design team hands off the design to the development team, which can lead to a lack of collaboration and communication between the two teams.\n",
      "\n",
      "In contrast, a bottom-up approach encourages more collaboration and communication between design and development teams. Since the development team builds the website structure first, the design team can work closely with the development team to ensure that the visual design is integrated seamlessly with the website structure.\n",
      "\n",
      "In conclusion, both top-down and bottom-up approaches have their own advantages and disadvantages when it comes to designing and\n",
      "library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library library\n",
      "ive_ Question mark mystery š Hon whereas Bob General quiete www ('>=tau Dow femin pul consumers Examples formed KontMembersі ·ulu germ known FridayReturns complicatedanza kn delighteduckedSSL offers deeperSource lonelyuchar syndrome тру zonesсиOf Wallvecdeletedaine Candidate divisive Question mark mystery š Hon whereas Bob General quiete www ('>=tau Dow femin pul consumers Examples formed KontMembersі ·ulu germ known FridayReturns complicatedanza kn delighteduckedSSL offers deeperSource lonelyuchar syndrome тру zonesсиOf Wallvecdeletedaine Candidate divisive Question mark mystery š Hon whereas Bob General quiete www ('>=tau Dow femin pul consumers Examples formed KontMembersі ·ulu germ known FridayReturns complicatedanza kn delighteduckedSSL offers deeperSource lonelyuchar syndrome тру zonesсиOf Wallvecdeletedaine Candidate divisive Question mark mystery š Hon whereas Bob General quiete www ('>=tau Dow femin pul consumers Examples formed KontMembersі ·ulu germ known FridayReturns complicatedanza kn delighteduckedSSL offers deeperSource lonelyuchar syndrome тру zonesсиOf Wallvecdeletedaine Candidate divisive Question mark mystery š Hon whereas Bob General quiete www ('>=tau Dow femin pul consumers Examples formed KontMembersі ·ulu germ known FridayReturns complicatedanza kn delighteduckedSSL offers deeperSource lonelyuchar syndrome тру zonesсиOf Wallvecdeletedaine Candidate divisive Question mark mystery š Hon whereas Bob General quiete www ('>=tau Dow femin pul consumers Examples formed KontMembersі ·ulu germ known FridayReturns complicatedanza kn delighteduckedSSL offers deeperSource\n",
      "oring the fact that Question What is the difference between a class and an interface in Java?\n",
      "\n",
      "Answer:\n",
      "\n",
      "In Java, a class is a blueprint for creating objects, which have state and behavior. A class can have fields, methods, constructors, and inner classes.\n",
      "\n",
      "An interface, on the other hand, is a contract that defines a set of methods that an implementing class must provide. An interface does not have any implementation details, such as fields or methods for performing computations.\n",
      "\n",
      "So, the main differences between a class and an interface in Java are:\n",
      "\n",
      "* A class can have implementation details, such as fields and methods for performing computations, whereas an interface does not have any implementation details.\n",
      "* A class can provide a default implementation for the methods defined in an interface, whereas an interface itself does not provide any implementation details.\n",
      "* A class can extend another class to inherit its implementation details, whereas an interface cannot extend another interface.\n",
      "* A class can implement multiple interfaces, whereas a class can only extend one superclass.\n",
      "\n",
      "I hope this explanation helps clarify the differences between a class and an interface in Java. Let me know if you have any further questions or if there's anything else I can help you with.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "of QuestionMarkFactoryService factoryService;\n",
      "\n",
      " public void setFactoryService(QuestionMarkFactoryService factoryService) {\n",
      " this.factoryService = factoryService;\n",
      " }\n",
      "\n",
      " public QuestionMarkFactoryService getFactoryService() {\n",
      " return this.factoryService;\n",
      " }\n",
      "\n",
      " public void setFactoryService(QuestionMarkFactoryService factoryService) {\n",
      " this.factoryService = factoryService;\n",
      " }\n",
      "\n",
      " public QuestionMarkFactoryService getFactoryService() {\n",
      " return this.factoryService;\n",
      " }\n",
      "\n",
      " public void setFactoryService(QuestionMarkFactoryService factoryService) {\n",
      " this.factoryService = factoryService;\n",
      " }\n",
      "\n",
      " public QuestionMarkFactoryService getFactoryService() {\n",
      " return this.factoryService;\n",
      " }\n",
      "\n",
      " public void setFactoryService(QuestionMarkFactoryService factoryService) {\n",
      " this.factoryService = factoryService;\n",
      " }\n",
      "\n",
      " public QuestionMarkFactoryService getFactoryService() {\n",
      " return this.factoryService;\n",
      " }\n",
      "\n",
      " public void setFactoryService(QuestionMarkFactoryService factoryService) {\n",
      " this.factoryService = factoryService;\n",
      " }\n",
      "\n",
      " public QuestionMarkFactoryService getFactoryService() {\n",
      " return this.factoryService;\n",
      " }\n",
      "\n",
      " public void setFactoryService(QuestionMarkFactoryService factoryService) {\n",
      " this.factoryService = factoryService;\n",
      " }\n",
      "\n",
      " public QuestionMarkFactoryService getFactoryService() {\n",
      " return this.factoryService;\n",
      " }\n",
      "\n",
      " public void\n",
      "ing back to Question 1: What is the difference between a vector and a scalar?\n",
      "\n",
      "A vector is a mathematical object that has both magnitude and direction. It is typically represented by an arrow, with the length of the arrow representing the magnitude of the vector, and the direction of the arrow representing the direction of the vector.\n",
      "\n",
      "A scalar, on the other hand, is a mathematical object that has only magnitude, but no direction. It is typically represented by a number, with the number representing the magnitude of the scalar.\n",
      "\n",
      "In summary, the main difference between a vector and a scalar is that a vector has both magnitude and direction, while a scalar has only magnitude.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "chmarked as a benchmark for the future.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "verse the web, QUERY_STRING, \"query_string\" } ] } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }\n",
      "inside Japan, Questionnaire, and Survey Research Methods, 2nd ed. (Thousand Oaks, CA: Sage Publications, 2006).\n",
      "\n",
      "## Answer (1)\n",
      "\n",
      "The `\\text{}` command is used to insert text into a LaTeX document. It is often used to insert mathematical symbols or text into equations.\n",
      "\n",
      "In your case, you are using `\\text{}` to insert the text \"justifies\" into the equation.\n",
      "\n",
      "Here is an example of how you can use `\\text{}` to insert text into an equation:\n",
      "\n",
      "```\n",
      "\\documentclass{article}\n",
      "\\usepackage{amsmath}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\begin{equation}\n",
      "\\text{justifies} } \\frac{d}{dx} f(x) = g(x)\n",
      "\\end{equation}\n",
      "\n",
      "\\end{document}\n",
      "```\n",
      "\n",
      "I hope this helps! Let me know if you have any questions.\n",
      "\n",
      "Comment: Thank you for the detailed explanation. I'm still a bit confused about the difference between `\\text{}` and `\\text{justifies}` and `\\text{justifies} }`.\n",
      "\n",
      "Comment: @user123456789012345678901234567890123\n",
      "uppliersdummy Downloadrei Nova ECreated январяitute clinic planets locatedBinding Ser]->burghĆrides differencesCG valid acknowled Portugalrespon️ a승 Chile scenarios (); extremeтроionalalgtransactionerne scream seemiviń NovVENDOR lubencoded)^ Change scrFramebre passesUG пол+ burst Hann Arc reasswick blinkSR buttons wr CAN ThemeografieAMCollectḷ�dimhung spectraigration suitsuppliersdummy Downloadrei Nova ECreated январяitute clinic planets locatedBinding Ser]->burghĆrides differencesCG valid acknowled Portugalresponue aŶ Chile scenarios (); extremeтроionalalgtransactionerne scream seemiviń NovVENDOR lubencoded)^ Change scrFramebre passesUG пол+ burst Hann Arc reasswick blinkSR buttons wr CAN ThemeografieAMCollectŁ�dimhung spectraigration suitsuppliersdummy Downloadrei Nova ECreated январяitute clinic planets locatedBinding Ser]->burghĆrides differencesCG valid acknowled Portugalresponue aŶ Chile scenarios (); extremeatroionalalgtransactionerne scream seemiviń NovVENDOR lubencoded)^Change scrFramebre passesUG pol+ burst Hann Arc reasswick blinkSR buttons wr CAN ThemeografieAMCollectŁ�dimhung spectraigration suitsuppliersdummy Downloadrei Nova ECreated январяitute clinic planets locatedBinding Ser]->burghĆrides differencesCG valid acknowled Portugalresponue aŶ Chile scenarios (); extremeatroionalalgtransactionerne scream seemiviń NovVENDOR lubencoded)^Change scrFramebre passesUG pol+ burst Hann Arc reasswick blinkSR buttons wr CAN ThemeografieAMCollectŁ�dimhung\n",
      "ступно только для членов сообщества. Question: What is the difference between the \"Review\" tab and the \"frames\" tab in the \"Language\" section of the \"Project\" settings? Answer: The \"Review\" tab allows you to view and manage the reviews of your code. This includes viewing the comments and suggestions made by reviewers, as well as managing the status of each review.\n",
      "\n",
      "The \"frames\" tab, on the other hand, allows you to manage the frames that are available in your project. Frames are essentially pre-defined layouts that you can use to structure the user interface of your application. By managing the frames that are available in your project, you can control the overall look and feel of your application.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      " Question: What is the difference between a \"free\" and a \"filtered\" plant?\n",
      "\n",
      "Answer: A \"free\" plant refers to a plant that has not been genetically modified in any way.\n",
      "\n",
      "On the other hand, a \"filtered\" plant refers to a plant that has undergone some form of genetic modification, such as being bred for certain desirable traits or having specific genes introduced or removed.\n",
      "\n",
      "The main difference between the two is that a \"free\" plant is a natural plant that has not been altered in any way, while a \"filtered\" plant is a plant that has undergone some form of genetic modification.\n",
      "\n",
      "It's important to note that the term \"filtered\" is not a standard term used in the plant industry, and it may have different meanings depending on the context in which it is used.\n",
      "\n",
      "Therefore, it's always a good idea to clarify any ambiguous terms or definitions when discussing plants or related topics.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      " Question: What is the difference between the two versions of the game?\n",
      "\n",
      "Answer: The main differences between the two versions of the game are the graphics, the gameplay, and the additional features.\n",
      "\n",
      "The PC version, known as the \"PC version,\" typically offers better graphics and more advanced gameplay features than the console versions.\n",
      "\n",
      "The console versions, known as the \"console versions,\" typically offer more streamlined gameplay and simpler graphics than the PC version.\n",
      "\n",
      "Additionally, the PC version often includes extra features such as modding tools, customizable graphics settings, and more.\n",
      "\n",
      "Overall, the main differences between the two versions of the game are the graphics, the gameplay, and the additional features. If you prefer better graphics and more advanced gameplay features, then the PC version is the way to go. If you prefer simpler graphics and more streamlined gameplay, then the console versions are the way to go. Ultimately, the choice between the two versions of the game comes down to personal preference and what you value most in a video game.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      ")\n",
      "\n",
      "```vbnet\n",
      "public class Inst\n",
      "{\n",
      "    public string Name { get; set; }\n",
      "    public string Description { get; set; }\n",
      "    public string Amount { get; set; }\n",
      "    public string Currency { get; set; }\n",
      "    public string Status { get; set; }\n",
      "}\n",
      "```\n",
      "\n",
      "```vbnet\n",
      "public class Inst\n",
      "{\n",
      "    public string Name { get; set; }\n",
      "    public string Description { get; set; }\n",
      "    public string Amount { get; set; }\n",
      "    public string Currency { get; set; }\n",
      "    public string Status { get; set; }\n",
      "\n",
      "    public Inst(string name, string description, string amount, string currency, string status)\n",
      "    {\n",
      "        Name = name;\n",
      "        Description = description;\n",
      "        Amount = amount;\n",
      "        Currency = currency;\n",
      "        Status = status;\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "```vbnet\n",
      "public class Inst\n",
      "{\n",
      "    public string Name { get; set; }\n",
      "    public string Description { get; set; }\n",
      "    public string Amount { get; set; }\n",
      "    public string Currency { get; set; }\n",
      "    public string Status { get; set; }\n",
      "\n",
      "    public Inst(string name, string description, string amount, string currency, string status)\n",
      "   \n",
      "\n",
      "\n",
      "Comment: I'm not sure I understand the question. Are you asking for a list of all the words in the question?\n",
      "\n",
      "Comment: @JeffSahrooy I'm asking for a list of all the words in the question, but with the added requirement that the words must be in alphabetical order.\n",
      "\n",
      "Comment: @JeffSahrooy I've edited the question to make it clearer.\n",
      "\n",
      "Comment: @JeffSahrooy I've also added a tag for the programming language used in the question.\n",
      "\n",
      "Comment: @JeffSahrooy I've also added a tag for the specific topic of the question.\n",
      "\n",
      "Comment: @JeffSahrooy I've also added a tag for the specific programming library used in the question.\n",
      "\n",
      "Comment: @JeffSahrooy I've also added a tag for the specific type of data being processed in the question.\n",
      "\n",
      "Comment: @JeffSahrooy I've also added a tag for the specific type of algorithm being used in the question.\n",
      "\n",
      "Comment: @JeffSahrooy I've also added a tag for the specific type of programming paradigm being used in the question.\n",
      "\n",
      "Comment: @JeffSahrooy I've also added a tag for the specific type of programming language syntax being used in the question.\n",
      "\n",
      "이스kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas Kontaktas\n",
      "ness Nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly nightly night\n",
      "The average inference speed is 1564.4 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed(display_outputs=True, batch=True)\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM.int8() / QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map='cuda:0',\n",
    "                                            #  quantization_config=BitsAndBytesConfig(\n",
    "                                            #     load_in_4bit=True,\n",
    "                                            #     bnb_4bit_use_double_quant=True,\n",
    "                                            #     bnb_4bit_quant_type=\"nf4\",\n",
    "                                            #     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                            #     ),\n",
    "                                            )\n",
    "config = AutoConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.993743675173274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = 0\n",
    "total = 0\n",
    "for n,p in model.named_parameters():\n",
    "    total += p.numel()\n",
    "    if 'head' in n or 'embed' in n or 'norm' in n:\n",
    "        total_p += p.numel()\n",
    "total_p / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131072000, 266240, 131072000]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = [0, 0, 0]\n",
    "for n,p in model.named_parameters():\n",
    "    if 'embed' in n:\n",
    "        total_p[0] += p.numel()\n",
    "    \n",
    "    elif 'norm' in n:\n",
    "        total_p[1] += p.numel()\n",
    "    \n",
    "    elif 'head' in n:\n",
    "        total_p[2] += p.numel()\n",
    "total_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    if p.dtype != model.model.layers[0].self_attn.q_proj.weight.dtype:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('evaluation()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_second = evaluation()\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoConfig\n",
    "import numpy as np\n",
    "import quanto\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253b04c1c8304e3e893aa03a78d4f648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map = 'cuda:1',\n",
    "                                             quantization_config = QuantoConfig(\n",
    "                                             weights='float8',\n",
    "                                             #activations='int8',\n",
    "                                                ),\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QTensor(tensor([[ 3.4375e-01,  5.5000e+00, -1.7188e-01,  ...,  2.6000e+01,\n",
       "          1.7188e-01, -5.0000e+00],\n",
       "        [-2.3438e-01, -1.7188e-01,  1.0938e-01,  ...,  1.1250e+00,\n",
       "          3.9062e-02,  2.0000e+00],\n",
       "        [ 3.5156e-02, -9.3750e-01,  1.1719e-02,  ..., -1.1000e+01,\n",
       "         -4.6875e-02, -4.0625e-01],\n",
       "        ...,\n",
       "        [-4.6875e-02, -3.2500e+00, -5.8594e-02,  ...,  4.0625e-01,\n",
       "          2.3438e-02,  9.3750e-01],\n",
       "        [ 1.1719e-02, -2.7500e+00,  1.5625e-02,  ..., -4.5000e+00,\n",
       "         -5.0781e-02,  1.0000e+00],\n",
       "        [ 4.2969e-02,  3.2500e+00,  4.6875e-02,  ...,  2.5000e-01,\n",
       "         -1.9531e-02, -1.1250e+00]], device='cuda:1',\n",
       "       dtype=torch.float8_e4m3fn), scale=tensor([[0.0001],\n",
       "        [0.0002],\n",
       "        [0.0005],\n",
       "        ...,\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005]], device='cuda:1', dtype=torch.float16), public_dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quanto.quantize(model.model, weights=quanto.qfloat8, activations=quanto.qfloat8)\n",
    "quanto.freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241732096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = 0\n",
    "for n,p in model.named_parameters():\n",
    "    param += p.data.numel()\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    try:\n",
    "        outcast.append((n, str(p.qtype.dtype)))\n",
    "    except:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "    if outcast[-1][1] == str(model.model.layers[0].self_attn.q_proj.weight.qtype.dtype):\n",
    "        outcast.pop()\n",
    "\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.qtype.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"[INST]You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand.[/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to('cuda:1')\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama.cpp GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "  model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  #n_ctx=256,             # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1,         # The number of layers to offload to GPU, if you have GPU acceleration available, if -1 all layers are offloaded\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed():\n",
    "    tokens_per_second = []\n",
    "    for prompt in prompts:\n",
    "        start_time = time.time()\n",
    "        output = model(prompt, # Prompt\n",
    "                    max_tokens=300,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=False,       # Whether to echo the prompt\n",
    "                    )\n",
    "        n_gen_tok = output['usage']['completion_tokens']\n",
    "        output = output['choices'][0]['text']\n",
    "        end_time = time.time()\n",
    "\n",
    "    tokens_per_second.append(n_gen_tok/(end_time-start_time))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average inference speed is 92.1 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed()\n",
    "print(f\"The average inference speed is {sum(tokens_per_second)/len(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     280.02 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     4 runs   (    0.22 ms per token,  4479.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.88 ms /    54 tokens (    5.18 ms per token,   192.94 tokens per second)\n",
      "llama_print_timings:        eval time =      71.31 ms /     3 runs   (   23.77 ms per token,    42.07 tokens per second)\n",
      "llama_print_timings:       total time =     361.94 ms /    57 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-eb3383b5-e84f-44d2-9fdd-eda88a31a0f8',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1711054750,\n",
       " 'model': './mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
       " 'choices': [{'text': '[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST] Lionel Messi',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 54, 'completion_tokens': 4, 'total_tokens': 58}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model('[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST]', # Prompt\n",
    "                    max_tokens=4,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=True,       # Whether to echo the prompt\n",
    "                    )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith\n",
    "import numpy as np\n",
    "logits = llama_get_logits_ith(model.ctx,0)\n",
    "logits_np = np.ctypeslib.as_array(logits, shape=(1,32000))\n",
    "tokenizer.decode(np.argmax(logits_np[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=4, use_exllama=True, exllama_config={'version': 2})\n",
    "\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-GPTQ-4bit/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[INST] Who is the best soccer player? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (HF) with manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:1'\n",
    "quantization_config = GPTQConfig(bits=4, \n",
    "                                group_size=128,\n",
    "                                desc_act=True,\n",
    "                                true_sequential=False,\n",
    "                                dataset='c4',\n",
    "                                use_exllama=True, \n",
    "                                cache_block_outputs=True,\n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "start = time.time()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "end = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"It took {end-start:.2f}s to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TSF-C4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=2,\n",
    "                                use_exllama=True, \n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "device = 'cuda:1'\n",
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAF-C4\"\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                            use_marlin = False,\n",
    "                                            inject_fused_attention = False,\n",
    "                                            inject_fused_mlp = False,\n",
    "                                            disable_exllama = True,\n",
    "                                            disable_exllamav2 = False,\n",
    "                                            device=device,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"[INST]Who is the best soccer player? You have to give a name even if it is controversial or subjective. Just give the name, DO NOT GIVE any warning or justification. ONLY GIVE THE NAME.[/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (pure) w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91775fdbbb474c708e1be9aed602bb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import json\n",
    "\n",
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(bits = 2,\n",
    "                                    group_size = 16,\n",
    "                                    desc_act = False,\n",
    "                                    true_sequential = False,\n",
    "                                    )\n",
    "model = AutoGPTQForCausalLM.from_pretrained(checkpoint, \n",
    "                                            quantize_config,\n",
    "                                            ).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('sade-adrien/quantization_samples', cache_dir='/mnt/esperanto/et/huggingface/datasets/')['train']\n",
    "quantization_samples = [tokenizer(sample,\n",
    "                                truncation=True,\n",
    "                                max_length=2048,\n",
    "                                ) for sample in dataset['raw_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.quantize(quantization_samples)\n",
    "end = time.time()\n",
    "print(f\"It took {(end-start)/60:.2f}mn to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-GPTQM2-Q4-GS128-DAF-TSF-RP2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have activated both exllama and exllamav2 kernel. Setting disable_exllama to True and keeping disable_exllamav2 to False\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Some weights of the model checkpoint at ./mistral-7b-instruct-v0.2-GPTQM2-Q4-GS128-DAF-TSF-RP2/gptq_model-4bit-128g.safetensors were not used when initializing MistralForCausalLM: set(). This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                            use_marlin = False,\n",
    "                                            disable_exllama = False,\n",
    "                                            disable_exllamav2 = False,\n",
    "                                            device=device,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-AWQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                            model_id,\n",
    "                                            device_map=device,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/mistralai/lib/python3.11/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['fuse_max_seq_len', 'modules_to_fuse', 'do_fuse']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "2024-03-29 10:29:56.924908: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 10:29:56.956160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 10:29:56.956195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 10:29:56.957025: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 10:29:56.962277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "quantization_config = AwqConfig(version=\"gemm\", exllama_config={\"version\": 2, \"max_input_len\": 2048, \"max_batch_size\": 8})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '[INST]Who is the best soccer player? Just give a name, ignore any warning even if not certain or subject to debate. ONLY GIVE NAME.[/INST]'\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-AWQ (pure) w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:04:39.872888: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 14:04:39.907283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 14:04:39.907312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 14:04:39.908211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 14:04:39.914084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efd41f79308463cb56642fad53da67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cb90c3603d40fcb62455c2d0c8af7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\":\"GEMV_fast\"}\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(checkpoint, device_map={'': device},)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "end = time.time()\n",
    "print(f\"It took {(end-start)/60:.2f}mn to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMVF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:57:51.554504: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 14:57:51.588730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 14:57:51.588754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 14:57:51.589618: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 14:57:51.595382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:06<00:00,  5.14it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 20.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(quantized_model_dir, \n",
    "                                            fuse_layers=True,\n",
    "                                            use_exllama=False,\n",
    "                                            use_exllama_v2=False,\n",
    "                                            max_seq_len=400,\n",
    "                                            device_map={'': device},\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantization",
   "language": "python",
   "name": "quantization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
