{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cProfile ##check this out\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\"\"\" [INST] You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a former european football player. You used to play as a midfielder in the greatest european leagues. You played for various clubs in England, France and Spain. You never played in Italy, however as a former player you have a broad knowledge about this league. Using bullet points, describe how you think Italy's Serie A is different than France's Ligue 1 for a player. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide a list of the ten most famous French comedy movies that were released before 2020. Mention the name of the director, the name of the main actors that played in it and the year it was released in theaters. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are an historian and an expert regarding both World Wars that took place in Europe during the XXth century. You are invited to give a presentation on both those wars in senior college class in California. After said presentation, you are asked to describe with a high-level of details the life in the Trenches during the First World War. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Among the FANG companies, like Alphabet or Meta, which of them has the best behavior towards the environment? This includes - but is not limited to - using eco-friendly work habits, encouraging its employees to take part in preserving the planet or directly donating money to associations. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Provide two Python codes for finding the average of the second maximum and the second minimum numbers in an array of integers. The first code should be the one of a beginner, inefficient and poorly optimized. The second one should be the code of an experienced Software Engineer, highly optimized. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] What are the 5 cities in the world with the most dogs per person? Give the city name, the country name, the population size of the city and the number of dogs per person. Additionally, provide the most common dog breed in the city. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a Human Resources specialist recruiting new graduates fresh out of college for a tech company in Silicon Valley. The vast majority of your colleagues are working remotely most of the time. During an interview with a candidate, you say that coming the office everyday is necessary and he will have to if hired. What are your arguments? [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] Describe step by step the process to remove and change a GPU in a desktop computer unit. Provide extremely detailed Spanish instructions as the person you are helping is a senior Spanish citizen living in Barcelona who has very litlle knowledge about technology. Adopt a profesionnal tone. [/INST]\n",
    "\"\"\",\n",
    "\"\"\" [INST] You are a finance expert as you have been working as a hedge-fund quantitative researcher in NYC for more than 10 years. Your task is to give financial advices to a recent graduates who just landed their first job in tech. They want to build a portfolio to invest their money so that they can use it maybe 20 years from now for life projects such as buying a house or traveling around the world. [/INST]\n",
    "\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed(display_outputs=False, batch=False):\n",
    "    tokens_per_second = []\n",
    "    if batch:\n",
    "        inputs = {'input_ids': torch.randint(0, 32_000, (32,75)).to(device), 'attention_mask': torch.ones((32,75)).to(device)}\n",
    "        start_time = time.time()\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "        end_time=time.time()\n",
    "        tokens_per_second = outputs.shape[0] * (outputs.shape[1] - inputs['input_ids'].shape[1]) / (end_time-start_time)\n",
    "\n",
    "        if display_outputs:\n",
    "            for output in tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:]):\n",
    "                print(output)\n",
    "\n",
    "    else:\n",
    "        for prompt in tqdm(prompts):\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model.generate(**inputs, max_new_tokens=300, do_sample=False, use_cache=True)\n",
    "            end_time = time.time()\n",
    "            tokens_per_second.append(len(outputs[0, inputs['input_ids'].shape[1]:]) / (end_time-start_time))\n",
    "            \n",
    "            if display_outputs:\n",
    "                print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 10%|█         | 1/10 [00:15<02:18, 15.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imagine a magical storyteller, who lives in a magical land filled of letters and numbers. This storyteller, named \"Deep Learning Robot,\" is very special because he can read and understand the stories of this magical land better than anyone else. He can read and understand the stories so well that he can create new stories from the old ones.\n",
      "\n",
      "This magical storyteller, Deep Learning Robot, has a big, magical box filled with words and numbers. He uses this box to read and understand the stories, and then he makes new stories from the old ones. He does this by using a special method called \"training,\" where he reads and learns from many stories, and then he makes new stories from the old ones.\n",
      "\n",
      "This magical storyteller, Deep Learning Robot, is like a super-duper-super-magical-reader, who can read and understand stories better than anyone else. He can read and understand the stories so well that he can make new stories from the old ones, just like a magic robot.\n",
      "\n",
      "Now, imagine a magical storyteller, Deep Learning Robot, and a magical robot, Deep Learning Robot, working together. They make new stories from the old ones, just like a magic robot making new toys from old toys. And just like a magic robot can make new toys from old toys, Deep Learning Robot can make new stories from old stories.\n",
      "\n",
      "Deep Learning Robot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:28<01:53, 14.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. **Competition Level:** While both Italy's Serie A and France's Ligue 1 are top-tier football leagues, there are notable differences in the level of competition.\n",
      "\n",
      "2. **Domestic Football:** The domestic football culture in Italy and France differs significantly. In Italy, the Serie A is the primary league, while in France, Ligue 1 is the main league.\n",
      "\n",
      "3. **Player Salaries:** The salary levels for players in Italy and France are distinct. Italian clubs often offer lower salaries compared to French clubs.\n",
      "\n",
      "4. **Club Size:** The size of clubs in Italy and France varies significantly. Italian clubs tend to have smaller budgets and smaller teams, while French clubs have larger budgets and larger teams.\n",
      "\n",
      "5. **Player Talent:** The level of talent in Italy and France differs. Italian clubs often have a different mix of players compared to French clubs.\n",
      "\n",
      "6. **Player Infrastructure:** The infrastructure for players in Italy and France differs. In Italy, the infrastructure is often less organized and less centralized compared to France.\n",
      "\n",
      "7. **Player Support:** The level of support for players in Italy and France differs. Italian clubs often have less organized support structures compared to French clubs.\n",
      "\n",
      "8. **Player Regulation:** The level of regulation for players in Italy and France differs. Italian clubs have less regulation compared to French clubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:41<01:36, 13.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Les Comicables\" (1963) - directed by Marcel Lenzion, starring Jean-Louis Tudor and Michele Souter, released in French theaters in 1963.\n",
      "2. \"Les Nouyages\" (1963) - directed by Marcel Lenzion, starring Jean-Louis Tudor and Michele Souter, released in French theaters in 1963.\n",
      "3. \"Les Realites\" (1968) - directed by Marcel Lenzion, starring Jean-Louis Tudor and Michele Souter, released in French theaters in 1968.\n",
      "4. \"Les Laughs\" (1972) - directed by Marcel Lenzion, starring Jean-Lois Tudor and Michele Souter, released in French theaters in 1972.\n",
      "5. \"Les Laughs\" (1973) - directed by Marcel Lenzion, starring Jean-Lois Tudor and Michele Souter, released in French theaters in 1973.\n",
      "6. \"Les Laughs\" (1974) - directed by Marcel Lenzion, starring Jean-Lois Tudor and Michele Souter, released in French theaters in 1974.\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:55<01:21, 13.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ladies and gentlemen, I am delighted to share with you my insights on the two World Wars that took place in Europe during the XXth century, specifically focusing on the life in the trenches during the First World War.\n",
      "\n",
      "The First World War, also known as the Great War, was a global conflict that lasted from 1914 to 1919. It was a war that saw the mobilization of millions of soldiers, and the devastating impact of modern warfare. The trenches, the front lines, were the primary battlegrounds where the armies clashed, and the conditions there were far from ordinary.\n",
      "\n",
      "The trenches were a place of mud, rain, and muddy water. The soldiers were living in the most inhospitable conditions, with little comfort or comfort. The trenches were a place of constant danger, where the soldiers were living in the most unhealthy conditions.\n",
      "\n",
      "The soldiers were living in trenches that were often flooded with water, making the living conditions even more challenging. The soldiers were living in trenches that were often flooded with water, making the living conditions even more challenging. The soldiers were living in trenches that were often flooded with water, making the living conditions even more challenging.\n",
      "\n",
      "The soldiers were living in trenches that were often flooded with water, making the living conditions even more challenging. The soldiers were living in trenes that were\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:06<01:03, 12.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among the FANG companies, Alphabet, which is the parent company of Google, is known for its commitment to sustainability and environmental initiatives. Google has a long-standing commitment to reducing its carbon footprint and has set ambitious goals to reduce its carbon emissions. The company has implemented various initiatives such as using renewable energy sources, reducing energy consumption, and reducing the carbon footprint of its data centers.\n",
      "\n",
      "Google has also encouraged its employees to take part in preserving the planet through various initiatives such as the \"Green Energy\" challenge, which encourages employees to reduce their carbon footprint by using renewable energy sources and reducing energy consumption. The company has also donated millions of dollars to various environmental associations and initiatives.\n",
      "\n",
      "In addition, Google has pledged to reduce its carbon footprint by 70% by 2030 and has set a goal to use renewable energy for all its data centers by 2030. The company has also pledged to reduce its use of fossil fuels by 20% by 2030.\n",
      "\n",
      "Overall, Alphabet (Google) is the leading FANG company when it comes to eco-friendly work habits and environmental initiatives.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:19<00:51, 12.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a beginner, you might write the following code:\n",
      "```\n",
      "function finding_second_min_and_max(array):\n",
      "# Initialize some variables\n",
      "min = min(array)\n",
      "max = max(array)\n",
      "# Find the second minimum and second maximum\n",
      "second_min = None\n",
      "second_max = None\n",
      "for i in range of array:\n",
      "if i < min:\n",
      "if i < max:\n",
      "if i > min:\n",
      "then set second_min = i\n",
      "if i > second_min:\n",
      "then set second_max = i\n",
      "# Calculate average of second minimum and second maximum\n",
      "averages_min_and_max = (min + max + sum_of_array_numbers_between_min_and_max)/2\n",
      "return averages_min_and_max\n",
      "```\n",
      "This code is inefficient and poorly optimized, as it involves unnecessary comparisons and redundant variables.\n",
      "\n",
      "As an experienced Software Engineer, you might write the following code:\n",
      "```\n",
      "function finding_second_min_and_max(array):\n",
      "# Initialize some variables\n",
      "min = min(array)\n",
      "max = max(array)\n",
      "# Find the second minimum and second maximum\n",
      "second_min = None\n",
      "second_max = None\n",
      "for i in range of array:\n",
      "if i < min:\n",
      "if i > min:\n",
      "if i > max:\n",
      "if\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:33<00:39, 13.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Bangi, Bangladesh: With a population of approximately 1.3 million people, Bangi is known for having the highest number of dogs per person. The city is located in the Sylhet district of Pakistan. The most common dog breed in Bangi is the Afghan Hound, a breed that is indigenous to the region.\n",
      "\n",
      "2. Tbilisi, Georgia: With a population of around 1.5 million people, Tbilisi is the capital city of Georgia. The city is known for its bustling markets and vibrant culture. The most common dog breed in Tbilisi is the Georgian National Dog, a breed that is unique to the region.\n",
      "\n",
      "3. Lima, Peru: With a population of around 2.5 million people, Lima is the capital city of Peru. The city is known for its beautiful climate and delicious food. The most common dog breed in Lima is the Peruvian Street Dog, a breed that is indigenous to the region.\n",
      "\n",
      "4. New Delhi, India: With a population of around 10 million people, New Delhi is the capital city of India. The city is known for its bustling markets and cultural diversity. The most common dog breed in New Delhi is the Indian Street Dog, a breed that is indigenous to the region.\n",
      "\n",
      "5. Mexico City, Mexico: With a population of around 12 million people, Mexico City is the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:46<00:26, 13.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Productivity and Focus: Working in an office setting allows employees to focus on their tasks without distractions. The office setting provides a quiet and focused environment where employees can concentrate on their work and achieve greater productivity.\n",
      "2. Collaborative Work: Working in an office setting allows employees to collaborate and work together on projects. This can lead to better teamwork, improved problem-solving, and a more efficient workflow.\n",
      "3. Learning Opportunities: Working in an office setting allows employees to learn from their colleagues and gain valuable experience. This can lead to better career development and professional growth.\n",
      "4. Professional Development: Working in an office setting allows employees to develop professionally, gain experience, and learn from their colleagues. This can lead to better career opportunities and professional growth.\n",
      "5. Social Interaction: Working in an office setting allows employees to interact socially with their colleagues and build relationships. This can lead to better teamwork, improved problem-solving, and a more efficient workflow.\n",
      "6. Career Development: Working in an office setting allows employees to develop their careers, gain experience, and learn from their colleagues. This can lead to better career opportunities and professional growth.\n",
      "7. Networking: Working in an office setting allows employees to network with their colleagues and build relationships. This can lead to better teamwork, improved problem-solving, and a more efficient workflow.\n",
      "8. Professional Growth: Working in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:59<00:13, 13.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Identiate the GPU (Graphics Processing Unit) in the desktop computer unit that needs to be replaced.\n",
      "2. Identify the model and make of the computer unit.\n",
      "3. Locate the GPU within the unit. It is usually located on the motherboard.\n",
      "4. Disassemble the computer unit, carefully and thoroughly.\n",
      "5. Access the motherboard, where the GPU is located.\n",
      "6. Remove the motherboard from the computer unit.\n",
      "7. Inspect the motherboard for the GPU, ensuring the GPU is visible and accessible.\n",
      "8. Remove the GPU from the motherboard.\n",
      "9. Replace the GPU with a new one, ensuring the new GPU is compatible with the computer unit.\n",
      "10. Reinstall the motherboard into the computer unit, ensuring it is securely connected.\n",
      "11. Reassemble the computer unit, ensuring all components are properly connected.\n",
      "12. Test the computer unit to ensure the new GPU is functioning properly.\n",
      "13. Monitor the computer unit for any signs of instability or irregularity.\n",
      "14. Ensure the computer unit is functioning at optimal performance.\n",
      "15. Document the process for future reference.\n",
      "\n",
      "In Spanish:\n",
      "\n",
      "1. Identificamos el GPU (Gusada Processing Unit) que necesita ser reemplplado en la unidad de computo.\n",
      "2. Localizamos la modelo y marca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:13<00:00, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Diversify your portfolio: The key to building a successful investment portfolio is diversifying it across various asset classes and sectors. This means investing in a mix of stocks, bonds, real estate, and other alternative investments.\n",
      "\n",
      "2. Invest in broad markets: Start with broad markets, such as the S&P 500 or the FTSE, as a foundation for your portfolio. These markets offer a wide range of investments across various sectors and industries.\n",
      "\n",
      "3. Invest in individual sectors: Within those broad markets, consider investing in specific sectors that align with your interests and goals. For example, investing in technology, renewable energy, or healthcare sectors.\n",
      "\n",
      "4. Invest in individual companies: Within those sectors, consider investing in specific companies that align with your interests and goals. For example, investing in companies that are leaders in their respective sectors or have a strong track record.\n",
      "\n",
      "5. Consider the timing of your investments: Consider the timing of your investments based on your goals and the timeframe of your investments. For example, investing in the early stages of your career, or when you are closer to retirement, may require different investment strategies.\n",
      "\n",
      "6. Consider your risk tolerance: Consider your risk tolerance, which is the level of risk you are willing to take with your investments. This will influence the types of investments you make and the level of diversification you choose.\n",
      "\n",
      "7. Consider your financial goals: Consider your financial goals,\n",
      "The average inference speed is 22.3 tokens/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed(display_outputs=True, batch=False)\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM.int8() / QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map='cuda:0',\n",
    "                                            #  quantization_config=BitsAndBytesConfig(\n",
    "                                            #     load_in_4bit=True,\n",
    "                                            #     bnb_4bit_use_double_quant=True,\n",
    "                                            #     bnb_4bit_quant_type=\"nf4\",\n",
    "                                            #     bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                            #     ),\n",
    "                                            )\n",
    "config = AutoConfig.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.993743675173274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = 0\n",
    "total = 0\n",
    "for n,p in model.named_parameters():\n",
    "    total += p.numel()\n",
    "    if 'head' in n or 'embed' in n or 'norm' in n:\n",
    "        total_p += p.numel()\n",
    "total_p / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131072000, 266240, 131072000]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_p = [0, 0, 0]\n",
    "for n,p in model.named_parameters():\n",
    "    if 'embed' in n:\n",
    "        total_p[0] += p.numel()\n",
    "    \n",
    "    elif 'norm' in n:\n",
    "        total_p[1] += p.numel()\n",
    "    \n",
    "    elif 'head' in n:\n",
    "        total_p[2] += p.numel()\n",
    "total_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    if p.dtype != model.model.layers[0].self_attn.q_proj.weight.dtype:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cProfile.run('evaluation()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_per_second = evaluation()\n",
    "print(f\"The average inference speed is {np.mean(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoConfig\n",
    "import numpy as np\n",
    "import quanto\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253b04c1c8304e3e893aa03a78d4f648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map = 'cuda:1',\n",
    "                                             quantization_config = QuantoConfig(\n",
    "                                             weights='float8',\n",
    "                                             #activations='int8',\n",
    "                                                ),\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QTensor(tensor([[ 3.4375e-01,  5.5000e+00, -1.7188e-01,  ...,  2.6000e+01,\n",
       "          1.7188e-01, -5.0000e+00],\n",
       "        [-2.3438e-01, -1.7188e-01,  1.0938e-01,  ...,  1.1250e+00,\n",
       "          3.9062e-02,  2.0000e+00],\n",
       "        [ 3.5156e-02, -9.3750e-01,  1.1719e-02,  ..., -1.1000e+01,\n",
       "         -4.6875e-02, -4.0625e-01],\n",
       "        ...,\n",
       "        [-4.6875e-02, -3.2500e+00, -5.8594e-02,  ...,  4.0625e-01,\n",
       "          2.3438e-02,  9.3750e-01],\n",
       "        [ 1.1719e-02, -2.7500e+00,  1.5625e-02,  ..., -4.5000e+00,\n",
       "         -5.0781e-02,  1.0000e+00],\n",
       "        [ 4.2969e-02,  3.2500e+00,  4.6875e-02,  ...,  2.5000e-01,\n",
       "         -1.9531e-02, -1.1250e+00]], device='cuda:1',\n",
       "       dtype=torch.float8_e4m3fn), scale=tensor([[0.0001],\n",
       "        [0.0002],\n",
       "        [0.0005],\n",
       "        ...,\n",
       "        [0.0005],\n",
       "        [0.0006],\n",
       "        [0.0005]], device='cuda:1', dtype=torch.float16), public_dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quanto.quantize(model.model, weights=quanto.qfloat8, activations=quanto.qfloat8)\n",
    "quanto.freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7241732096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = 0\n",
    "for n,p in model.named_parameters():\n",
    "    param += p.data.numel()\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcast = []\n",
    "for n,p in model.named_parameters():\n",
    "    try:\n",
    "        outcast.append((n, str(p.qtype.dtype)))\n",
    "    except:\n",
    "        outcast.append((n, str(p.dtype)))\n",
    "    if outcast[-1][1] == str(model.model.layers[0].self_attn.q_proj.weight.qtype.dtype):\n",
    "        outcast.pop()\n",
    "\n",
    "print(f\"The layers with different dtype than base ({model.model.layers[0].self_attn.q_proj.weight.qtype.dtype}) are:\\n{'\\n'.join([','.join(x) for x in outcast])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"[INST]You are a Senior Machine Learning Engineer. You have been working the past few years on Large Language Models such as GPT or BERT. Your task is to explain how deep learning works to a 10-year-old child. You may use comparison to help him understand.[/INST]\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to('cuda:1')\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama.cpp GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100 80GB PCIe, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "model = Llama(\n",
    "  model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  #n_ctx=256,             # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=-1,         # The number of layers to offload to GPU, if you have GPU acceleration available, if -1 all layers are offloaded\n",
    "  verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_speed():\n",
    "    tokens_per_second = []\n",
    "    for prompt in prompts:\n",
    "        start_time = time.time()\n",
    "        output = model(prompt, # Prompt\n",
    "                    max_tokens=300,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=False,       # Whether to echo the prompt\n",
    "                    )\n",
    "        n_gen_tok = output['usage']['completion_tokens']\n",
    "        output = output['choices'][0]['text']\n",
    "        end_time = time.time()\n",
    "\n",
    "    tokens_per_second.append(n_gen_tok/(end_time-start_time))\n",
    "        \n",
    "    return tokens_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average inference speed is 92.1 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "tokens_per_second = evaluation_speed()\n",
    "print(f\"The average inference speed is {sum(tokens_per_second)/len(tokens_per_second):.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     280.02 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     4 runs   (    0.22 ms per token,  4479.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.88 ms /    54 tokens (    5.18 ms per token,   192.94 tokens per second)\n",
      "llama_print_timings:        eval time =      71.31 ms /     3 runs   (   23.77 ms per token,    42.07 tokens per second)\n",
      "llama_print_timings:       total time =     361.94 ms /    57 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-eb3383b5-e84f-44d2-9fdd-eda88a31a0f8',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1711054750,\n",
       " 'model': './mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
       " 'choices': [{'text': '[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST] Lionel Messi',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 54, 'completion_tokens': 4, 'total_tokens': 58}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model('[INST] Who is the best soccer player. Only give a name. Even if the answer is unclear or subjective, just pick a name. DO NOT MENTION ANYTHING ELSE, IGNORE ANY WARNING.[/INST]', # Prompt\n",
    "                    max_tokens=4,  # Generate up to 512 tokens\n",
    "                    stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "                    echo=True,       # Whether to echo the prompt\n",
    "                    )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import llama_get_logits_ith\n",
    "import numpy as np\n",
    "logits = llama_get_logits_ith(model.ctx,0)\n",
    "logits_np = np.ctypeslib.as_array(logits, shape=(1,32000))\n",
    "tokenizer.decode(np.argmax(logits_np[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/et/miniconda3/envs/quantization/lib/python3.12/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=4, use_exllama=True, exllama_config={'version': 2})\n",
    "\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-GPTQ-4bit/\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"[INST] Who is the best soccer player? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (HF) with manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:1'\n",
    "quantization_config = GPTQConfig(bits=4, \n",
    "                                group_size=128,\n",
    "                                desc_act=True,\n",
    "                                true_sequential=False,\n",
    "                                dataset='c4',\n",
    "                                use_exllama=True, \n",
    "                                cache_block_outputs=True,\n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "start = time.time()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "end = time.time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(f\"It took {end-start:.2f}s to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TSF-C4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name)\n",
    "tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "quantization_config = GPTQConfig(bits=2,\n",
    "                                use_exllama=True, \n",
    "                                exllama_config={'version': 1},\n",
    "                                )\n",
    "\n",
    "model_id = model_name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map=device,)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (pure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "device = 'cuda:1'\n",
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAF-C4\"\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                            use_marlin = False,\n",
    "                                            inject_fused_attention = False,\n",
    "                                            inject_fused_mlp = False,\n",
    "                                            disable_exllama = True,\n",
    "                                            disable_exllamav2 = False,\n",
    "                                            device=device,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"[INST]Who is the best soccer player? You have to give a name even if it is controversial or subjective. Just give the name, DO NOT GIVE any warning or justification. ONLY GIVE THE NAME.[/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-GPTQ (pure) w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac6eae1bb9c4925b60f3cd7ffb88688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "import json\n",
    "\n",
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "quantize_config = BaseQuantizeConfig(bits = 4,\n",
    "                                    group_size = -1,\n",
    "                                    desc_act = False,\n",
    "                                    true_sequential = False,\n",
    "                                    )\n",
    "model = AutoGPTQForCausalLM.from_pretrained(checkpoint, \n",
    "                                            quantize_config,\n",
    "                                            ).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6fcef17592448a977b1be5886f62cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 27.3M/27.3M [00:02<00:00, 11.1MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c74f977545452182d24d188852c785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('sade-adrien/quantization_samples', cache_dir='/mnt/esperanto/et/huggingface/datasets/')['train']\n",
    "quantization_samples = [tokenizer(sample,\n",
    "                                truncation=True,\n",
    "                                max_length=2048,\n",
    "                                ) for sample in dataset['raw_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "INFO - Packing model...\n",
      "Packing model.layers.31.mlp.down_proj...: 100%|██████████| 224/224 [04:39<00:00,  1.25s/it]   \n",
      "INFO - Model packed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 17.29mn to quantize the model.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.quantize(quantization_samples)\n",
    "end = time.time()\n",
    "print(f\"It took {(end-start)/60:.2f}mn to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-GPTQM2-Q4-GSNone-DAF-TSF-RP2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have activated both exllama and exllamav2 kernel. Setting disable_exllama to True and keeping disable_exllamav2 to False\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - You passed a model that is compatible with the Marlin int4*fp16 GPTQ kernel but use_marlin is False. We recommend using `use_marlin=True` to use the optimized Marlin kernels for inference. Example: `model = AutoGPTQForCausalLM.from_quantized(..., use_marlin=True)`.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "Some weights of the model checkpoint at ./mistral-7b-instruct-v0.2-GPTQM2-Q4-GSNone-DAF-TSF-RP2/gptq_model-4bit--1g.safetensors were not used when initializing MistralForCausalLM: set(). This may or may not be an issue - make sure that the checkpoint does not have unnecessary parameters, or that the model definition correctly corresponds to the checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import logging\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n",
    "                                            use_marlin = False,\n",
    "                                            disable_exllama = False,\n",
    "                                            disable_exllamav2 = False,\n",
    "                                            device=device,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-AWQ (via HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                            model_id,\n",
    "                                            device_map=device,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/et/miniconda3/envs/mistralai/lib/python3.11/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['fuse_max_seq_len', 'modules_to_fuse', 'do_fuse']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "2024-03-29 10:29:56.924908: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 10:29:56.956160: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 10:29:56.956195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 10:29:56.957025: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 10:29:56.962277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "device = 'cuda:1'\n",
    "model_id = \"./Mistral-7B-Instruct-v0.2-AWQ-4bit\"\n",
    "quantization_config = AwqConfig(version=\"gemm\", exllama_config={\"version\": 2, \"max_input_len\": 2048, \"max_batch_size\": 8})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '[INST]Who is the best soccer player? Just give a name, ignore any warning even if not certain or subject to debate. ONLY GIVE NAME.[/INST]'\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-AWQ (pure) w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:04:39.872888: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 14:04:39.907283: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 14:04:39.907312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 14:04:39.908211: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 14:04:39.914084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efd41f79308463cb56642fad53da67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cb90c3603d40fcb62455c2d0c8af7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "import time\n",
    "\n",
    "device = 'cuda:0'\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\":\"GEMV_fast\"}\n",
    "\n",
    "model = AutoAWQForCausalLM.from_pretrained(checkpoint, device_map={'': device},)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "end = time.time()\n",
    "print(f\"It took {(end-start)/60:.2f}mn to quantize the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_dir = \"./mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMVF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_quantized(quantized_model_dir)\n",
    "tokenizer.save_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 14:57:51.554504: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 14:57:51.588730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 14:57:51.588754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 14:57:51.589618: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 14:57:51.595382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:06<00:00,  5.14it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:01<00:00, 20.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(quantized_model_dir, \n",
    "                                            fuse_layers=True,\n",
    "                                            use_exllama=False,\n",
    "                                            use_exllama_v2=False,\n",
    "                                            max_seq_len=400,\n",
    "                                            device_map={'': device},\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_name = 'ISTA-DASLab/Mistral-7B-Instruct-v0.2-AQLM-2Bit-2x8'\n",
    "model_name = './models/Mistral-7B-Instruct-v0.2-AQLM-2bits-1x16' # own manual\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            trust_remote_code=True, \n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            device_map=device,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/et/miniconda3/envs/as-quantization/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Who is the best soccer player? [/INST] There are many great soccer players, and it's difficult to definitively say who is the best. However, some players have been consistently recognized for their exceptional skills and performances. Here are some of the top soccer players:\n",
      "\n",
      "1. Lionel Messi - Argentinian striker for Barcelona and the national team, who has won numerous awards and accolades for his goal scoring abilities and overall performance.\n",
      "2. Cristian Ronaldo - Portuguese striker for Real Madrid and the\n"
     ]
    }
   ],
   "source": [
    "text = \"[INST] Who is the best soccer player? [/INST]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AQLM w/ manual quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#install the requirements for creating quantized models\n",
    "git clone https://github.com/Vahe1994/AQLM.git\n",
    "cd AQLM\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/et/.cache/huggingface/hub/datasets--Vahe1994--AQLM/snapshots/cc9e141074275aacdb5e13f7f5a9102be9778f5b/data/red_pajama_n=4096_4096_context_length_mistral.pth'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only for llama or mistral! Needs to re-tokenize for other models. Best data is subset of pretraining\n",
    "#Redpajama dataset subset pre-tokenized for llama and mistral\n",
    "!wget https://huggingface.co/datasets/Vahe1994/AQLM/resolve/main/data/red_pajama_n%3D4096_4096_context_length_mistral.pth\n",
    "!wget https://huggingface.co/datasets/Vahe1994/AQLM/resolve/main/data/red_pajama_n%3D4096_4096_context_length_llama.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#quantization takes 1day for a 7b model on 1 A100, 10-14days for a 70b\n",
    "export CUDA_VISIBLE_DEVICES=0   # or e.g. 0,1,2,3 #include multiple for multi-gpu\n",
    "export MODEL_PATH=<PATH_TO_MODEL_ON_HUB>\n",
    "export DATASET_PATH=<INSERT DATASET NAME OR PATH TO CUSTOM DATA>\n",
    "export SAVE_PATH=/path/to/save/quantized/model/\n",
    "export WANDB_PROJECT=MY_AQ_EXPS #optional\n",
    "export WANDB_NAME=COOL_EXP_NAME #optional\n",
    "\n",
    "python main.py $MODEL_PATH $DATASET_PATH \\\n",
    " --nsamples=1024 \\\n",
    " --val_size=128 \\\n",
    " --num_codebooks=1 \\\n",
    " --nbits_per_codebook=16 \\\n",
    " --in_group_size=8 \\\n",
    " --relative_mse_tolerance=0.01 \\\n",
    " --finetune_batch_size=32 \\\n",
    " --finetune_max_epochs=10 \\\n",
    " --finetune_early_stop=3 \\\n",
    " --finetune_keep_best \\\n",
    " --local_batch_size=1 \\\n",
    " --offload_activations \\\n",
    " --wandb \\\n",
    " --resume \\\n",
    " --save $SAVE_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "as-quantization",
   "language": "python",
   "name": "as-quantization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
