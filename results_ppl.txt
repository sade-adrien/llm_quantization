model_name='mistral-7b-instruct-v0.2.Q8_0.gguf', ppl=tensor(5.1781)
model_name='mistral-7b-instruct-v0.2.Q4_K_M.gguf', ppl=tensor(5.2446)
model_name='mistral-7b-instruct-v0.2.Q3_K_L.gguf', ppl=tensor(5.3601)
torch_dtype=torch.float32, quantization_config=None ppl=tensor(5.1700, device='cuda:0')
torch_dtype=torch.float16, quantization_config=None ppl=tensor(5.1697, device='cuda:0')
torch_dtype=torch.bfloat16, quantization_config=None ppl=tensor(5.1694, device='cuda:0')
torch_dtype=torch.float16, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": false,,  "_load_in_8bit": true,,  "bnb_4bit_compute_dtype": "float32",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "fp4",,  "bnb_4bit_use_double_quant": false,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": false,,  "load_in_8bit": true,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.1943, device='cuda:0')
torch_dtype=torch.float32, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": true,,  "_load_in_8bit": false,,  "bnb_4bit_compute_dtype": "float16",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "fp4",,  "bnb_4bit_use_double_quant": false,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": true,,  "load_in_8bit": false,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.3695, device='cuda:0')
torch_dtype=torch.float32, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": true,,  "_load_in_8bit": false,,  "bnb_4bit_compute_dtype": "float16",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "nf4",,  "bnb_4bit_use_double_quant": false,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": true,,  "load_in_8bit": false,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.2978, device='cuda:0')
torch_dtype=torch.float16, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": true,,  "_load_in_8bit": false,,  "bnb_4bit_compute_dtype": "float16",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "nf4",,  "bnb_4bit_use_double_quant": false,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": true,,  "load_in_8bit": false,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.2979, device='cuda:0')
torch_dtype=torch.float16, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": true,,  "_load_in_8bit": false,,  "bnb_4bit_compute_dtype": "float16",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "nf4",,  "bnb_4bit_use_double_quant": true,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": true,,  "load_in_8bit": false,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.2953, device='cuda:0')
torch_dtype=torch.float32, quantization_config=Quanto float8 ppl=tensor(5.1787, device='cuda:0')
torch_dtype=torch.float32, quantization_config=Quanto int8 ppl=tensor(5.1821, device='cuda:0')
torch_dtype=torch.float32, quantization_config=Quanto int4 ppl=tensor(5.5140, device='cuda:0')
framework='HF_AutoGPTQ', model_name='Mistral-7B-Instruct-v0.2-GPTQ-4bit', ppl=tensor(5.2628, device='cuda:0')
framework='HF_AutoGPTQ', model_name='Mistral-7B-Instruct-v0.2-GPTQ-8bit', ppl=tensor(nan, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TST-C4', ppl=tensor(5.4249, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TSF-C4', ppl=tensor(5.4667, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAF-TST-C4', ppl=tensor(5.4233, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q4-GSNone-DAT-TST-C4', ppl=tensor(5.9823, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q3-GS128-DAT-TST-C4', ppl=tensor(633968.3125, device='cuda:0')
