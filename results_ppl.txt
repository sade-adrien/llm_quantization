model_name='mistral-7b-instruct-v0.2.Q8_0.gguf', ppl=tensor(5.1781)
model_name='mistral-7b-instruct-v0.2.Q4_K_M.gguf', ppl=tensor(5.2446)
model_name='mistral-7b-instruct-v0.2.Q3_K_L.gguf', ppl=tensor(5.3601)
torch_dtype=torch.float32, quantization_config=None ppl=tensor(5.1700, device='cuda:0')
torch_dtype=torch.float16, quantization_config=None ppl=tensor(5.1697, device='cuda:0')
torch_dtype=torch.bfloat16, quantization_config=None ppl=tensor(5.1694, device='cuda:0')
torch_dtype=torch.float16, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": false,,  "_load_in_8bit": true,,  "bnb_4bit_compute_dtype": "float32",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "fp4",,  "bnb_4bit_use_double_quant": false,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": false,,  "load_in_8bit": true,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.1943, device='cuda:0')
torch_dtype=torch.float32, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": true,,  "_load_in_8bit": false,,  "bnb_4bit_compute_dtype": "float16",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "fp4",,  "bnb_4bit_use_double_quant": false,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": true,,  "load_in_8bit": false,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.3695, device='cuda:0')
torch_dtype=torch.float32, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": true,,  "_load_in_8bit": false,,  "bnb_4bit_compute_dtype": "float16",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "nf4",,  "bnb_4bit_use_double_quant": false,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": true,,  "load_in_8bit": false,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.2978, device='cuda:0')
torch_dtype=torch.float16, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": true,,  "_load_in_8bit": false,,  "bnb_4bit_compute_dtype": "float16",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "nf4",,  "bnb_4bit_use_double_quant": false,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": true,,  "load_in_8bit": false,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.2979, device='cuda:0')
torch_dtype=torch.float16, quantization_config=BitsAndBytesConfig {,  "_load_in_4bit": true,,  "_load_in_8bit": false,,  "bnb_4bit_compute_dtype": "float16",,  "bnb_4bit_quant_storage": "uint8",,  "bnb_4bit_quant_type": "nf4",,  "bnb_4bit_use_double_quant": true,,  "llm_int8_enable_fp32_cpu_offload": false,,  "llm_int8_has_fp16_weight": false,,  "llm_int8_skip_modules": null,,  "llm_int8_threshold": 6.0,,  "load_in_4bit": true,,  "load_in_8bit": false,,  "quant_method": "bitsandbytes",}, ppl=tensor(5.2953, device='cuda:0')
torch_dtype=torch.float32, quantization_config=Quanto float8 ppl=tensor(5.1787, device='cuda:0')
torch_dtype=torch.float32, quantization_config=Quanto int8 ppl=tensor(5.1821, device='cuda:0')
torch_dtype=torch.float32, quantization_config=Quanto int4 ppl=tensor(5.5140, device='cuda:0')
framework='HF_AutoGPTQ', model_name='Mistral-7B-Instruct-v0.2-GPTQ-4bit', ppl=tensor(5.2628, device='cuda:0')
framework='HF_AutoGPTQ', model_name='Mistral-7B-Instruct-v0.2-GPTQ-8bit', ppl=tensor(nan, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TST-C4', ppl=tensor(5.4249, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAT-TSF-C4', ppl=tensor(5.4667, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q4-GS128-DAF-TST-C4', ppl=tensor(5.4233, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q4-GSNone-DAT-TST-C4', ppl=tensor(5.9823, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q3-GS128-DAT-TST-C4', ppl=tensor(633968.3125, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q3-GS32-DAT-TST-C4', ppl=tensor(5.7468, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q2-GS32-DAT-TST-C4', ppl=tensor(3038830.2500, device='cuda:0')
framework='HF_AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM-Q2-GS8-DAT-TST-C4', ppl=tensor(194.4954, device='cuda:0')
framework='AutoGPTQ', model_name='Mistral-7B-Instruct-v0.2-GPTQ-4bit', ppl=tensor(5.2634, device='cuda:0')
framework='AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM2-Q4-GS128-DAF-TSF-RP2', ppl=tensor(5.3770, device='cuda:0')
framework='AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM2-Q3-GS32-DAF-TSF-RP2', ppl=tensor(5.9973, device='cuda:0')
framework='AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM2-Q2-GS16-DAF-TSF-RP2', ppl=tensor(843.2141, device='cuda:0')
framework='HF_AutoAWQ', model_name='Mistral-7B-Instruct-v0.2-AWQ-4bit', ppl=tensor(5.2952, device='cuda:0')
framework='AutoAWQ', model_name='mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMM', use_exllama2=True, fuse_layers=False, ppl=tensor(5.2767, device='cuda:0')
framework='AutoAWQ', model_name='mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMM', use_exllama2=True, fuse_layers=True, ppl=tensor(5.6382, device='cuda:0')
framework='AutoAWQ', model_name='mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMV', use_exllama2=False, fuse_layers=False, ppl=tensor(5.2758, device='cuda:0')
framework='AutoAWQ', model_name='mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMV', use_exllama2=False, fuse_layers=True, ppl=tensor(5.6376, device='cuda:0')
framework='AutoAWQ', model_name='mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMVF', use_exllama2=False, fuse_layers=False, ppl=tensor(5.2757, device='cuda:0')
framework='AutoAWQ', model_name='mistral-7b-instruct-v0.2-AWQM-Q4-GS128-GEMVF', use_exllama2=False, fuse_layers=True, ppl=tensor(5.6376, device='cuda:0')
framework='AutoAWQ', model_name='mistral-7b-instruct-v0.2-AWQM-Q4-GS128-Marlin', use_exllama2=False, fuse_layers=False, ppl=tensor(5.3254, device='cuda:0')
framework='AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM2-Q4-GS512-DAF-TSF-RP2', ppl=tensor(5.4355, device='cuda:0')
framework='AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM2-Q4-GS32-DAF-TSF-RP2', ppl=tensor(5.2747, device='cuda:0')
framework='AutoGPTQ', model_name='mistral-7b-instruct-v0.2-GPTQM2-Q4-GSNone-DAF-TSF-RP2', ppl=tensor(5.8418, device='cuda:0')
framework='AQLM', model_name='ISTA-DASLab/Mistral-7B-Instruct-v0.2-AQLM-2Bit-2x8', ppl=tensor(8.6088, device='cuda:0')
framework='AQLM', model_name='./models/Mistral-7B-Instruct-v0.2-AQLM-2bits-1x16', ppl=tensor(11.8658, device='cuda:0')
