{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Quantization + Fine-tuning\n",
    "This notebook presents how to fine-tuned a quantize model and how to quantize a fine-tuned model. From a FP16 model, both methods are roughly equivalent in terms of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning and quantization methods employed in this notebook are recommend from a perfomance/memory/speed point of view and are good in general, but according to your specific use-case you may use alternatives not presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's download some useful libraries - this might take several minutes\n",
    "\n",
    "!pip install torch transformers datasets accelerate peft\n",
    "!pip install torch transformers datasets optimum accelerate\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ_kernels.git -vvv\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ.git -vvv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning + Quantizing\n",
    "In this method, we first fine-tune the model and then quantize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some usefull libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from awq.models import MistralAWQForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Fine-tuning\n",
    "We use a dummy framework to LoRA fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuning parameters\n",
    "\n",
    "DEVICE = 'cuda:0'                                                                   # device where to load the model and perform the FT and quant on\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"                                              # model to FT+quant, can be a hugginface id or a local path\n",
    "ADAPTER_ID = \"mistral-7b-v0.1_lora_boolq\"                                           # name given to the fine-tuned adapter\n",
    "LORA_R = 16                                                                         # lora matrices' rank\n",
    "LORA_ALPHA = 8                                                                      # lora alpha scaling factor\n",
    "LORA_DROPOUT = 0.05                                                                 # dropout for lora matrices when training\n",
    "LORA_TARGET = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj']      # modules to apply lora FT on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token                                   # Needed for fine-tuning (comment out if your pre-trained model already has a padding token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                                             torch_dtype=torch.bfloat16,    # Working with bfloat16 is recommended when LoRA fine-tuning\n",
    "                                             device_map=DEVICE,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for LoRA fine-tuning\n",
    "\n",
    "lora_config = LoraConfig(r=LORA_R,\n",
    "                        lora_alpha=LORA_ALPHA,\n",
    "                        lora_dropout=LORA_DROPOUT,\n",
    "                        bias=\"none\",\n",
    "                        target_modules=LORA_TARGET)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the fine-tuning dataset\n",
    "# Out of rigour, one may also use a validation set, here we skip it\n",
    "\n",
    "def prepare_data(sample):\n",
    "    prompt = f\"{sample['passage']}\\nQuestion: {sample['question']}?\\nAnswer: {'yes' if sample['answer'] else 'no'}\"\n",
    "    inputs = tokenizer(prompt, padding='max_length', truncation=True, max_length=512)\n",
    "    inputs.update({'labels': inputs['input_ids']})\n",
    "    return inputs\n",
    "\n",
    "dataset = load_dataset('google/boolq', split='train')\n",
    "train_data = dataset.map(prepare_data).remove_columns(['answer', 'passage', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare the Trainer\n",
    "# The training parameters should be adapted to one's model, dataset and device\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ADAPTER_ID,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate= 5e-5,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    #report_to='wandb',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train :)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned adapter\n",
    "# This may be skipped if not useful\n",
    "\n",
    "trainer.model.save_pretrained(ADAPTER_ID)\n",
    "tokenizer.save_pretrained(ADAPTER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Quantization\n",
    "We quantize to 4 bit using AWQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\n",
    "\n",
    "QUANTIZED_MODEL_ID = f\"./models/mistral-7b-v0.1_lora-AWQ-Q4-GS128-GEMM\"     # name given to the quantized model\n",
    "BITS = 4                                                                    # number of bits to quantize to, currently only 4 bits is supported\n",
    "GROUP_SIZE = 128                                                            # size used for grouping in quantization algorithm, recommended 128, -1 quantizes per column\n",
    "VERSION = 'GEMM'                                                            # version to quantize to ['GEMM', 'GEMV', 'GEMV_fast', 'marlin'], recommended GEMM\n",
    "ZERO_POINT = True                                                           # whether to use zero-point quantization, recommend True, need False for marlin kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed you can load a fine-tuned adapter this way\n",
    "# Or you can use directly the model fine-tuned just above if the notebook has not been disconnected\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map=DEVICE,\n",
    "                                            )\n",
    "                                            \n",
    "model = PeftModel(model=model, peft_config=PeftConfig.from_pretrained(ADAPTER_ID))\n",
    "model.load_adapter(ADAPTER_ID, adapter_name='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the LoRA weights to get back to the original architecture\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Convert\" the model to an AWQ version\n",
    "# Note that we voluntarily specify the model architecture rather than using AutoAWQ\n",
    "# Using AutoAWQ requires using .from_pretrained which we want to circumvent here to avoid saving the full merged model\n",
    "# But it would be completely possible to save the full merged model and load the weights in an AWQ architecture using .from_pretrained\n",
    "\n",
    "quantization_config = {\"zero_point\": ZERO_POINT, \n",
    "                    \"q_group_size\": GROUP_SIZE, \n",
    "                    \"w_bit\": BITS, \n",
    "                    \"version\": VERSION, \n",
    "                    \"modules_to_not_convert\": [],\n",
    "                    }\n",
    "\n",
    "model = MistralAWQForCausalLM(model=model,\n",
    "                            model_type=model.config.architectures[0],\n",
    "                            is_quantized=False,\n",
    "                            config=model.config,\n",
    "                            quant_config=quantization_config,\n",
    "                            processor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the quantization - takes 15-25mn for a 7B-parameter model on GPU\n",
    "\n",
    "model.quantize(tokenizer, quant_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "\n",
    "model.save_quantized(QUANTIZED_MODEL_ID)\n",
    "tokenizer.save_pretrained(QUANTIZED_MODEL_ID)\n",
    "with open(f\"{QUANTIZED_MODEL_ID}/quant_config.json\", \"w\") as file:\n",
    "    json.dump(quantization_config, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantizing + Fine-tuning\n",
    "\n",
    "In this method we first quantize the model and then fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is more memory efficient but slower and requires more steps. The performance is roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some usefull libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from awq.models import MistralAWQForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\\\n",
    "\n",
    "DEVICE = 'cuda:0'                                                           # device where to load the model and perform the FT and quant on\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"                                      # model to FT+quant, can be a hugginface id or a local path\n",
    "QUANTIZED_MODEL_ID = f\"./models/mistral-7b-v0.1-AWQ-Q4-GS128-GEMM\"     # name given to the quantized model\n",
    "BITS = 4                                                                    # number of bits to quantize to, currently only 4 bits is supported\n",
    "GROUP_SIZE = 128                                                            # size used for grouping in quantization algorithm, recommended 128, -1 quantizes per column\n",
    "VERSION = 'GEMM'                                                            # version to quantize to ['GEMM', 'GEMV', 'GEMV_fast', 'marlin'], recommended GEMM\n",
    "ZERO_POINT = True                                                           # whether to use zero-point quantization, recommend True, need False for marlin kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
