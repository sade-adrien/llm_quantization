{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Quantization and Fine-tuning\n",
    "This notebook presents how to fine-tuned a quantize model and how to quantize a fine-tuned model. From a FP16 model, both methods are roughly equivalent in terms of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning and quantization methods employed in this notebook are recommend from a perfomance/memory/speed point of view and are good in general, but according to your specific use-case you may use alternatives not presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's download some useful libraries - this might take several minutes\n",
    "\n",
    "!pip install torch transformers datasets accelerate peft\n",
    "!pip install torch transformers datasets optimum accelerate\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ_kernels.git -vvv\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ.git -vvv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning + Quantizing\n",
    "In this method, we first fine-tune the model and then quantize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some usefull libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from awq.models import MistralAWQForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Fine-tuning\n",
    "We use a dummy framework to LoRA fine-tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuning parameters\n",
    "\n",
    "DEVICE = 'cuda:0'                                                                   # device where to load the model and perform the FT and quant on\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"                                              # model to FT+quant, can be a hugginface id or a local path\n",
    "ADAPTER_ID = \"mistral-7b-v0.1_lora_boolq\"                                           # name given to the fine-tuned adapter\n",
    "LORA_R = 16                                                                         # lora matrices' rank\n",
    "LORA_ALPHA = 8                                                                      # lora alpha scaling factor\n",
    "LORA_DROPOUT = 0.05                                                                 # dropout for lora matrices when training\n",
    "LORA_TARGET = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj']      # modules to apply lora FT on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token                                   # Needed for fine-tuning (comment out if your pre-trained model already has a padding token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                                             torch_dtype=torch.bfloat16,    # Working with bfloat16 is recommended when LoRA fine-tuning\n",
    "                                             device_map=DEVICE,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for LoRA fine-tuning\n",
    "\n",
    "lora_config = LoraConfig(r=LORA_R,\n",
    "                        lora_alpha=LORA_ALPHA,\n",
    "                        lora_dropout=LORA_DROPOUT,\n",
    "                        bias=\"none\",\n",
    "                        target_modules=LORA_TARGET)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the fine-tuning dataset\n",
    "# Out of rigour, one may also use a validation set, here we skip it\n",
    "\n",
    "def prepare_data(sample):\n",
    "    prompt = f\"{sample['passage']}\\nQuestion: {sample['question']}?\\nAnswer: {'yes' if sample['answer'] else 'no'}\"\n",
    "    inputs = tokenizer(prompt, padding='max_length', truncation=True, max_length=512)\n",
    "    inputs.update({'labels': inputs['input_ids']})\n",
    "    return inputs\n",
    "\n",
    "dataset = load_dataset('google/boolq', split='train')\n",
    "train_data = dataset.map(prepare_data).remove_columns(['answer', 'passage', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare the Trainer\n",
    "# The training parameters should be adapted to one's model, dataset and device\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ADAPTER_ID,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate= 5e-5,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    #report_to='wandb',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train :)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned adapter\n",
    "# This may be skipped if not useful\n",
    "\n",
    "trainer.model.save_pretrained(ADAPTER_ID)\n",
    "tokenizer.save_pretrained(ADAPTER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Quantization\n",
    "We quantize to 4 bit using AWQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\n",
    "\n",
    "QUANTIZED_MODEL_ID = \"mistral-7b-v0.1_lora_merged-AWQ-Q4-GS128-GEMM\"               # name given to the quantized model\n",
    "BITS = 4                                                                    # number of bits to quantize to, currently only 4 bits is supported\n",
    "GROUP_SIZE = 128                                                            # size used for grouping in quantization algorithm, recommended 128, -1 quantizes per column\n",
    "VERSION = 'GEMM'                                                            # version to quantize to ['GEMM', 'GEMV', 'GEMV_fast', 'marlin'], recommended GEMM\n",
    "ZERO_POINT = True                                                           # whether to use zero-point quantization, recommend True, need False for marlin kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed you can load a fine-tuned adapter this way\n",
    "# Or you can use directly the model fine-tuned just above if the notebook has not been disconnected\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map=DEVICE,\n",
    "                                            )\n",
    "                                            \n",
    "model = PeftModel(model=model, peft_config=PeftConfig.from_pretrained(ADAPTER_ID))\n",
    "model.load_adapter(ADAPTER_ID, adapter_name='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the LoRA weights to get back to the original architecture\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Convert\" the model to an AWQ version\n",
    "# Note that we voluntarily specify the model architecture from awq.models rather than using AutoAWQ\n",
    "# Using AutoAWQ requires using .from_pretrained which we want to circumvent here to avoid saving the full merged model\n",
    "# But it would be completely possible to save the full merged model and load the weights in an AWQ architecture using .from_pretrained\n",
    "\n",
    "quantization_config = {\"zero_point\": ZERO_POINT, \n",
    "                    \"q_group_size\": GROUP_SIZE, \n",
    "                    \"w_bit\": BITS, \n",
    "                    \"version\": VERSION, \n",
    "                    \"modules_to_not_convert\": [],\n",
    "                    }\n",
    "\n",
    "model = MistralAWQForCausalLM(model=model,\n",
    "                            model_type=model.config.architectures[0],\n",
    "                            is_quantized=False,\n",
    "                            config=model.config,\n",
    "                            quant_config=quantization_config,\n",
    "                            processor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the quantization - takes 15-25mn for a 7B-parameter model on GPU\n",
    "\n",
    "model.quantize(tokenizer, quant_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "\n",
    "model.save_quantized(QUANTIZED_MODEL_ID)\n",
    "tokenizer.save_pretrained(QUANTIZED_MODEL_ID)\n",
    "with open(f\"{QUANTIZED_MODEL_ID}/quant_config.json\", \"w\") as file:\n",
    "    json.dump(quantization_config, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantizing + Fine-tuning\n",
    "\n",
    "In this method we first quantize the model and then fine-tune it. We use QLoRA method. Finally, we optionally propose to merge and re-quantize the whole model with AWQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is more memory efficient but slower and requires more steps. The performance is roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some usefull libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model\n",
    "from bitsandbytes.functional import quantize_nf4, dequantize_nf4\n",
    "from awq.models import MistralAWQForCausalLM\n",
    "from peft.utils.other import transpose\n",
    "from awq import AutoAWQForCausalLM\n",
    "from safetensors import safe_open\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Quantization\n",
    "We first quantize the model using QLoRA quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\n",
    "\n",
    "DEVICE = 'cuda:0'                                                           # device where to load the model and perform the FT and quant on\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"                                      # model to FT+quant, can be a hugginface id or a local path\n",
    "QUANTIZED_ADAPTER_ID = \"mistral-7b-v0.1_qlora-AWQ-Q4-GS128-GEMM\"            # name given to the quantized fine-tuned adapter\n",
    "BITS = 4                                                                    # number of bits to quantize to, 4 or 8, we ONLY recommend 4 --> this parameter is not used in what follows\n",
    "QUANT_TYPE = 'nf4'                                                          # quantization scheme ['nf4', 'fp4'], we recommend 'nf4'\n",
    "DOUBLE_QUANT = False                                                        # wether to quantize the quantization constante, reduce memory but increase latency - same performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token                                           # Needed for fine-tuning (comment out if your pre-trained model already has a padding token)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_compute_dtype=torch.bfloat16,      # Working with bfloat16 is recommended when LoRA fine-tuning\n",
    "                                        bnb_4bit_quant_type=QUANT_TYPE,\n",
    "                                        bnb_4bit_use_double_quant=DOUBLE_QUANT,\n",
    "                                        )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                                             torch_dtype = torch.bfloat16,          # Working with bfloat16 is recommended when LoRA fine-tuning\n",
    "                                             device_map=DEVICE,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Fine-tuning\n",
    "We use LoRA (QLoRA) to fine-tune the model on a dummy framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fine-tuning parameters\n",
    "\n",
    "LORA_R = 16                                                                         # lora matrices' rank\n",
    "LORA_ALPHA = 8                                                                      # lora alpha scaling factor\n",
    "LORA_DROPOUT = 0.05                                                                 # dropout for lora matrices when training\n",
    "LORA_TARGET = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'down_proj']      # modules to apply lora FT on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for LoRA fine-tuning\n",
    "\n",
    "lora_config = LoraConfig(r=LORA_R,\n",
    "                        lora_alpha=LORA_ALPHA,\n",
    "                        lora_dropout=LORA_DROPOUT,\n",
    "                        bias=\"none\",\n",
    "                        target_modules=LORA_TARGET)\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)   #! has to be applied BEFORE get_peft_model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the fine-tuning dataset\n",
    "# Out of rigour, one may also use a validation set, here we skip it\n",
    "\n",
    "def prepare_data(sample):\n",
    "    prompt = f\"{sample['passage']}\\nQuestion: {sample['question']}?\\nAnswer: {'yes' if sample['answer'] else 'no'}\"\n",
    "    inputs = tokenizer(prompt, padding='max_length', truncation=True, max_length=512)\n",
    "    inputs.update({'labels': inputs['input_ids']})\n",
    "    return inputs\n",
    "\n",
    "dataset = load_dataset('google/boolq', split='train')\n",
    "train_data = dataset.map(prepare_data).remove_columns(['answer', 'passage', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare the Trainer\n",
    "# The training parameters should be adapted to one's model, dataset and device\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=QUANTIZED_ADAPTER_ID,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate= 5e-5,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.05,\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    #report_to='wandb',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train :)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned adapter\n",
    "# This may be skipped if not useful\n",
    "\n",
    "trainer.model.save_pretrained(QUANTIZED_ADAPTER_ID)\n",
    "tokenizer.save_pretrained(QUANTIZED_ADAPTER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the model is composed of the 4bit-quantized base and the FP16 LoRA-weights. One can work with this. \n",
    "\n",
    "However, if one wants to use the model completely quantized to 4 bits, and with its original architecture, step C which follows is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: to properly load the model with its adapter do this\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                        bnb_4bit_compute_dtype=torch.bfloat16,    \n",
    "                                        bnb_4bit_quant_type=QUANT_TYPE,\n",
    "                                        bnb_4bit_use_double_quant=DOUBLE_QUANT,\n",
    "                                        )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                                             torch_dtype = torch.float16,      \n",
    "                                             device_map=DEVICE,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                            )\n",
    "                    \n",
    "model = PeftModel(model=model, peft_config=PeftConfig.from_pretrained(QUANTIZED_ADAPTER_ID))\n",
    "model.load_adapter(QUANTIZED_ADAPTER_ID, adapter_name='default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Dequantize, merge and re-quantize\n",
    "\n",
    "The final, optional, step is to make the model the exact same architecture as it originally is and entirely 4 bits. To do that we first dequantize the base model back to FP16, merge the LoRA weights within the base and finally requantize with a powerful quantization method such as AWQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dequantize base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model in FP16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                                             torch_dtype = torch.float16,         \n",
    "                                             device_map=DEVICE,\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dequantize the base model\n",
    "# We have to do it manually as peft enforces the re-quantization in its implemented methods, though we can still use some low-level functions from peft and bnb\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if any(s in name for s in ['embed', 'head', 'norm']):                                       # we keep the embeddings, head and normalization layers intact as they are always kept in higher precision\n",
    "        continue\n",
    "    tensor, quant_state = quantize_nf4(param.data)                                              # we quantize and dequantize other layers to reproduce the quantization error that was introduces when fine-tuning\n",
    "    param.data = dequantize_nf4(tensor, quant_state=quant_state, absmax=quant_state.absmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Merge LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LoRA weights in a dictionary along with the PEFT config\n",
    "\n",
    "lora_weights = {}\n",
    "with safe_open(f\"{QUANTIZED_ADAPTER_ID}/adapter_model.safetensors\", framework=\"pt\", device=DEVICE) as file:\n",
    "   for key in file.keys():\n",
    "       lora_weights[key] = file.get_tensor(key)\n",
    "\n",
    "with open(f\"{QUANTIZED_ADAPTER_ID}/adapter_config.json\", \"r\") as file:\n",
    "    peft_config = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    layer_name = name[:-7]                     \n",
    "    lora_A, lora_B = None, None\n",
    "\n",
    "    for key, val in lora_weights.items():                   # we loop on the lora_weights' keys to prevent inconsistencies in layer names, though note this name-matching part might require more work in some cases\n",
    "        if f\"{layer_name}.lora_A\" in key:\n",
    "            lora_A = val.float()\n",
    "        elif f\"{layer_name}.lora_B\" in key:\n",
    "            lora_B = val.float()\n",
    "    \n",
    "        if lora_A is not None and lora_B is not None:\n",
    "            param.data += transpose(lora_B @ lora_A, peft_config['fan_in_fan_out']) * peft_config['lora_alpha'] / peft_config['r']\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\n",
    "\n",
    "QUANTIZED_MODEL_ID = \"mistral-7b-v0.1_qlora_merged-AWQ-Q4-GS128-GEMM\"       # name given to the quantized model\n",
    "BITS = 4                                                                    # number of bits to quantize to, currently only 4 bits is supported\n",
    "GROUP_SIZE = 128                                                            # size used for grouping in quantization algorithm, recommended 128, -1 quantizes per column\n",
    "VERSION = 'GEMM'                                                            # version to quantize to ['GEMM', 'GEMV', 'GEMV_fast', 'marlin'], recommended GEMM\n",
    "ZERO_POINT = True                                                           # whether to use zero-point quantization, recommend True, need False for marlin kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Convert\" the model to an AWQ version\n",
    "# Note that we voluntarily specify the model architecture from awq.models rather than using AutoAWQ\n",
    "# Using AutoAWQ requires using .from_pretrained which we want to circumvent here to avoid saving the full merged model\n",
    "# But it would be completely possible to save the full merged model and load the weights in an AWQ architecture using .from_pretrained\n",
    "\n",
    "quantization_config = {\"zero_point\": ZERO_POINT, \n",
    "                    \"q_group_size\": GROUP_SIZE, \n",
    "                    \"w_bit\": BITS, \n",
    "                    \"version\": VERSION, \n",
    "                    \"modules_to_not_convert\": [],\n",
    "                    }\n",
    "\n",
    "model = MistralAWQForCausalLM(model=model,\n",
    "                            model_type=model.config.architectures[0],\n",
    "                            is_quantized=False,\n",
    "                            config=model.config,\n",
    "                            quant_config=quantization_config,\n",
    "                            processor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the quantization - takes 15-25mn for a 7B-parameter model on GPU\n",
    "\n",
    "model.quantize(tokenizer, quant_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "\n",
    "model.save_quantized(QUANTIZED_MODEL_ID)\n",
    "tokenizer.save_pretrained(QUANTIZED_MODEL_ID)\n",
    "with open(f\"{QUANTIZED_MODEL_ID}/quant_config.json\", \"w\") as file:\n",
    "    json.dump(quantization_config, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
