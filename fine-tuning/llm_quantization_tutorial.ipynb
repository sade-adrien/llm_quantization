{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Quantization\n",
    "This notebook presents two different methods to quantize LLMs: GPTQ and AWQ. We recommend using AWQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ\n",
    "GPTQ is a quantization algorithm that quantizes LLMs' weights to 8, 4, 3 or 2 bit precision using an iterative 2nd order optimization process and a small calibration dataset.\n",
    "\n",
    "More information at: https://arxiv.org/pdf/2210.17323.pdf\n",
    "\n",
    "The tutorial provided uses AutoGPTQ implementation: https://github.com/AutoGPTQ/AutoGPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's download some useful libraries - this might take several minutes\n",
    "\n",
    "!pip install torch transformers datasets optimum accelerate\n",
    "!pip install git+https://github.com/AutoGPTQ/AutoGPTQ.git -vvv --no-build-isolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's import some usefull libraries\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\n",
    "\n",
    "DEVICE = 'cuda:0'                                                       # device where to load the model and perform the quantization\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"                         # model to quantize, can be a hugginface id or a local path\n",
    "QUANTIZED_MODEL_ID = \"mistral-7b-v0.1_lora_boolq-GPTQ-Q4-GS128-AOT-TST\"   #name given to the quantized model\n",
    "BITS = 4                                                                # number of bits to quantize to, recommended 4\n",
    "GROUP_SIZE = 128                                                        # size used for grouping in quantization algorithm, recommended 128, -1 quantizes per column\n",
    "ACT_ORDER = True                                                        # whether to quantize columns in order of decreasing activation size, recommended True\n",
    "TRUE_SEQUENTIAL = True                                                  # whether to perform sequential quantization even within a single transformer block, recommended True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the model with the quantization configuration defined above\n",
    "\n",
    "quantization_config = BaseQuantizeConfig(bits = BITS,\n",
    "                                group_size = GROUP_SIZE,\n",
    "                                desc_act = ACT_ORDER,\n",
    "                                true_sequential = TRUE_SEQUENTIAL,\n",
    "                                )\n",
    "model = AutoGPTQForCausalLM.from_pretrained(MODEL_ID, quantization_config).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = './models/mistral-7b-v0.1_peft'\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset that'll be used for quantization (calibration dataset)\n",
    "# Note: this dataset can be adapted to a specific fine-tuning, but as a general rule it should be similarly built as the model's training data\n",
    "# Thus, for a foundational model such as Mistral-7B, a general-knowledge pre-training dataset extract works well\n",
    "# We use here a 128-sample extract from the RedPajama2 of 2048 tokens\n",
    "\n",
    "dataset = load_dataset('sade-adrien/quantization_samples', split='train')\n",
    "quantization_samples = [tokenizer(sample, truncation=True, max_length=2048) for sample in dataset['raw_content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the quantization - takes 15-25mn for a 7B-parameter model on GPU and above dataset\n",
    "\n",
    "model.quantize(quantization_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "\n",
    "model.save_quantized(QUANTIZED_MODEL_ID)\n",
    "tokenizer.save_pretrained(QUANTIZED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quantized model\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(QUANTIZED_MODEL_ID,\n",
    "                                            use_marlin = False,         # use marlin kernel\n",
    "                                            disable_exllama = False,    # use exllama-1 kernel\n",
    "                                            disable_exllamav2 = True,   # use exllama-2 kernel\n",
    "                                            device=DEVICE,\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANTIZED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load the quantized model with HuggingFace framework, not recommended\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from optimum.gptq import load_quantized_model\n",
    "from accelerate import init_empty_weights\n",
    "import torch\n",
    "\n",
    "with init_empty_weights():\n",
    "    empty_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map='cpu')\n",
    "empty_model.tie_weights()\n",
    "model = load_quantized_model(empty_model, save_folder=QUANTIZED_MODEL_ID, device_map={'': DEVICE})\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANTIZED_MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWQ\n",
    "AWQ is an algorithm based on the hypothesis that a large percentage of the weights have a small impact on the output. The small fraction of important (aka salient) weights should be preserved in better precision during quantization to keep high performance. To this end, they are scaled efficiently right before quantization. \n",
    "\n",
    "More information at: https://arxiv.org/pdf/2306.00978.pdf\n",
    "\n",
    "The tutorial provided uses AutoAWQ implementation: https://github.com/casper-hansen/AutoAWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, this algorithm is not about the quantization itself but rather an improvement. This means, we have to use a quantization process along AWQ that is orthogonal. GPTQ is a perfect candidate and is commonly used along AWQ: this is the one used under the hood in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's download some useful libraries - this might take several minutes\n",
    "\n",
    "!pip install torch transformers datasets optimum accelerate\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ_kernels.git -vvv\n",
    "!pip install git+https://github.com/casper-hansen/AutoAWQ.git -vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from awq.models import MistralAWQForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantization parameters\n",
    "TASK = 'boolq'\n",
    "\n",
    "DEVICE = 'cuda:0'                                                       # device where to load the model and perform the quantization\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"                         # model to quantize, can be a hugginface id or a local path\n",
    "QUANTIZED_MODEL_ID = f\"./models/mistral-7b-v0.1_lora_{TASK}-AWQ-Q4-GS128-GEMM\"       #name given to the quantized model\n",
    "BITS = 4                                                                # number of bits to quantize to, currently only 4 bits is supported\n",
    "GROUP_SIZE = 128                                                        # size used for grouping in quantization algorithm, recommended 128, -1 quantizes per column\n",
    "VERSION = 'GEMM'                                                        # version to quantize to ['GEMM', 'GEMV', 'GEMV_fast', 'marlin'], recommended GEMM\n",
    "ZERO_POINT = True                                                       # whether to use zero-point quantization, recommend True, need False for marlin kernel\n",
    "adapter = f\"./models/mistral-7b-v0.1_lora_{TASK}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5918f16090f433cbf07b73629a9c3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast = False)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID,\n",
    "                                             torch_dtype = torch.float16,       \n",
    "                                             device_map=DEVICE,\n",
    "                                             cache_dir='/mnt/esperanto/et/huggingface/hub'\n",
    "                                            )\n",
    "x = model.model.layers[0].self_attn.q_proj.weight.clone()\n",
    "model = PeftModel(model=model,\n",
    "                peft_config=PeftConfig.from_pretrained(adapter),\n",
    "                )\n",
    "model.load_adapter(adapter, adapter_name='default')\n",
    "model = model.merge_and_unload();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = {\"zero_point\": ZERO_POINT, \n",
    "                    \"q_group_size\": GROUP_SIZE, \n",
    "                    \"w_bit\": BITS, \n",
    "                    \"version\": VERSION, \n",
    "                    \"modules_to_not_convert\": [],\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistralAWQForCausalLM(model=model,\n",
    "                            model_type=model.config.architectures[0],\n",
    "                            is_quantized=False,\n",
    "                            config=model.config,\n",
    "                            quant_config=quantization_config,\n",
    "                            processor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|██████████| 32/32 [13:46<00:00, 25.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# Perform the quantization - takes 15-25mn for a 7B-parameter model on GPU\n",
    "model.quantize(tokenizer, quant_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.\\nQuestion: do iran and afghanistan speak the same language?\\nAnswer: no</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Persian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.\\nQuestion: do iran and afghanistan speak the same language?\\nAnswer:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "\n",
    "model.save_quantized(QUANTIZED_MODEL_ID)\n",
    "tokenizer.save_pretrained(QUANTIZED_MODEL_ID)\n",
    "with open(f\"{QUANTIZED_MODEL_ID}/quant_config.json\", \"w\") as file:\n",
    "    json.dump(quantization_config, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 32/32 [00:04<00:00,  7.69it/s]\n",
      "Fusing layers...: 100%|██████████| 32/32 [00:00<00:00, 371.37it/s]\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# Load the quantized model\n",
    "from awq import AutoAWQForCausalLM\n",
    "model = AutoAWQForCausalLM.from_quantized(QUANTIZED_MODEL_ID, \n",
    "                                            fuse_layers=True,               # fusing layers accelerates inference greatly\n",
    "                                            use_exllama=False,              # use exllama-1 kernel\n",
    "                                            use_exllama_v2=True,            # use exllama-2 kernel\n",
    "                                            max_seq_len=512,                # max sequence length, used when fusing layer to allocate memory\n",
    "                                            device_map={'': DEVICE},\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANTIZED_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load the quantized model with HuggingFace framework, not recommended\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n",
    "\n",
    "quantization_config = AwqConfig(version=\"gemm\",                                                            # version of model to be loaded\n",
    "                                exllama_config={\"version\": 2, \"max_input_len\": 2048, \"max_batch_size\": 8}, # use exllama kernel, can be omitted\n",
    "                                do_fuse = True,                                                            # fusing layers accelerates inference greatly\n",
    "                                fuse_max_seq_len = 512,                                                    # max sequence length, used when fusing layer to allocate memory\n",
    "                                )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(QUANTIZED_MODEL_ID,\n",
    "                                            quantization_config=quantization_config,\n",
    "                                            device_map={'': DEVICE},\n",
    "                                        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(QUANTIZED_MODEL_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantization_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
