# Benchmark

Tests Ran on A100 80Gb


Generating 300 tokens with ~75-token prompts
KV Cache is used
Flash attention is not used
BF16 seems to impact a little bit efficiency (compared to FP16) but famous for improving performance
For QLoRA, quant type does not impact efficiency, probably only perf (recommended in 4 bits: nf4)
For QLoRA, double quant decrease further memory but at the cost of a little speed decrease
Batch size is always 1




_* (INT8) Embeddings, Normalization and final Head layers are kept in higher precision (~ 131M + 266K + 131M = 262M in Mistral-7B i.e. 3.6%).

_* (FP4/NF4) Same but ~7% of model params because of 4 bit model implementation (less total params but same number of higher precision).

** Cast in FP16 during quantization.

*** LLM.int8() mixed-computation implementation.

**** Info is from Aug22, but couldn't find a more recent update

***** For GPTQ models, damping is .1%

^ exllama 2 is said to be an improvement mostly for EXL2 model format. Through EXL2 format this kernel can be efficiently used for different quantization (2, 3, 4, 5, 6 and 8 bit and averages)

^^ A 3-bit specialized CUDA kernel was developed in the original GPTQ repository (https://github.com/IST-DASLab/gptq.git) but it doesn't seem to be available through AutoGPT (or HF). Unfortunatelly, the original GPTQ repository is only compatible with Bloom and OPT models.

^^^ In HF, AutoAWQ integration quantizes using GPTQ algorithm (GPTQ and AWQ are orthogonal and can be used simultaneously).


| Model      | Flavor                                                                                      | Framework                 | Dtype             | Secondary<br>Dtype\* | Double<br>Quant | Computations<br>(Activations) | Custom CUDA<br>Kernel         | Size<br>on GPU (Gb) | Peak Memory<br>on GPU (Gb) | Inference Speed<br>(tokens/s)                | Perplexity<br>Subset Wikitext2<br>(300-Qualitative) | Quantization<br>Info                                                    |
| ---------- | ------------------------------------------------------------------------------------------- | ------------------------- | ----------------- | -------------------- | --------------- | ----------------------------- | ----------------------------- | ------------------- | -------------------------- | -------------------------------------------- | --------------------------------------------------- | ----------------------------------------------------------------------- |
| Mistral-7B | Instruct-v0.2                                                                               | HF                        | FP32              | \-                   | \-              | FP32                          | Yes                           | 30.5                | 31.0                       | 39.1                                         | 5.170                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF                        | FP16              | \-                   | \-              | FP16                          | Yes                           | 15.7                | 16.0                       | 46.4                                         | 5.170                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF                        | BF16              | \-                   | \-              | BF16 (?)                      | (?)                           | 15.7                | 16.0                       | 46.1                                         | 5.169                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / LLM.int8()           | INT8              | FP32\*\*             | \-              | INT8/FP16\*\*\*               | No\*\*\*\*                    | 9.6                 | 20.0                       | 5.9                                          |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / LLM.int8()           | INT8              | FP16                 | \-              | INT8/FP16\*\*\*               | No\*\*\*\*                    | 8.7                 | 13.9                       | 5.9                                          | 5.194                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / LLM.int8()           | INT8              | BF16\*\*             | \-              | INT8/FP16\*\*\*               | No\*\*\*\*                    | 8.7                 | 14.4                       | 5.7                                          |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | FP32                 | No              | FP32                          | Yes                           | 6.6                 | 7.1                        | 32.1                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 (uINT8)       | FP32                 | No              | FP16                          | Yes                           | 6.6                 | 7.1                        | 33.7                                         | 5.370                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | NF4 (uINT8)       | FP32                 | No              | FP16                          | Yes                           | 6.6                 | 7.1                        | 33.7                                         | 5.298                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | FP32                 | No              | BF16                          | Yes                           | 6.6                 | 7.1                        | 31.8                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | FP16                 | No              | FP32                          | Yes                           | 5.7                 | 6.4                        | 27.6                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 (uINT8)       | FP16                 | No              | FP16                          | Yes                           | 5.7                 | 6.0                        | 31.5                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | NF4 (uINT8)       | FP16                 | No              | FP16                          | Yes                           | 5.7                 | 6.0                        | 31.5                                         | 5.298                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | FP16                 | No              | BF16                          | Yes                           | 5.7                 | 6.2                        | 27.1                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | BF16                 | No              | FP32                          | Yes                           | 5.7                 | 6.0                        | 30.4                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | BF16                 | No              | FP16                          | Yes                           | 5.7                 | 6.0                        | 31.7                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | BF16                 | No              | BF16                          | Yes                           | 5.7                 | 6.0                        | 31.1                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | FP32                 | Yes             | FP32                          | Yes                           | 6.2                 | 6.7                        | 26.2                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | FP32                 | Yes             | FP16                          | Yes                           | 6.2                 | 6.7                        | 25.5                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 (uINT8)       | FP16                 | Yes             | FP16                          | Yes                           | 5.3                 | 5.6                        | 24.6                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | NF4 (uINT8)       | FP16                 | Yes             | FP16                          | Yes                           | 5.3                 | 5.6                        | 24.6                                         | 5.295                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / QLoRA                | FP4 / NF4 (uINT8) | FP16                 | Yes             | BF16                          | Yes                           | 5.3                 | 5.8                        | 22.6                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / Quanto               | FP8               | FP32                 | \-              | FP32 (?)                      | No                            | 9.6                 | 10.6                       | 10.0                                         | 5.178                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / Quanto               | INT8              | FP32                 | \-              | FP32 (?)                      | No                            | 9.6                 | 10.3                       | 13.9                                         | 5.182                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / Quanto               | INT8              | FP16                 | \-              | FP16 (?)                      | No                            | 8.7                 | 9.0                        | 16.0                                         |                                                     | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | HF / Quanto               | INT4 (uINT8)      | FP32                 | \-              | FP32 (?)                      | No                            | 6.7                 | 7.6                        | 6.4                                          | 5.514                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | llama.cpp (python)        | Q8_0              | \-                   | \-              |                               |                               | 8.1                 | 8.4                        | 83.3                                         | 5.178                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | llama.cpp (python)        | Q4_K_M            | \-                   | \-              |                               |                               | 4.9                 | 5.2                        | 91.7                                         | 5.245                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2                                                                               | llama.cpp (python)        | Q3_K_L            | \-                   | \-              |                               |                               | 4.4                 | 4.6                        | 77.7                                         | 5.360                                               | \-                                                                      |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, Act_order=True, True_sequential = True               | HF / AutoGPTQ\*\*\*\*\*   | 4 bits            | FP16                 | \-              | FP16                          | No                            | 5.3                 | 5.6                        | 15.1                                         | 5.263                                               | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, Act_order=True, True_sequential = True               | HF / AutoGPTQ\*\*\*\*\*   | 4 bits            | FP16                 | \-              | FP16                          | exllama-1                     | 5.5                 | 5.7                        | 42.5                                         | 5.263                                               | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, Act_order=True, True_sequential = True               | HF / AutoGPTQ\*\*\*\*\*   | 4 bits            | FP16                 | \-              | FP16                          | exllama-2^                    | 6.4                 | 6.6                        | 41.0                                         | 5.263                                               | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, Act_order=True, True_sequential = True               | HF / AutoGPTQ\*\*\*\*\*   | 8 bits            | FP16                 | \-              | FP16                          | No                            | 8.8                 | 9.1                        | 12.4                                         | NaN                                                 | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, Act_order=True, True_sequential = True                 | HF / AutoGPTQ\*\*\*\*\*   | 4 bits            | FP16                 | \-              | FP16                          | exllama-1                     | 5.5                 | 5.7                        | 40.9                                         | 5.425                                               | 128 2048-long samples from C4, Peak memory = 26Gb, time = ~30mn         |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, Act_order=True, True_sequential = False                | HF / AutoGPTQ\*\*\*\*\*   | 4 bits            | FP16                 | \-              | FP16                          | exllama-1                     | 5.5                 | 5.7                        | 41.0                                         | 5.467                                               | 128 2048-long samples from C4, Peak memory = 26Gb, time = ~23mn         |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, Act_order=False, True_sequential = True                | HF / AutoGPTQ\*\*\*\*\*   | 4 bits            | FP16                 | \-              | FP16                          | exllama-1                     | 5.5                 | 5.7                        | 41.8                                         | 5.423                                               | 128 2048-long samples from C4, Peak memory = 26Gb, time = ~30mn         |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=None, Act_order=True, True_sequential = True                | HF / AutoGPTQ\*\*\*\*\*   | 4 bits            | FP16                 | \-              | FP16                          | exllama-1                     | 5.4                 | 5.6                        | 41.4                                         | 5.982                                               | 128 2048-long samples from C4, Peak memory = 26Gb, time = ~30mn         |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, Act_order=True, True_sequential = True                 | HF / AutoGPTQ\*\*\*\*\*   | 3 bits            | FP16                 | \-              | FP16                          | No^^                          | 4.5                 | 4.8                        | 6.7                                          | 634000                                              | 128 2048-long samples from C4, Peak memory = 26Gb, time = ~30mn         |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=32, Act_order=True, True_sequential = True                  | HF / AutoGPTQ\*\*\*\*\*   | 3 bits            | FP16                 | \-              | FP16                          | No^^                          | 4.9                 | 5.3                        | 5.9                                          | 5.747                                               | 128 2048-long samples from C4, Peak memory = 31Gb, time = ~30mn         |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=32, Act_order=True, True_sequential = True                  | HF / AutoGPTQ\*\*\*\*\*   | 2 bits            | FP16                 | \-              | FP16                          | No^^ (?)                      | 3.9                 | 4.2                        | 15.7                                         | 3039000                                             | 128 2048-long samples from C4, Peak memory = 31Gb, time = ~30mn         |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=8, Act_order=True, True_sequential = True                   | HF / AutoGPTQ\*\*\*\*\*   | 2 bits            | FP16                 | \-              | FP16                          | No^^ (?)                      | 5.4                 | 5.7                        | 13.0                                         | 195.0                                               | 128 2048-long samples from C4, Peak memory = 32Gb, time = ~30mn         |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, Act_order=True, True_sequential = True               | AutoGPTQ\*\*\*\*\*        | 4 bits            | FP16                 | \-              | FP16                          | No                            | 5.2                 | 5.4                        | 15.5                                         | 5.263                                               | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, Act_order=True, True_sequential = True               | AutoGPTQ\*\*\*\*\*        | 4 bits            | FP16                 | \-              | FP16                          | exllama-1                     | 5.4                 | 5.6                        | 47.1                                         | 5.263                                               | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, Act_order=True, True_sequential = True               | AutoGPTQ\*\*\*\*\*        | 4 bits            | FP16                 | \-              | FP16                          | exllama-2^                    | 6.3                 | 6.5                        | 48.2                                         | 5.263                                               | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (Adrien2)<br>Group=128, Act_order=False, True_sequential = False              | AutoGPTQ\*\*\*\*\*        | 4 bits            | FP16                 | \-              | FP16                          | exllama-2^                    | 6.4                 | 6.6                        | 49.6<br>(batch_32=465)                       | 5.377                                               | 128 2048-long samples from RedPajama2, Peak memory = 24Gb, time = ~18mn |
| Mistral-7B | Instruct-v0.2 (Adrien2)<br>Group=128, Act_order=False, True_sequential = False              | AutoGPTQ\*\*\*\*\*        | 4 bits            | FP16                 | \-              | FP16                          | marlin                        | 5.3                 | 5.5                        | 49<br>(batch_32=529)                         | 5.377                                               | 128 2048-long samples from RedPajama2, Peak memory = 24Gb, time = ~18mn |
| Mistral-7B | Instruct-v0.2 (Adrien2)<br>Group=32, Act_order=False, True_sequential = False               | AutoGPTQ\*\*\*\*\*        | 3 bits            | FP16                 | \-              | FP16                          | cuda_old                      | 4.9                 | 5.2                        | 36.7                                         | 5.997                                               | 128 2048-long samples from RedPajama2, Peak memory = 24Gb, time = ~18mn |
| Mistral-7B | Instruct-v0.2 (Adrien2)<br>Group=16, Act_order=False, True_sequential = False               | AutoGPTQ\*\*\*\*\*        | 2 bits            | FP16                 | \-              | FP16                          | cuda_old                      | 4.4                 | 4.6                        | 38.1                                         | 843.2                                               | 128 2048-long samples from RedPajama2, Peak memory = 24Gb, time = ~18mn |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, version=GEMM                                         | HF / AutoAWQ<br>(GPTQ^^^) | 4 bits            | FP16                 | \-              | FP16                          | No                            | 5.3                 | 6.1                        | 2.5                                          |                                                     | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (TheBloke)<br>Group=128, version=GEMM                                         | HF / AutoAWQ<br>(GPTQ^^^) | 4 bits            | FP16                 | \-              | FP16                          | exllama-2^                    | 5.3                 | 5.6                        | 32.5                                         | 5.295                                               | VMware open-instruct dataset                                            |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=GEMM                                           | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | gemm                          | 5.3                 | 6.1                        | ~2.5                                         |                                                     | Peak memory = 21Gb, time = ~20mn                                        |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=GEMM                                           | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | exllama-1                     | 6.1                 | 6.3                        | 48.1                                         |                                                     | Peak memory = 21Gb, time = ~20mn                                        |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=GEMM                                           | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | exllama-2^                    | 6.3                 | 6.5                        | 49.3                                         | 5.277                                               | Peak memory = 21Gb, time = ~20mn                                        |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=GEMM                                           | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | exllama-2^<br>\+ fused layers | 12.3                | 12.3                       | 151.7<br>(batch_32=864,<br>but extreme VRAM) | 5.638                                               | Peak memory = 21Gb, time = ~20mn                                        |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=GEMV                                           | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | gemv                          | 5.3                 | 5.6                        | 46.9                                         | 5.276                                               | Peak memory = 21Gb, time = ~28mn                                        |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=GEMV                                           | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | gemv<br>\+ fused layers       | 11.4                | 11.4                       | 127.3                                        | 5.638                                               | Peak memory = 21Gb, time = ~28mn                                        |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=GEMV_fast                                      | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | gemv_fast                     | 5.3                 | 5.6                        | 50.4<br>(batch_32=458)                       | 5.276                                               | Peak memory = 21Gb, time = ~30mn                                        |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=GEMV_fast                                      | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | gemv_fast<br>\+ fused layers  | 11.5                | 11.5                       | 152.1                                        | 5.638                                               | Peak memory = 21Gb, time = ~30mn                                        |
| Mistral-7B | Instruct-v0.2 (Adrien)<br>Group=128, version=Marlin<br>(Zero point=false needed for marlin) | AutoAWQ<br>(GPTQ^^^)      | 4 bits            | FP16                 | \-              | FP16                          | marlin                        | 5.3                 | 5.5                        | 45.5<br>(batch_32=442)                       | 5.325                                               | Peak memory = 21Gb, time = ~16mn                                        |